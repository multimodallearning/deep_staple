{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3175666",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name                     Util    Mem free  Cuda             User(s)\n",
      "----  --------------------------  ------  ----------  ---------------  ---------\n",
      "   2  NVIDIA GeForce RTX 2080 Ti     0 %   11018 MiB  11.5(495.29.05)\n",
      "   1  NVIDIA GeForce RTX 2080 Ti     0 %   11016 MiB  11.5(495.29.05)\n",
      "   3  NVIDIA GeForce RTX 2080 Ti     0 %    9269 MiB  11.5(495.29.05)  nicke\n",
      "   0  NVIDIA GeForce RTX 2080 Ti     0 %    2575 MiB  11.5(495.29.05)  schneider\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   2  NVIDIA GeForce RTX 2080 Ti  ->  cuda:0\n",
      "1.9.0a0+gitdfbd030\n",
      "8204\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Running in: /share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "import glob\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -4\"))\n",
    "import pickle\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import functools\n",
    "from enum import Enum, auto\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "import scipy\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import visualize_seg\n",
    "\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "from curriculum_deeplab.utils import interpolate_sample, in_notebook, dilate_label_class, LabelDisturbanceMode, ensure_dense\n",
    "from curriculum_deeplab.CrossmodaHybridIdLoader import CrossmodaHybridIdLoader, get_crossmoda_data_load_closure\n",
    "from curriculum_deeplab.MobileNet_LR_ASPP_3D import MobileNet_LRASPP_3D, MobileNet_ASPP_3D\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "if in_notebook:\n",
    "    THIS_SCRIPT_DIR = os.path.abspath('')\n",
    "else:\n",
    "    THIS_SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "print(f\"Running in: {THIS_SCRIPT_DIR}\")\n",
    "\n",
    "def get_batch_dice_per_class(b_dice, class_tags, exclude_bg=True) -> dict:\n",
    "    score_dict = {}\n",
    "    for cls_idx, cls_tag in enumerate(class_tags):\n",
    "        if exclude_bg and cls_idx == 0:\n",
    "            continue\n",
    "\n",
    "        if torch.all(torch.isnan(b_dice[:,cls_idx])):\n",
    "            score = float('nan')\n",
    "        else:\n",
    "            score = np.nanmean(b_dice[:,cls_idx]).item()\n",
    "\n",
    "        score_dict[cls_tag] = score\n",
    "\n",
    "    return score_dict\n",
    "\n",
    "def get_batch_dice_over_all(b_dice, exclude_bg=True) -> float:\n",
    "\n",
    "    start_idx = 1 if exclude_bg else 0\n",
    "    if torch.all(torch.isnan(b_dice[:,start_idx:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,start_idx:]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5750fa2f-e706-4c65-ac09-c3ab8050b479",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DataParamMode(Enum):\n",
    "    INSTANCE_PARAMS = auto()\n",
    "    DISABLED = auto()\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "        See https://stackoverflow.com/questions/49901590/python-using-copy-deepcopy-on-dotdict\n",
    "    \"\"\"\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        try:\n",
    "            return self[item]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError from e\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "\n",
    "    'use_mind': False,\n",
    "    'epochs': 40,\n",
    "\n",
    "    'batch_size': 8,\n",
    "    'val_batch_size': 1,\n",
    "    'use_2d_normal_to': None,\n",
    "    'train_patchwise': False,\n",
    "\n",
    "    'num_val_images': 20,\n",
    "    'atlas_count': 1,\n",
    "\n",
    "    'dataset': 'crossmoda',\n",
    "    'reg_state': \"acummulate_convex_adam_FT2_MT1\",\n",
    "    'train_set_max_len': None,\n",
    "    'crop_3d_w_dim_range': (45, 95),\n",
    "    'crop_2d_slices_gt_num_threshold': 0,\n",
    "\n",
    "    'lr': 0.01,\n",
    "    'use_scheduling': True,\n",
    "\n",
    "    # Data parameter config\n",
    "    'data_param_mode': DataParamMode.INSTANCE_PARAMS,\n",
    "    'init_inst_param': 0.0,\n",
    "    'lr_inst_param': 0.1,\n",
    "    'use_risk_regularization': True,\n",
    "    'use_fixed_weighting': True,\n",
    "    'use_ool_dp_loss': False,\n",
    "\n",
    "    'fixed_weight_file': None,\n",
    "    'fixed_weight_min_quantile': None,#.9,\n",
    "    'fixed_weight_min_value': None,\n",
    "    'override_embedding_weights': False,\n",
    "    # ),\n",
    "\n",
    "    'save_every': 200,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'debug': False,\n",
    "    'wandb_mode': 'disabled', # e.g. online, disabled\n",
    "    'do_sweep': False,\n",
    "\n",
    "    'checkpoint_name': None,\n",
    "    'fold_override': None,\n",
    "    'checkpoint_epx': None,\n",
    "\n",
    "    'do_plot': False,\n",
    "    'save_dp_figures': False,\n",
    "    'save_labels': False,\n",
    "\n",
    "    'disturbance_mode': None,\n",
    "    'disturbance_strength': 0.,\n",
    "    'disturbed_percentage': 0.,\n",
    "})\n",
    "\n",
    "if config_dict.train_patchwise:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de18b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(config):\n",
    "    reset_determinism()\n",
    "    if config.reg_state:\n",
    "        print(\"Loading registered data.\")\n",
    "\n",
    "        if config.reg_state == \"mix_combined_best\":\n",
    "            config.atlas_count = 1\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "\n",
    "            perm = np.random.permutation(len(loaded_identifier))\n",
    "            _clen = int(.5*len(loaded_identifier))\n",
    "            best_choice = perm[:_clen]\n",
    "            combined_choice = perm[_clen:]\n",
    "\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)[best_choice]\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)[combined_choice]\n",
    "            label_data = torch.zeros([107,128,128,128])\n",
    "            label_data[best_choice] = best_label_data\n",
    "            label_data[combined_choice] = combined_label_data\n",
    "            var_identifier = [\"mBST\" if idx in best_choice else \"mCMB\" for idx in range(len(loaded_identifier))]\n",
    "            loaded_identifier = [f\"{_id}:{var_id}\" for _id, var_id in zip(loaded_identifier, var_identifier)]\n",
    "\n",
    "        elif config.reg_state == \"acummulate_combined_best\":\n",
    "            config.atlas_count = 2\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)\n",
    "            label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            loaded_identifier = [_id+':mBST' for _id in loaded_identifier] + [_id+':mCMB' for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"best\":\n",
    "            config.atlas_count = 1\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            label_data = torch.cat([label_data_left[config.reg_state+'_all'][:44], label_data_right[config.reg_state+'_all'][:63]], dim=0)\n",
    "            postfix = 'mBST'\n",
    "            loaded_identifier = [_id+':'+postfix for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"combined\":\n",
    "            config.atlas_count = 1\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            label_data = torch.cat([label_data_left[config.reg_state+'_all'][:44], label_data_right[config.reg_state+'_all'][:63]], dim=0)\n",
    "            postfix = 'mCMB'\n",
    "            loaded_identifier = [_id+':'+postfix for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"acummulate_convex_adam_FT2_MT1\":\n",
    "            config.atlas_count = 10\n",
    "            domain = 'target'\n",
    "            bare_data = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220318_crossmoda_convex_adam_lr/crossmoda_convex_registered_new_convex.pth\")\n",
    "            label_data = []\n",
    "            loaded_identifier = []\n",
    "            for fixed_id, moving_dict in bare_data.items():\n",
    "                sorted_moving_dict = OrderedDict(moving_dict)\n",
    "                for idx_mov, (moving_id, moving_sample) in enumerate(sorted_moving_dict.items()):\n",
    "                    # Only use every third warped sample\n",
    "                    if idx_mov % 3 == 0:\n",
    "                        label_data.append(moving_sample['warped_label'].cpu())\n",
    "                        loaded_identifier.append(f\"{fixed_id}:m{moving_id}\")\n",
    "\n",
    "        elif config.reg_state == \"acummulate_every_third_deeds_FT2_MT1\":\n",
    "            config.atlas_count = 10\n",
    "            domain = 'target'\n",
    "            bare_data = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220114_crossmoda_multiple_registrations/crossmoda_deeds_registered.pth\")\n",
    "            label_data = []\n",
    "            loaded_identifier = []\n",
    "            for fixed_id, moving_dict in bare_data.items():\n",
    "                sorted_moving_dict = OrderedDict(moving_dict)\n",
    "                for idx_mov, (moving_id, moving_sample) in enumerate(sorted_moving_dict.items()):\n",
    "                    # Only use every third warped sample\n",
    "                    if idx_mov % 3 == 0:\n",
    "                        label_data.append(moving_sample['warped_label'].cpu())\n",
    "                        loaded_identifier.append(f\"{fixed_id}:m{moving_id}\")\n",
    "\n",
    "        elif config.reg_state == \"acummulate_every_deeds_FT2_MT1\":\n",
    "            config.atlas_count = 30\n",
    "            domain = 'target'\n",
    "            bare_data = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220114_crossmoda_multiple_registrations/crossmoda_deeds_registered.pth\")\n",
    "            label_data = []\n",
    "            loaded_identifier = []\n",
    "            for fixed_id, moving_dict in bare_data.items():\n",
    "                sorted_moving_dict = OrderedDict(moving_dict)\n",
    "                for idx_mov, (moving_id, moving_sample) in enumerate(sorted_moving_dict.items()):\n",
    "                    label_data.append(moving_sample['warped_label'].cpu())\n",
    "                    loaded_identifier.append(f\"{fixed_id}:m{moving_id}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        modified_3d_label_override = {}\n",
    "        for idx, identifier in enumerate(loaded_identifier):\n",
    "            # Find sth. like 100r:mBST or 100r:m001l\n",
    "            nl_id, lr_id, m_id = re.findall(r'(\\d{1,3})([lr]):m([A-Z0-9a-z]{3,4})$', identifier)[0]\n",
    "            nl_id = int(nl_id)\n",
    "            crossmoda_var_id = f\"{nl_id:03d}{lr_id}:m{m_id}\"\n",
    "            modified_3d_label_override[crossmoda_var_id] = label_data[idx]\n",
    "\n",
    "        prevent_disturbance = True\n",
    "\n",
    "    else:\n",
    "        domain = 'source'\n",
    "        modified_3d_label_override = None\n",
    "        prevent_disturbance = False\n",
    "\n",
    "    if config.dataset == 'crossmoda':\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.5\n",
    "        clsre = get_crossmoda_data_load_closure(\n",
    "            base_dir=\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "            domain=domain, state='l4', use_additional_data=False,\n",
    "            size=(128,128,128), resample=True, normalize=True, crop_3d_w_dim_range=config.crop_3d_w_dim_range,\n",
    "            ensure_labeled_pairs=True, modified_3d_label_override=modified_3d_label_override,\n",
    "            debug=config.debug\n",
    "        )\n",
    "        training_dataset = CrossmodaHybridIdLoader(\n",
    "            clsre,\n",
    "            size=(128,128,128), resample=True, normalize=True, crop_3d_w_dim_range=config.crop_3d_w_dim_range,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor,\n",
    "            fixed_weight_file=config.fixed_weight_file, fixed_weight_min_quantile=config.fixed_weight_min_quantile, fixed_weight_min_value=config.fixed_weight_min_value,\n",
    "        )\n",
    "\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "    if config.dataset == 'ixi':\n",
    "        raise NotImplementedError()\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.\n",
    "        clsre = get_ixi_data_load_closure()\n",
    "        training_dataset = IXIHybridIdLoader(\n",
    "            clsre,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(f\"Nonzero slices: \" \\\n",
    "            f\"{sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)*100}%\"\n",
    "        )\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "\n",
    "    elif config['dataset'] == 'organmnist3d':\n",
    "        training_dataset = WrapperOrganMNIST3D(\n",
    "            split='train', root='./data/medmnist', download=True, normalize=True,\n",
    "            max_load_num=300, crop_3d_w_dim_range=None,\n",
    "            disturbed_idxs=None, use_2d_normal_to='W'\n",
    "        )\n",
    "        print(training_dataset.mnist_set.info)\n",
    "        print(\"Classes: \", training_dataset.label_tags)\n",
    "        print(\"Samples: \", len(training_dataset))\n",
    "\n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fbc5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    _, all_labels, _ = training_dataset.get_data(use_2d_override=False)\n",
    "    print(all_labels.shape)\n",
    "    sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "    plt.xlabel(\"W\")\n",
    "    plt.ylabel(\"ground truth>0\")\n",
    "    plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0adbde3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CrossMoDa ceT1 images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209 images, 209 labels: 100%|██████████| 418/418 [00:37<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postprocessing 3D volumes\n",
      "Removed 0 3D images in postprocessing\n",
      "Equal image and label numbers: True (107)\n",
      "Image shape: torch.Size([107, 128, 128, 50]), mean.: -0.00, std.: 1.00\n",
      "Label shape: torch.Size([107, 128, 128, 50]), max.: 1\n",
      "Postprocessing 2D slices\n",
      "Removed 0 of 5350 2D slices in postprocessing\n",
      "Data import finished.\n",
      "CrossMoDa loader will yield 2D samples\n",
      "[]\n",
      "Displaying 2D training sample\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.5\n",
      "1.0\n",
      "2.0\n",
      "5.0\n",
      "Sample 0:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1760201/772130294.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0msample_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_3d_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mground_truth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/mdl_seg_class/visualization.py\u001b[0m in \u001b[0;36mvisualize_seg\u001b[0;34m(in_type, reduce_dim, img, seg, ground_truth, crop_to_non_zero_seg, crop_to_non_zero_gt, alpha_seg, alpha_gt, onehot_color_map, n_per_row, overlay_text, frame_elements, annotate_color, file_path)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mground_truth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Ground-truth need to have dimensions DxHxW but is {ground_truth.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_2d_stack_from_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0min_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_2D\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "\n",
    "\n",
    "\n",
    "def save_parameter_figure(_path, title, text, parameters, reweighted_parameters, dices):\n",
    "    # Show weights and weights with compensation\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12, 4), dpi=80)\n",
    "    sc1 = axs[0].scatter(\n",
    "        range(len(parameters)),\n",
    "        parameters.cpu().detach(), c=dices,s=1, cmap='plasma', vmin=0., vmax=1.)\n",
    "    sc2 = axs[1].scatter(\n",
    "        range(len(reweighted_parameters)),\n",
    "        reweighted_parameters.cpu().detach(), s=1,c=dices, cmap='plasma', vmin=0., vmax=1.)\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.text(0, 0, text)\n",
    "    axs[0].set_title('Bare parameters')\n",
    "    axs[1].set_title('Reweighted parameters')\n",
    "    axs[0].set_ylim(-10, 10)\n",
    "    axs[1].set_ylim(-3, 1)\n",
    "    plt.colorbar(sc2)\n",
    "    plt.savefig(_path)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "\n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "\n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "\n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "\n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio\n",
    "\n",
    "def log_data_parameter_stats(log_path, epx, data_parameters):\n",
    "    \"\"\"Log stats for data parameters on wandb.\"\"\"\n",
    "    wandb.log({f'{log_path}/highest': torch.max(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/lowest': torch.min(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/mean': torch.mean(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/std': torch.std(data_parameters).item()}, step=epx)\n",
    "\n",
    "\n",
    "\n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_class_dices(log_prefix, log_postfix, class_dices, log_idx):\n",
    "    if not class_dices:\n",
    "        return\n",
    "\n",
    "    for cls_name in class_dices[0].keys():\n",
    "        log_path = f\"{log_prefix}{cls_name}{log_postfix}\"\n",
    "\n",
    "        cls_dices = list(map(lambda dct: dct[cls_name], class_dices))\n",
    "        mean_per_class =np.nanmean(cls_dices)\n",
    "        print(log_path, f\"{mean_per_class*100:.2f}%\")\n",
    "        wandb.log({log_path: mean_per_class}, step=log_idx)\n",
    "\n",
    "def CELoss(logits, targets, bin_weight=None):\n",
    "    # -logits[0,targets[0,0,0],0,0]+torch.log(logits[0,:,0,0].exp().sum())\n",
    "    targets = torch.nn.functional.one_hot(targets).permute(0,3,1,2)\n",
    "    loss = -targets*F.log_softmax(logits, 1)\n",
    "\n",
    "    if bin_weight is not None:\n",
    "        brdcast_shape = torch.Size((loss.shape[0], loss.shape[1], *((loss.dim()-2)*[1])))\n",
    "        inv_weight = (bin_weight+np.exp(1)).log()+np.exp(1)\n",
    "        inv_weight = inv_weight/inv_weight.mean()\n",
    "        loss = inv_weight.view(brdcast_shape)*loss\n",
    "\n",
    "    return loss.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = prepare_data(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "\n",
    "    # Print transformed 2D data\n",
    "    training_dataset.train(use_modified=True, augment=False)\n",
    "    # print(training_dataset.disturbed_idxs)\n",
    "\n",
    "    print(\"Displaying 2D training sample\")\n",
    "\n",
    "    img_stack = []\n",
    "    label_stack = []\n",
    "    mod_label_stack = []\n",
    "\n",
    "    for sample in (training_dataset[idx] for idx in [500,590]):\n",
    "        print(sample['id'])\n",
    "        img_stack.append(sample['image'])\n",
    "        label_stack.append(sample['label'])\n",
    "        mod_label_stack.append(sample['modified_label'])\n",
    "\n",
    "    # Change label num == hue shift for display\n",
    "    img_stack = torch.stack(img_stack).unsqueeze(1)\n",
    "    label_stack = torch.stack(label_stack)\n",
    "    mod_label_stack = torch.stack(mod_label_stack)\n",
    "\n",
    "    mod_label_stack*=4\n",
    "\n",
    "    visualize_seg(in_type=\"batch_3D\", reduce_dim=\"W\",\n",
    "        img=img_stack,\n",
    "        # ground_truth=label_stack,\n",
    "        seg=(mod_label_stack-label_stack).abs(),\n",
    "        # crop_to_non_zero_gt=True,\n",
    "        crop_to_non_zero_seg=True,\n",
    "        alpha_seg = .5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use module.named_modules() to retrieve valid keychains for layers.\n",
    "       e.g.\n",
    "       first_keychain = list(module.keys())[0]\n",
    "       new_first_replacee = torch.nn.Conv1d(1,2,3)\n",
    "       set_module(first_keychain, torch.nn.Conv1d(1,2,3))\n",
    "    \"\"\"\n",
    "\n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "335f4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(_path, **statefuls):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "    _path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for name, stful in statefuls.items():\n",
    "        if stful != None:\n",
    "            torch.save(stful.state_dict(), _path.joinpath(name+'.pth'))\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config, dataset_len, num_classes, THIS_SCRIPT_DIR, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        # Use vanilla torch model\n",
    "        lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(\n",
    "            pretrained=False, progress=True, num_classes=num_classes\n",
    "        )\n",
    "        set_module(lraspp, 'backbone.0.0',\n",
    "            torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2),\n",
    "                            padding=(1, 1), bias=False)\n",
    "        )\n",
    "    else:\n",
    "        # Use custom 3d model\n",
    "        lraspp = MobileNet_LRASPP_3D(\n",
    "            in_num=in_channels, num_classes=num_classes,\n",
    "            use_checkpointing=True\n",
    "        )\n",
    "\n",
    "    # lraspp.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "    lraspp.to(device)\n",
    "    print(f\"Param count lraspp: {sum(p.numel() for p in lraspp.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(lraspp.parameters(), lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2)\n",
    "    else:\n",
    "        # Use ExponentialLR in 3D\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.99)\n",
    "\n",
    "    # Add data paramters embedding and optimizer\n",
    "    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len, 1, sparse=True)\n",
    "        embedding = embedding.to(device)\n",
    "\n",
    "        # Init embedding values\n",
    "        #\n",
    "        if config.override_embedding_weights:\n",
    "            fixed_weightdata = torch.load(config.fixed_weight_file)\n",
    "            fixed_weights = fixed_weightdata['data_parameters']\n",
    "            fixed_d_ids = fixed_weightdata['d_ids']\n",
    "            if config.use_2d_normal_to is not None:\n",
    "                corresp_dataset_idxs = [training_dataset.get_2d_ids().index(_id) for _id in fixed_d_ids]\n",
    "            else:\n",
    "                corresp_dataset_idxs = [training_dataset.get_3d_ids().index(_id) for _id in fixed_d_ids]\n",
    "            embedding_weight_tensor = torch.zeros_like(embedding.weight)\n",
    "            embedding_weight_tensor[corresp_dataset_idxs] = fixed_weights.view(-1,1).cuda()\n",
    "            embedding = nn.Embedding(len(training_dataset), 1, sparse=True, _weight=embedding_weight_tensor)\n",
    "\n",
    "        elif _path and _path.is_dir():\n",
    "            embedding.load_state_dict(torch.load(_path.joinpath('embedding.pth'), map_location=device))\n",
    "        else:\n",
    "            torch.nn.init.normal_(embedding.weight.data, mean=config.init_inst_param, std=0.00)\n",
    "\n",
    "        print(f\"Param count embedding: {sum(p.numel() for p in embedding.parameters())}\")\n",
    "\n",
    "        optimizer_dp = torch.optim.SparseAdam(\n",
    "            embedding.parameters(), lr=config.lr_inst_param,\n",
    "            betas=(0.9, 0.999), eps=1e-08)\n",
    "        scaler_dp =  amp.GradScaler()\n",
    "\n",
    "        if _path and _path.is_dir():\n",
    "            print(f\"Loading dp_optimizer and scaler_dp from {_path}\")\n",
    "            optimizer_dp.load_state_dict(torch.load(_path.joinpath('optimizer_dp.pth'), map_location=device))\n",
    "            scaler_dp.load_state_dict(torch.load(_path.joinpath('scaler_dp.pth'), map_location=device))\n",
    "\n",
    "    else:\n",
    "        embedding = None\n",
    "        optimizer_dp = None\n",
    "        scaler_dp = None\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading lr-aspp model, optimizers and grad scalers from {_path}\")\n",
    "        lraspp.load_state_dict(torch.load(_path.joinpath('lraspp.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scheduler.load_state_dict(torch.load(_path.joinpath('scheduler.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "\n",
    "    return (lraspp, optimizer, scheduler, optimizer_dp, embedding, scaler, scaler_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "490f1722",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def map_embedding_idxs(idxs, grid_size_y, grid_size_x):\n",
    "    with torch.no_grad():\n",
    "        t_sz = grid_size_y * grid_size_x\n",
    "        return ((idxs*t_sz).long().repeat(t_sz).view(t_sz, idxs.numel())+torch.tensor(range(t_sz)).to(idxs).view(t_sz,1)).permute(1,0).reshape(-1)\n",
    "\n",
    "def inference_wrap(lraspp, img, use_2d, use_mind):\n",
    "    with torch.inference_mode():\n",
    "        b_img = img.unsqueeze(0).unsqueeze(0).float()\n",
    "        if use_2d and use_mind:\n",
    "            # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "            b_img = mindssc(b_img.unsqueeze(0)).squeeze(2)\n",
    "        elif not use_2d and use_mind:\n",
    "            # MIND 3D in Bx1xDxHxW out BxMINDxDxHxW\n",
    "            b_img = mindssc(b_img)\n",
    "        elif use_2d or not use_2d:\n",
    "            # 2D Bx1xHxW\n",
    "            # 3D out Bx1xDxHxW\n",
    "            pass\n",
    "\n",
    "        b_out = lraspp(b_img)['out']\n",
    "        b_out = b_out.argmax(1)\n",
    "        return b_out\n",
    "\n",
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    # kf.get_n_splits(training_dataset.__len__(use_2d_override=False))\n",
    "    fold_iter = enumerate(kf.split(range(training_dataset.__len__(use_2d_override=False))))\n",
    "\n",
    "    if config.get('fold_override', None):\n",
    "        selected_fold = config.get('fold_override', 0)\n",
    "        fold_iter = list(fold_iter)[selected_fold:selected_fold+1]\n",
    "    elif config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "\n",
    "    if config.wandb_mode != 'disabled':\n",
    "        warnings.warn(\"Logging of dataset file paths is disabled.\")\n",
    "        # # Log dataset info\n",
    "        # training_dataset.eval()\n",
    "        # dataset_info = [[smp['dataset_idx'], smp['id'], smp['image_path'], smp['label_path']] \\\n",
    "        #                 for smp in training_dataset]\n",
    "        # wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', 'id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        n_dims = (-2,-1)\n",
    "    else:\n",
    "        n_dims = (-3,-2,-1)\n",
    "\n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        all_3d_ids = training_dataset.get_3d_ids()\n",
    "\n",
    "        # Override fold idxs #TODO automate\n",
    "\n",
    "        if config.debug:\n",
    "            num_val_images = 2\n",
    "            atlas_count = 1\n",
    "        else:\n",
    "            num_val_images = config.num_val_images\n",
    "            atlas_count = config.atlas_count\n",
    "\n",
    "        if config.use_2d_normal_to is not None:\n",
    "            # Override idxs\n",
    "            all_3d_ids = training_dataset.get_3d_ids()\n",
    "\n",
    "            val_3d_idxs = torch.tensor(list(range(0, num_val_images*atlas_count, atlas_count)))\n",
    "            val_3d_ids = training_dataset.switch_3d_identifiers(val_3d_idxs)\n",
    "\n",
    "            train_3d_idxs = list(range(num_val_images*atlas_count, len(all_3d_ids)))\n",
    "\n",
    "            # Get corresponding 2D idxs\n",
    "            train_2d_ids = []\n",
    "            dcts = training_dataset.get_id_dicts()\n",
    "            for id_dict in dcts:\n",
    "                _2d_id = id_dict['2d_id']\n",
    "                _3d_idx = id_dict['3d_dataset_idx']\n",
    "                if _2d_id in training_dataset.label_data_2d.keys() and _3d_idx in train_3d_idxs:\n",
    "                    train_2d_ids.append(_2d_id)\n",
    "\n",
    "            train_2d_idxs = training_dataset.switch_2d_identifiers(train_2d_ids)\n",
    "            train_idxs = torch.tensor(train_2d_idxs)\n",
    "\n",
    "        else:\n",
    "            val_3d_idxs = torch.tensor(list(range(0, num_val_images*atlas_count, atlas_count)))\n",
    "            val_3d_ids = training_dataset.switch_3d_identifiers(val_3d_idxs)\n",
    "\n",
    "            train_3d_idxs = list(range(num_val_images*atlas_count, len(all_3d_ids)))\n",
    "            train_idxs = torch.tensor(train_3d_idxs)\n",
    "\n",
    "        print(f\"Will run validation with these 3D samples (#{len(val_3d_ids)}):\", sorted(val_3d_ids))\n",
    "\n",
    "        _, _, all_modified_segs = training_dataset.get_data()\n",
    "\n",
    "        if config.disturbed_percentage > 0.:\n",
    "            with torch.no_grad():\n",
    "                non_empty_train_idxs = [(all_modified_segs[train_idxs].sum(dim=n_dims) > 0)]\n",
    "\n",
    "            ### Disturb dataset (only non-emtpy idxs)###\n",
    "            proposed_disturbed_idxs = np.random.choice(non_empty_train_idxs, size=int(len(non_empty_train_idxs)*config.disturbed_percentage), replace=False)\n",
    "            proposed_disturbed_idxs = torch.tensor(proposed_disturbed_idxs)\n",
    "            training_dataset.disturb_idxs(proposed_disturbed_idxs,\n",
    "                disturbance_mode=config.disturbance_mode,\n",
    "                disturbance_strength=config.disturbance_strength\n",
    "            )\n",
    "            disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "            disturbed_bool_vect[training_dataset.disturbed_idxs] = 1.\n",
    "        else:\n",
    "            disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "\n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, training_dataset.disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(training_dataset.disturbed_idxs))\n",
    "\n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in training_dataset.disturbed_idxs])},\n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "\n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels = 12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "\n",
    "\n",
    "\n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "            sampler=train_subsampler, pin_memory=False, drop_last=False,\n",
    "            # collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "        )\n",
    "\n",
    "        # training_dataset.set_augment_at_collate(True)\n",
    "\n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        if 'checkpoint_epx' in config and config['checkpoint_epx'] is not None:\n",
    "            epx_start = config['checkpoint_epx']\n",
    "        else:\n",
    "            epx_start = 0\n",
    "\n",
    "        if config.checkpoint_name:\n",
    "            # Load from checkpoint\n",
    "            _path = f\"{config.mdl_save_prefix}/{config.checkpoint_name}_fold{fold_idx}_epx{epx_start}\"\n",
    "        else:\n",
    "            _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx_start}\"\n",
    "\n",
    "        (lraspp, optimizer, scheduler, optimizer_dp, embedding, scaler, scaler_dp) = get_model(config, len(training_dataset), len(training_dataset.label_tags),\n",
    "            THIS_SCRIPT_DIR=THIS_SCRIPT_DIR, _path=_path, device='cuda')\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        dice_func = dice2d if config.use_2d_normal_to is not None else dice3d\n",
    "\n",
    "        bn_count = torch.zeros([len(training_dataset.label_tags)], device=all_modified_segs.device)\n",
    "        wise_dice = torch.zeros([len(training_dataset), len(training_dataset.label_tags)])\n",
    "        gt_num = torch.zeros([len(training_dataset)])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(\"Fetching training metrics for samples.\")\n",
    "            # _, wise_lbls, mod_lbls = training_dataset.get_data()\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            for sample in tqdm((training_dataset[idx] for idx in train_idxs), desc=\"metric:\", total=len(train_idxs)):\n",
    "                d_idxs = sample['dataset_idx']\n",
    "                wise_label, mod_label = sample['label'], sample['modified_label']\n",
    "                mod_label = mod_label.cuda()\n",
    "                wise_label = wise_label.cuda()\n",
    "                mod_label, _ = ensure_dense(mod_label)\n",
    "\n",
    "                dsc = dice_func(\n",
    "                    torch.nn.functional.one_hot(wise_label.unsqueeze(0), len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(mod_label.unsqueeze(0), len(training_dataset.label_tags)),\n",
    "                    one_hot_torch_style=True, nan_for_unlabeled_target=False\n",
    "                )\n",
    "                bn_count += torch.bincount(mod_label.reshape(-1).long(), minlength=len(training_dataset.label_tags)).cpu()\n",
    "                wise_dice[d_idxs] = dsc.cpu()\n",
    "                gt_num[d_idxs] = (mod_label > 0).sum(dim=n_dims).float().cpu()\n",
    "\n",
    "            class_weights = 1/(bn_count).float().pow(.35)\n",
    "            class_weights /= class_weights.mean()\n",
    "\n",
    "            fixed_weighting = (gt_num+np.exp(1)).log()+np.exp(1)\n",
    "\n",
    "        class_weights = class_weights.cuda()\n",
    "        fixed_weighting = fixed_weighting.cuda()\n",
    "\n",
    "        for epx in range(epx_start, config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "\n",
    "            lraspp.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            training_dataset.train(use_modified=True)\n",
    "\n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            class_dices = []\n",
    "\n",
    "            # Load data\n",
    "            for batch_idx, batch in tqdm(enumerate(train_dataloader), desc=\"batch:\", total=len(train_dataloader)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if optimizer_dp:\n",
    "                    optimizer_dp.zero_grad()\n",
    "\n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "                b_spat_aug_grid = batch['spat_augment_grid']\n",
    "\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "                b_img = b_img.float()\n",
    "\n",
    "                b_img = b_img.cuda()\n",
    "                b_seg_modified = b_seg_modified.cuda()\n",
    "                b_idxs_dataset = b_idxs_dataset.cuda()\n",
    "                b_seg = b_seg.cuda()\n",
    "                b_spat_aug_grid = b_spat_aug_grid.cuda()\n",
    "\n",
    "                if training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "                elif not training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 3D\n",
    "                    b_img = mindssc(b_img.unsqueeze(1))\n",
    "                else:\n",
    "                    b_img = b_img.unsqueeze(1)\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=True):\n",
    "                    assert b_img.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input image for model must be {len(n_dims)+2}D: BxCxSPATIAL but is {b_img.shape}\"\n",
    "                    for param in lraspp.parameters():\n",
    "                        param.requires_grad = True\n",
    "\n",
    "                    lraspp.use_checkpointing = True\n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    ### Calculate loss ###\n",
    "                    assert logits.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input shape for loss must be BxNUM_CLASSESxSPATIAL but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == len(n_dims)+1, \\\n",
    "                        f\"Target shape for loss must be BxSPATIAL but is {b_seg_modified.shape}\"\n",
    "\n",
    "                    ce_loss = nn.CrossEntropyLoss(class_weights)(logits, b_seg_modified)\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.DISABLED) or config.use_ool_dp_loss:\n",
    "                        scaler.scale(ce_loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                        if config.use_ool_dp_loss:\n",
    "                            # Run second consecutive forward pass\n",
    "                            for param in lraspp.parameters():\n",
    "                                param.requires_grad = False\n",
    "                            lraspp.use_checkpointing = False\n",
    "                            dp_logits = lraspp(b_img)['out']\n",
    "                        else:\n",
    "                            # Do not run a second forward pass\n",
    "                            for param in lraspp.parameters():\n",
    "                                param.requires_grad = True\n",
    "                            lraspp.use_checkpointing = True\n",
    "                            dp_logits = logits\n",
    "\n",
    "                        dp_loss = nn.CrossEntropyLoss(reduction='none')(dp_logits, b_seg_modified)\n",
    "                        dp_loss = dp_loss.mean(n_dims)\n",
    "\n",
    "                        bare_weight = embedding(b_idxs_dataset).squeeze()\n",
    "\n",
    "                        weight = torch.sigmoid(bare_weight)\n",
    "                        weight = weight/weight.mean()\n",
    "\n",
    "                        # This improves scores significantly: Reweight with log(gt_numel)\n",
    "                        if config.use_fixed_weighting:\n",
    "                            weight = weight/fixed_weighting[b_idxs_dataset]\n",
    "\n",
    "                        if config.use_risk_regularization:\n",
    "                            p_pred_num = (dp_logits.argmax(1) > 0).sum(dim=n_dims).detach()\n",
    "                            if config.use_2d_normal_to is not None:\n",
    "                                risk_regularization = -weight*p_pred_num/(dp_logits.shape[-2]*dp_logits.shape[-1])\n",
    "                            else:\n",
    "                                risk_regularization = -weight*p_pred_num/(dp_logits.shape[-3]*dp_logits.shape[-2]*dp_logits.shape[-1])\n",
    "\n",
    "                            dp_loss = (dp_loss*weight).sum() + risk_regularization.sum()\n",
    "                        else:\n",
    "                            dp_loss = (dp_loss*weight).sum()\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                    scaler_dp.scale(dp_loss).backward()\n",
    "\n",
    "                    if config.use_ool_dp_loss:\n",
    "                        # LRASPP already stepped.\n",
    "                        if not config.override_embedding_weights:\n",
    "                            scaler_dp.step(optimizer_dp)\n",
    "                            scaler_dp.update()\n",
    "                    else:\n",
    "                        scaler_dp.step(optimizer)\n",
    "                        if not config.override_embedding_weights:\n",
    "                            scaler_dp.step(optimizer_dp)\n",
    "                        scaler_dp.update()\n",
    "\n",
    "                    epx_losses.append(dp_loss.item())\n",
    "                else:\n",
    "                    epx_losses.append(ce_loss.item())\n",
    "\n",
    "                logits_for_score = logits.argmax(1)\n",
    "\n",
    "                # Calculate dice score\n",
    "                b_dice = dice_func(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(b_seg, len(training_dataset.label_tags)), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_over_all(\n",
    "                    b_dice, exclude_bg=True))\n",
    "                class_dices.append(get_batch_dice_per_class(\n",
    "                    b_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                ###  Scheduler management ###\n",
    "                if config.use_scheduling and epx % atlas_count == 0:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED) and batch_idx % 10 == 0 and config.save_dp_figures:\n",
    "                    # Output data parameter figure\n",
    "                    train_params = embedding.weight[train_idxs].squeeze()\n",
    "                    # order = np.argsort(train_params.cpu().detach()) # Order by DP value\n",
    "                    order = torch.arange(len(train_params))\n",
    "                    pearson_corr_coeff = np.corrcoef(train_params.cpu().detach(), wise_dice[train_idxs][:,1].cpu().detach())[0,1]\n",
    "                    dp_figure_path = Path(f\"data/output_figures/{wandb.run.name}_fold{fold_idx}/dp_figure_epx{epx:03d}_batch{batch_idx:03d}.png\")\n",
    "                    dp_figure_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    save_parameter_figure(dp_figure_path, wandb.run.name, f\"corr. coeff. DP vs. dice(expert label, train gt): {pearson_corr_coeff:4f}\",\n",
    "                        train_params[order], train_params[order]/fixed_weighting[train_idxs][order], dices=wise_dice[train_idxs][:,1][order])\n",
    "\n",
    "                if config.debug:\n",
    "                    break\n",
    "\n",
    "            ### Logging ###\n",
    "            print(f\"### Log epoch {epx} @ {time.time()-t0:.2f}s\")\n",
    "            print(\"### Training\")\n",
    "            ### Log wandb data ###\n",
    "            # Log the epoch idx per fold - so we can recover the diagram by setting\n",
    "            # ref_epoch_idx as x-axis in wandb interface\n",
    "            wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "\n",
    "            mean_loss = torch.tensor(epx_losses).mean()\n",
    "            wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "\n",
    "            mean_dice = np.nanmean(dices)\n",
    "            print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "            wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "\n",
    "            log_class_dices(\"scores/dice_mean_\", f\"_fold{fold_idx}\", class_dices, global_idx)\n",
    "\n",
    "            # Log data parameters of disturbed samples\n",
    "            if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                # Calculate dice score corr coeff (unknown to network)\n",
    "                train_params = embedding.weight[train_idxs].squeeze()\n",
    "                order = np.argsort(train_params.cpu().detach())\n",
    "                pearson_corr_coeff = np.corrcoef(train_params[order].cpu().detach(), wise_dice[train_idxs][:,1][order].cpu().detach())[0,1]\n",
    "                spearman_corr_coeff, spearman_p = scipy.stats.spearmanr(train_params[order].cpu().detach(), wise_dice[train_idxs][:,1][order].cpu().detach())\n",
    "\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/pearson_corr_coeff_fold{fold_idx}': pearson_corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/spearman_corr_coeff_fold{fold_idx}': spearman_corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/spearman_p_fold{fold_idx}': spearman_p},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                print(f'data_parameters/pearson_corr_coeff_fold{fold_idx}', f\"{pearson_corr_coeff:.2f}\")\n",
    "                print(f'data_parameters/spearman_corr_coeff_fold{fold_idx}', f\"{spearman_corr_coeff:.2f}\")\n",
    "                print(f'data_parameters/spearman_p_fold{fold_idx}', f\"{spearman_p:.5f}\")\n",
    "\n",
    "                # Log stats of data parameters and figure\n",
    "                log_data_parameter_stats(f'data_parameters/iter_stats_fold{fold_idx}', global_idx, embedding.weight.data)\n",
    "\n",
    "            if (epx % config.save_every == 0 and epx != 0) \\\n",
    "                or (epx+1 == config.epochs):\n",
    "                _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx}\"\n",
    "                save_model(\n",
    "                    _path,\n",
    "                    lraspp=lraspp,\n",
    "                    optimizer=optimizer, optimizer_dp=optimizer_dp,\n",
    "                    scheduler=scheduler,\n",
    "                    embedding=embedding,\n",
    "                    scaler=scaler,\n",
    "                    scaler_dp=scaler_dp)\n",
    "\n",
    "                (lraspp, optimizer, optimizer_dp, embedding, scaler) = \\\n",
    "                    get_model(\n",
    "                        config, len(training_dataset),\n",
    "                        len(training_dataset.label_tags),\n",
    "                        THIS_SCRIPT_DIR=THIS_SCRIPT_DIR,\n",
    "                        _path=_path, device='cuda')\n",
    "\n",
    "            print()\n",
    "            print(\"### Validation\")\n",
    "            lraspp.eval()\n",
    "            training_dataset.eval()\n",
    "\n",
    "            val_dices = []\n",
    "            val_class_dices = []\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    for val_idx in val_3d_idxs:\n",
    "                        val_sample = training_dataset.get_3d_item(val_idx)\n",
    "                        stack_dim = training_dataset.use_2d_normal_to\n",
    "                        # Create batch out of single val sample\n",
    "                        b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                        b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "\n",
    "                        B = b_val_img.shape[0]\n",
    "\n",
    "                        b_val_img = b_val_img.unsqueeze(1).float().cuda()\n",
    "                        b_val_seg = b_val_seg.cuda()\n",
    "\n",
    "                        if training_dataset.use_2d():\n",
    "                            b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=training_dataset.use_2d_normal_to)\n",
    "\n",
    "                            if config.use_mind:\n",
    "                                # MIND 2D model, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                                b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                            output_val = lraspp(b_val_img_2d)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "                            # Prepare logits for scoring\n",
    "                            # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                            val_logits_for_score = make_3d_from_2d_stack(\n",
    "                                val_logits_for_score.unsqueeze(1), stack_dim, B\n",
    "                            ).squeeze(1)\n",
    "\n",
    "                        else:\n",
    "                            if config.use_mind:\n",
    "                                # MIND 3D model shape BxMINDxDxHxW\n",
    "                                b_val_img = mindssc(b_val_img)\n",
    "                            else:\n",
    "                                # 3D model shape Bx1xDxHxW\n",
    "                                pass\n",
    "\n",
    "                            output_val = lraspp(b_val_img)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "\n",
    "                        b_val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(val_logits_for_score, len(training_dataset.label_tags)),\n",
    "                            torch.nn.functional.one_hot(b_val_seg, len(training_dataset.label_tags)),\n",
    "                            one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        # Get mean score over batch\n",
    "                        val_dices.append(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=True))\n",
    "\n",
    "                        val_class_dices.append(get_batch_dice_per_class(\n",
    "                            b_val_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                        if config.do_plot:\n",
    "                            print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                            print(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=False))\n",
    "                            # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                            display_seg(in_type=\"single_3D\",\n",
    "                                reduce_dim=\"W\",\n",
    "                                img=val_sample['image'].unsqueeze(0).cpu(),\n",
    "                                seg=val_logits_for_score_3d.squeeze(0).cpu(), # CHECK TODO\n",
    "                                ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                crop_to_non_zero_seg=True,\n",
    "                                crop_to_non_zero_gt=True,\n",
    "                                alpha_seg=.3,\n",
    "                                alpha_gt=.0\n",
    "                            )\n",
    "\n",
    "                    mean_val_dice = np.nanmean(val_dices)\n",
    "                    print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                    wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                    log_class_dices(\"scores/val_dice_mean_\", f\"_fold{fold_idx}\", val_class_dices, global_idx)\n",
    "\n",
    "            print()\n",
    "            # End of training loop\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        if str(config.data_param_mode) == str(DataParamMode.INSTANCE_PARAMS):\n",
    "            # Write sample data\n",
    "            save_dict = {}\n",
    "\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            all_idxs = torch.tensor(range(len(training_dataset))).cuda()\n",
    "            train_label_snapshot_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/train_label_snapshot.pth\")\n",
    "            seg_viz_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weighted_samples.png\")\n",
    "\n",
    "            train_label_snapshot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            dp_weights = embedding(all_idxs)\n",
    "            save_data = []\n",
    "            data_generator = zip(\n",
    "                dp_weights[train_idxs], \\\n",
    "                disturbed_bool_vect[train_idxs],\n",
    "                torch.utils.data.Subset(training_dataset, train_idxs)\n",
    "            )\n",
    "\n",
    "            for dp_weight, disturb_flg, sample in data_generator:\n",
    "                data_tuple = ( \\\n",
    "                    dp_weight,\n",
    "                    bool(disturb_flg.item()),\n",
    "                    sample['id'],\n",
    "                    sample['dataset_idx'],\n",
    "                    # sample['image'],\n",
    "                    sample['label'].to_sparse(),\n",
    "                    sample['modified_label'].to_sparse(),\n",
    "                    inference_wrap(lraspp, sample['image'].cuda(), use_2d=training_dataset.use_2d(), use_mind=config.use_mind).to_sparse()\n",
    "                )\n",
    "                save_data.append(data_tuple)\n",
    "\n",
    "            save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "            (dp_weight, disturb_flags,\n",
    "                d_ids, dataset_idxs,\n",
    "            #  _imgs,\n",
    "                _labels, _modified_labels, _predictions) = zip(*save_data)\n",
    "\n",
    "            dp_weight = torch.stack(dp_weight)\n",
    "            dataset_idxs = torch.stack(dataset_idxs)\n",
    "\n",
    "            save_dict.update(\n",
    "                {\n",
    "                    'data_parameters': dp_weight.cpu(),\n",
    "                    'disturb_flags': disturb_flags,\n",
    "                    'd_ids': d_ids,\n",
    "                    'dataset_idxs': dataset_idxs.cpu(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if config.save_labels:\n",
    "                _labels = torch.stack(_labels)\n",
    "                _modified_labels = torch.stack(_modified_labels)\n",
    "                _predictions = torch.stack(_predictions)\n",
    "                save_dict.update(\n",
    "                    {\n",
    "                        'labels': _labels.cpu(),\n",
    "                        'modified_labels': _modified_labels.cpu(),\n",
    "                        'train_predictions': _predictions.cpu()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            print(f\"Writing data parameters output to '{train_label_snapshot_path}'\")\n",
    "            torch.save(save_dict, train_label_snapshot_path)\n",
    "\n",
    "            if len(training_dataset.disturbed_idxs) > 0:\n",
    "                # Log histogram\n",
    "                separated_params = list(zip(dp_weights[clean_idxs], dp_weights[training_dataset.disturbed_idxs]))\n",
    "                s_table = wandb.Table(columns=['clean_idxs', 'disturbed_idxs'], data=separated_params)\n",
    "                fields = {\"primary_bins\": \"clean_idxs\", \"secondary_bins\": \"disturbed_idxs\", \"title\": \"Data parameter composite histogram\"}\n",
    "                composite_histogram = wandb.plot_table(vega_spec_name=\"rap1ide/composite_histogram\", data_table=s_table, fields=fields)\n",
    "                wandb.log({f\"data_parameters/separated_params_fold_{fold_idx}\": composite_histogram})\n",
    "\n",
    "            # Write out data of modified and un-modified labels and an overview image\n",
    "\n",
    "            if training_dataset.use_2d():\n",
    "                reduce_dim = None\n",
    "                in_type = \"batch_2D\"\n",
    "                skip_writeout = len(training_dataset) > 3000\n",
    "            else:\n",
    "                reduce_dim = \"W\"\n",
    "                in_type = \"batch_3D\"\n",
    "                skip_writeout = len(training_dataset) > 150\n",
    "            skip_writeout = True\n",
    "\n",
    "            if not skip_writeout:\n",
    "                print(\"Writing train sample image.\")\n",
    "                # overlay text example: d_idx=0, dp_i=1.00, dist? False\n",
    "                overlay_text_list = [f\"id:{d_id} dp:{instance_p.item():.2f}\" \\\n",
    "                    for d_id, instance_p, disturb_flg in zip(d_ids, dp_weight, disturb_flags)]\n",
    "\n",
    "                use_2d = training_dataset.use_2d()\n",
    "                scf = 1/training_dataset.pre_interpolation_factor\n",
    "\n",
    "                show_img = interpolate_sample(b_label=_labels.to_dense(), scale_factor=scf, use_2d=use_2d)[1].unsqueeze(1)\n",
    "                show_seg = interpolate_sample(b_label=_predictions.to_dense().squeeze(1), scale_factor=scf, use_2d=use_2d)[1]\n",
    "                show_gt = interpolate_sample(b_label=_modified_labels.to_dense(), scale_factor=scf, use_2d=use_2d)[1]\n",
    "\n",
    "                visualize_seg(in_type=in_type, reduce_dim=reduce_dim,\n",
    "                    img=show_img, # Expert label in BW\n",
    "                    seg=4*show_seg, # Prediction in blue\n",
    "                    ground_truth=show_gt, # Modified label in red\n",
    "                    crop_to_non_zero_seg=False,\n",
    "                    alpha_seg = .5,\n",
    "                    alpha_gt = .5,\n",
    "                    n_per_row=70,\n",
    "                    overlay_text=overlay_text_list,\n",
    "                    annotate_color=(0,255,255),\n",
    "                    frame_elements=disturb_flags,\n",
    "                    file_path=seg_viz_out_path,\n",
    "                )\n",
    "\n",
    "        # End of fold loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779c09b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Config overrides\n",
    "# config_dict['wandb_mode'] = 'disabled'\n",
    "# config_dict['debug'] = True\n",
    "# Model loading\n",
    "# config_dict['checkpoint_name'] = 'ethereal-serenity-1138'\n",
    "# config_dict['fold_override'] = 0\n",
    "# config_dict['checkpoint_epx'] = 39\n",
    "\n",
    "# Define sweep override dict\n",
    "sweep_config_dict = dict(\n",
    "    method='grid',\n",
    "    metric=dict(goal='maximize', name='scores/val_dice_mean_tumour_fold0'),\n",
    "    parameters=dict(\n",
    "        # disturbance_mode=dict(\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        # disturbance_strength=dict(\n",
    "        #     values=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "        # ),\n",
    "        # disturbed_percentage=dict(\n",
    "        #     values=[0.3, 0.6]\n",
    "        # ),\n",
    "        # data_param_mode=dict(\n",
    "        #     values=[\n",
    "        #         DataParamMode.INSTANCE_PARAMS,\n",
    "        #         DataParamMode.DISABLED,\n",
    "        #     ]\n",
    "        # ),\n",
    "        use_risk_regularization=dict(\n",
    "            values=[False, True]\n",
    "        ),\n",
    "        use_fixed_weighting=dict(\n",
    "            values=[False, True]\n",
    "        ),\n",
    "        # fixed_weight_min_quantile=dict(\n",
    "        #     values=[0.9, 0.8, 0.6, 0.4, 0.2, 0.0]\n",
    "        # ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "549637cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrap1ide\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rap1ide/curriculum_deeplab/runs/2tdi1vm1\" target=\"_blank\">prosperous-chrysanthemum-1269</a></strong> to <a href=\"https://wandb.ai/rap1ide/curriculum_deeplab\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normal_run():\n",
    "    with wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        ) as run:\n",
    "\n",
    "        run_name = run.name\n",
    "        print(\"Running\", run_name)\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init() as run:\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "\n",
    "        run_name = run.name\n",
    "        print(\"Running\", run_name)\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "\n",
    "if config_dict['do_sweep']:\n",
    "    # Integrate all config_dict entries into sweep_dict.parameters -> sweep overrides config_dict\n",
    "    cp_config_dict = copy.deepcopy(config_dict)\n",
    "    # cp_config_dict.update(copy.deepcopy(sweep_config_dict['parameters']))\n",
    "    for del_key in sweep_config_dict['parameters'].keys():\n",
    "        if del_key in cp_config_dict:\n",
    "            del cp_config_dict[del_key]\n",
    "    merged_sweep_config_dict = copy.deepcopy(sweep_config_dict)\n",
    "    # merged_sweep_config_dict.update(cp_config_dict)\n",
    "    for key, value in cp_config_dict.items():\n",
    "        merged_sweep_config_dict['parameters'][key] = dict(value=value)\n",
    "    # Convert enum values in parameters to string. They will be identified by their numerical index otherwise\n",
    "    for key, param_dict in merged_sweep_config_dict['parameters'].items():\n",
    "        if 'value' in param_dict and isinstance(param_dict['value'], Enum):\n",
    "            param_dict['value'] = str(param_dict['value'])\n",
    "        if 'values' in param_dict:\n",
    "            param_dict['values'] = [str(elem) if isinstance(elem, Enum) else elem for elem in param_dict['values']]\n",
    "\n",
    "        merged_sweep_config_dict['parameters'][key] = param_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(merged_sweep_config_dict, project=\"curriculum_deeplab\")\n",
    "    wandb.agent(sweep_id, function=sweep_run)\n",
    "\n",
    "else:\n",
    "    normal_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2244b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading registered data.\n",
      "Loading CrossMoDa hrT2 images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60 images, 60 labels: 100%|██████████| 120/120 [00:11<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding label data with modified_3d_label_override from 60 to 1800 labels\n",
      "Postprocessing 3D volumes\n",
      "Removed 0 3D images in postprocessing\n",
      "Equal image and label numbers: True (1800)\n",
      "Image shape: torch.Size([1800, 128, 128, 50]), mean.: 0.00, std.: 1.00\n",
      "Label shape: torch.Size([1800, 128, 128, 50]), max.: 1\n",
      "Data import finished.\n",
      "CrossMoDa loader will yield 3D samples\n"
     ]
    }
   ],
   "source": [
    "if not in_notebook():\n",
    "    sys.exit(0)\n",
    "\n",
    "d_set = prepare_data(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04ac42f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run validation with these 3D samples (#0): []\n"
     ]
    }
   ],
   "source": [
    "config = config_dict\n",
    "training_dataset = d_set\n",
    "all_3d_ids = training_dataset.get_3d_ids()\n",
    "if config.debug:\n",
    "    num_val_images = 2\n",
    "    atlas_count = 1\n",
    "else:\n",
    "    num_val_images = 00\n",
    "    atlas_count = 30 #TODO automate\n",
    "\n",
    "if config.use_2d_normal_to is not None:\n",
    "    # Override idxs\n",
    "    all_3d_ids = training_dataset.get_3d_ids()\n",
    "\n",
    "    val_3d_idxs = torch.tensor(list(range(0, num_val_images*atlas_count, atlas_count)))\n",
    "    val_3d_ids = training_dataset.switch_3d_identifiers(val_3d_idxs)\n",
    "\n",
    "    train_3d_idxs = list(range(num_val_images*atlas_count, len(all_3d_ids)))\n",
    "\n",
    "    # Get corresponding 2D idxs\n",
    "    train_2d_ids = []\n",
    "    dcts = training_dataset.get_id_dicts()\n",
    "    for id_dict in dcts:\n",
    "        _2d_id = id_dict['2d_id']\n",
    "        _3d_idx = id_dict['3d_dataset_idx']\n",
    "        if _2d_id in training_dataset.label_data_2d.keys() and _3d_idx in train_3d_idxs:\n",
    "            train_2d_ids.append(_2d_id)\n",
    "\n",
    "    train_2d_idxs = training_dataset.switch_2d_identifiers(train_2d_ids)\n",
    "    train_idxs = torch.tensor(train_2d_idxs)\n",
    "\n",
    "else:\n",
    "    val_3d_idxs = torch.tensor(list(range(0, num_val_images*atlas_count, atlas_count)))\n",
    "    val_3d_ids = training_dataset.switch_3d_identifiers(val_3d_idxs)\n",
    "\n",
    "    train_3d_idxs = list(range(num_val_images*atlas_count, len(all_3d_ids)))\n",
    "    train_idxs = torch.tensor(train_3d_idxs)\n",
    "\n",
    "print(f\"Will run validation with these 3D samples (#{len(val_3d_ids)}):\", sorted(val_3d_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f77cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['108r:m100r', '108r:m101r', '108r:m011r', '108r:m012r', '108r:m016r', '108r:m017r', '108r:m019r', '108r:m001r', '108r:m023r', '108r:m024r', '108r:m025r', '108r:m027r', '108r:m029r', '108r:m030r', '108r:m032r', '108r:m033r', '108r:m035r', '108r:m036r', '108r:m037r', '108r:m039r', '108r:m003r', '108r:m040r', '108r:m042l', '108r:m044r', '108r:m045r', '108r:m046r', '108r:m047r', '108r:m048r', '108r:m004r', '108r:m050r', '111l:m018l', '111l:m102l', '111l:m103l', '111l:m104l', '111l:m105l', '111l:m010l', '111l:m013l', '111l:m020l', '111l:m014l', '111l:m015l', '111l:m026l', '111l:m028l', '111l:m055l', '111l:m022l', '111l:m002l', '111l:m031l', '111l:m034l', '111l:m038l', '111l:m041l', '111l:m042l', '111l:m043l', '111l:m049l', '111l:m051l', '111l:m021l', '111l:m061l', '111l:m062l', '111l:m063l', '111l:m064l', '111l:m058l', '111l:m056l', '112r:m100r', '112r:m101r', '112r:m011r', '112r:m012r', '112r:m016r', '112r:m017r', '112r:m019r', '112r:m001r', '112r:m023r', '112r:m024r', '112r:m025r', '112r:m027r', '112r:m029r', '112r:m030r', '112r:m032r', '112r:m033r', '112r:m035r', '112r:m036r', '112r:m037r', '112r:m039r', '112r:m003r', '112r:m040r', '112r:m042l', '112r:m044r', '112r:m045r', '112r:m046r', '112r:m047r', '112r:m048r', '112r:m004r', '112r:m050r', '115l:m013l', '115l:m102l', '115l:m103l', '115l:m104l', '115l:m105l', '115l:m010l', '115l:m018l', '115l:m020l', '115l:m021l', '115l:m022l', '115l:m026l', '115l:m028l', '115l:m002l', '115l:m042l', '115l:m015l', '115l:m031l', '115l:m034l', '115l:m038l', '115l:m041l', '115l:m014l', '115l:m051l', '115l:m055l', '115l:m056l', '115l:m058l', '115l:m061l', '115l:m062l', '115l:m063l', '115l:m064l', '115l:m049l', '115l:m043l', '118r:m100r', '118r:m101r', '118r:m011r', '118r:m012r', '118r:m016r', '118r:m017r', '118r:m019r', '118r:m001r', '118r:m023r', '118r:m024r', '118r:m025r', '118r:m027r', '118r:m029r', '118r:m030r', '118r:m032r', '118r:m033r', '118r:m035r', '118r:m036r', '118r:m037r', '118r:m039r', '118r:m003r', '118r:m040r', '118r:m042l', '118r:m044r', '118r:m045r', '118r:m046r', '118r:m047r', '118r:m048r', '118r:m004r', '118r:m050r', '120r:m100r', '120r:m101r', '120r:m011r', '120r:m012r', '120r:m016r', '120r:m017r', '120r:m019r', '120r:m001r', '120r:m023r', '120r:m024r', '120r:m025r', '120r:m027r', '120r:m029r', '120r:m030r', '120r:m032r', '120r:m033r', '120r:m035r', '120r:m036r', '120r:m037r', '120r:m039r', '120r:m003r', '120r:m040r', '120r:m042l', '120r:m044r', '120r:m045r', '120r:m046r', '120r:m047r', '120r:m048r', '120r:m004r', '120r:m050r', '117l:m102l', '117l:m104l', '117l:m103l', '117l:m105l', '117l:m010l', '117l:m013l', '117l:m014l', '117l:m015l', '117l:m018l', '117l:m020l', '117l:m021l', '117l:m022l', '117l:m026l', '117l:m028l', '117l:m002l', '117l:m063l', '117l:m034l', '117l:m038l', '117l:m041l', '117l:m042l', '117l:m043l', '117l:m049l', '117l:m051l', '117l:m055l', '117l:m056l', '117l:m058l', '117l:m061l', '117l:m062l', '117l:m031l', '117l:m064l', '123r:m100r', '123r:m101r', '123r:m011r', '123r:m012r', '123r:m016r', '123r:m017r', '123r:m019r', '123r:m001r', '123r:m023r', '123r:m024r', '123r:m025r', '123r:m027r', '123r:m029r', '123r:m030r', '123r:m032r', '123r:m033r', '123r:m035r', '123r:m036r', '123r:m037r', '123r:m039r', '123r:m003r', '123r:m040r', '123r:m042l', '123r:m044r', '123r:m045r', '123r:m046r', '123r:m047r', '123r:m048r', '123r:m004r', '123r:m050r', '127r:m100r', '127r:m101r', '127r:m011r', '127r:m012r', '127r:m016r', '127r:m017r', '127r:m019r', '127r:m001r', '127r:m023r', '127r:m024r', '127r:m025r', '127r:m027r', '127r:m029r', '127r:m030r', '127r:m032r', '127r:m033r', '127r:m035r', '127r:m036r', '127r:m037r', '127r:m039r', '127r:m003r', '127r:m040r', '127r:m042l', '127r:m044r', '127r:m045r', '127r:m046r', '127r:m047r', '127r:m048r', '127r:m004r', '127r:m050r', '125l:m102l', '125l:m103l', '125l:m021l', '125l:m104l', '125l:m105l', '125l:m010l', '125l:m013l', '125l:m014l', '125l:m015l', '125l:m018l', '125l:m020l', '125l:m028l', '125l:m002l', '125l:m031l', '125l:m034l', '125l:m038l', '125l:m056l', '125l:m026l', '125l:m041l', '125l:m042l', '125l:m043l', '125l:m049l', '125l:m051l', '125l:m055l', '125l:m022l', '125l:m062l', '125l:m063l', '125l:m064l', '125l:m061l', '125l:m058l', '134r:m100r', '134r:m101r', '134r:m011r', '134r:m012r', '134r:m016r', '134r:m017r', '134r:m019r', '134r:m001r', '134r:m023r', '134r:m024r', '134r:m025r', '134r:m027r', '134r:m029r', '134r:m030r', '134r:m032r', '134r:m033r', '134r:m035r', '134r:m036r', '134r:m037r', '134r:m039r', '134r:m003r', '134r:m040r', '134r:m042l', '134r:m044r', '134r:m045r', '134r:m046r', '134r:m047r', '134r:m048r', '134r:m004r', '134r:m050r', '135r:m100r', '135r:m101r', '135r:m011r', '135r:m012r', '135r:m016r', '135r:m017r', '135r:m019r', '135r:m001r', '135r:m023r', '135r:m024r', '135r:m025r', '135r:m027r', '135r:m029r', '135r:m030r', '135r:m032r', '135r:m033r', '135r:m035r', '135r:m036r', '135r:m037r', '135r:m039r', '135r:m003r', '135r:m040r', '135r:m042l', '135r:m044r', '135r:m045r', '135r:m046r', '135r:m047r', '135r:m048r', '135r:m004r', '135r:m050r', '126l:m102l', '126l:m103l', '126l:m104l', '126l:m014l', '126l:m105l', '126l:m010l', '126l:m013l', '126l:m020l', '126l:m021l', '126l:m022l', '126l:m026l', '126l:m028l', '126l:m002l', '126l:m031l', '126l:m034l', '126l:m038l', '126l:m041l', '126l:m043l', '126l:m018l', '126l:m042l', '126l:m015l', '126l:m051l', '126l:m055l', '126l:m056l', '126l:m058l', '126l:m061l', '126l:m062l', '126l:m063l', '126l:m064l', '126l:m049l', '142r:m100r', '142r:m101r', '142r:m011r', '142r:m012r', '142r:m016r', '142r:m017r', '142r:m019r', '142r:m001r', '142r:m023r', '142r:m024r', '142r:m025r', '142r:m027r', '142r:m029r', '142r:m030r', '142r:m032r', '142r:m033r', '142r:m035r', '142r:m036r', '142r:m037r', '142r:m039r', '142r:m003r', '142r:m040r', '142r:m042l', '142r:m044r', '142r:m045r', '142r:m046r', '142r:m047r', '142r:m048r', '142r:m004r', '142r:m050r', '144r:m100r', '144r:m101r', '144r:m011r', '144r:m012r', '144r:m016r', '144r:m017r', '144r:m019r', '144r:m001r', '144r:m023r', '144r:m024r', '144r:m025r', '144r:m027r', '144r:m029r', '144r:m030r', '144r:m032r', '144r:m033r', '144r:m035r', '144r:m036r', '144r:m037r', '144r:m039r', '144r:m003r', '144r:m040r', '144r:m042l', '144r:m044r', '144r:m045r', '144r:m046r', '144r:m047r', '144r:m048r', '144r:m004r', '144r:m050r', '133l:m102l', '133l:m103l', '133l:m104l', '133l:m031l', '133l:m105l', '133l:m010l', '133l:m013l', '133l:m014l', '133l:m015l', '133l:m018l', '133l:m020l', '133l:m021l', '133l:m022l', '133l:m026l', '133l:m028l', '133l:m002l', '133l:m041l', '133l:m042l', '133l:m064l', '133l:m038l', '133l:m043l', '133l:m049l', '133l:m051l', '133l:m055l', '133l:m056l', '133l:m058l', '133l:m061l', '133l:m062l', '133l:m063l', '133l:m034l', '148r:m100r', '148r:m101r', '148r:m011r', '148r:m012r', '148r:m016r', '148r:m017r', '148r:m019r', '148r:m001r', '148r:m023r', '148r:m024r', '148r:m025r', '148r:m027r', '148r:m029r', '148r:m030r', '148r:m032r', '148r:m033r', '148r:m035r', '148r:m036r', '148r:m037r', '148r:m039r', '148r:m003r', '148r:m040r', '148r:m042l', '148r:m044r', '148r:m045r', '148r:m046r', '148r:m047r', '148r:m048r', '148r:m004r', '148r:m050r', '154r:m100r', '154r:m101r', '154r:m011r', '154r:m012r', '154r:m016r', '154r:m017r', '154r:m019r', '154r:m001r', '154r:m023r', '154r:m024r', '154r:m025r', '154r:m027r', '154r:m029r', '154r:m030r', '154r:m032r', '154r:m033r', '154r:m035r', '154r:m036r', '154r:m037r', '154r:m039r', '154r:m003r', '154r:m040r', '154r:m042l', '154r:m044r', '154r:m045r', '154r:m046r', '154r:m047r', '154r:m048r', '154r:m004r', '154r:m050r', '136l:m103l', '136l:m104l', '136l:m105l', '136l:m010l', '136l:m102l', '136l:m022l', '136l:m013l', '136l:m014l', '136l:m015l', '136l:m018l', '136l:m020l', '136l:m021l', '136l:m002l', '136l:m031l', '136l:m034l', '136l:m038l', '136l:m041l', '136l:m042l', '136l:m043l', '136l:m058l', '136l:m028l', '136l:m049l', '136l:m051l', '136l:m055l', '136l:m056l', '136l:m026l', '136l:m063l', '136l:m064l', '136l:m062l', '136l:m061l', '160r:m100r', '160r:m101r', '160r:m011r', '160r:m012r', '160r:m016r', '160r:m017r', '160r:m019r', '160r:m001r', '160r:m023r', '160r:m024r', '160r:m025r', '160r:m027r', '160r:m029r', '160r:m030r', '160r:m032r', '160r:m033r', '160r:m035r', '160r:m036r', '160r:m037r', '160r:m039r', '160r:m003r', '160r:m040r', '160r:m042l', '160r:m044r', '160r:m045r', '160r:m046r', '160r:m047r', '160r:m048r', '160r:m004r', '160r:m050r', '165r:m100r', '165r:m101r', '165r:m011r', '165r:m012r', '165r:m016r', '165r:m017r', '165r:m019r', '165r:m001r', '165r:m023r', '165r:m024r', '165r:m025r', '165r:m027r', '165r:m029r', '165r:m030r', '165r:m032r', '165r:m033r', '165r:m035r', '165r:m036r', '165r:m037r', '165r:m039r', '165r:m003r', '165r:m040r', '165r:m042l', '165r:m044r', '165r:m045r', '165r:m046r', '165r:m047r', '165r:m048r', '165r:m004r', '165r:m050r', '140l:m102l', '140l:m103l', '140l:m104l', '140l:m105l', '140l:m010l', '140l:m013l', '140l:m015l', '140l:m014l', '140l:m020l', '140l:m021l', '140l:m022l', '140l:m026l', '140l:m028l', '140l:m002l', '140l:m031l', '140l:m034l', '140l:m038l', '140l:m041l', '140l:m042l', '140l:m043l', '140l:m018l', '140l:m051l', '140l:m055l', '140l:m056l', '140l:m058l', '140l:m061l', '140l:m062l', '140l:m063l', '140l:m064l', '140l:m049l', '166r:m100r', '166r:m101r', '166r:m011r', '166r:m012r', '166r:m016r', '166r:m017r', '166r:m019r', '166r:m001r', '166r:m023r', '166r:m024r', '166r:m025r', '166r:m027r', '166r:m029r', '166r:m030r', '166r:m032r', '166r:m033r', '166r:m035r', '166r:m036r', '166r:m037r', '166r:m039r', '166r:m003r', '166r:m040r', '166r:m042l', '166r:m044r', '166r:m045r', '166r:m046r', '166r:m047r', '166r:m048r', '166r:m004r', '166r:m050r', '167r:m100r', '167r:m101r', '167r:m011r', '167r:m012r', '167r:m016r', '167r:m017r', '167r:m019r', '167r:m001r', '167r:m023r', '167r:m024r', '167r:m025r', '167r:m027r', '167r:m029r', '167r:m030r', '167r:m032r', '167r:m033r', '167r:m035r', '167r:m036r', '167r:m037r', '167r:m039r', '167r:m003r', '167r:m040r', '167r:m042l', '167r:m044r', '167r:m045r', '167r:m046r', '167r:m047r', '167r:m048r', '167r:m004r', '167r:m050r', '141l:m102l', '141l:m103l', '141l:m104l', '141l:m013l', '141l:m014l', '141l:m034l', '141l:m010l', '141l:m015l', '141l:m018l', '141l:m020l', '141l:m021l', '141l:m022l', '141l:m026l', '141l:m028l', '141l:m002l', '141l:m031l', '141l:m105l', '141l:m042l', '141l:m043l', '141l:m049l', '141l:m051l', '141l:m055l', '141l:m041l', '141l:m056l', '141l:m058l', '141l:m061l', '141l:m062l', '141l:m063l', '141l:m064l', '141l:m038l', '168r:m100r', '168r:m101r', '168r:m011r', '168r:m012r', '168r:m016r', '168r:m017r', '168r:m019r', '168r:m001r', '168r:m023r', '168r:m024r', '168r:m025r', '168r:m027r', '168r:m029r', '168r:m030r', '168r:m032r', '168r:m033r', '168r:m035r', '168r:m036r', '168r:m037r', '168r:m039r', '168r:m003r', '168r:m040r', '168r:m042l', '168r:m044r', '168r:m045r', '168r:m046r', '168r:m047r', '168r:m048r', '168r:m004r', '168r:m050r', '171r:m100r', '171r:m101r', '171r:m011r', '171r:m012r', '171r:m016r', '171r:m017r', '171r:m019r', '171r:m001r', '171r:m023r', '171r:m024r', '171r:m025r', '171r:m027r', '171r:m029r', '171r:m030r', '171r:m032r', '171r:m033r', '171r:m035r', '171r:m036r', '171r:m037r', '171r:m039r', '171r:m003r', '171r:m040r', '171r:m042l', '171r:m044r', '171r:m045r', '171r:m046r', '171r:m047r', '171r:m048r', '171r:m004r', '171r:m050r', '143l:m102l', '143l:m104l', '143l:m105l', '143l:m010l', '143l:m013l', '143l:m014l', '143l:m015l', '143l:m026l', '143l:m103l', '143l:m018l', '143l:m020l', '143l:m021l', '143l:m022l', '143l:m031l', '143l:m034l', '143l:m038l', '143l:m041l', '143l:m042l', '143l:m043l', '143l:m049l', '143l:m051l', '143l:m055l', '143l:m056l', '143l:m061l', '143l:m002l', '143l:m058l', '143l:m028l', '143l:m063l', '143l:m064l', '143l:m062l', '173r:m100r', '173r:m101r', '173r:m011r', '173r:m012r', '173r:m016r', '173r:m017r', '173r:m019r', '173r:m001r', '173r:m023r', '173r:m024r', '173r:m025r', '173r:m027r', '173r:m029r', '173r:m030r', '173r:m032r', '173r:m033r', '173r:m035r', '173r:m036r', '173r:m037r', '173r:m039r', '173r:m003r', '173r:m040r', '173r:m042l', '173r:m044r', '173r:m045r', '173r:m046r', '173r:m047r', '173r:m048r', '173r:m004r', '173r:m050r', '174r:m100r', '174r:m101r', '174r:m011r', '174r:m012r', '174r:m016r', '174r:m017r', '174r:m019r', '174r:m001r', '174r:m023r', '174r:m024r', '174r:m025r', '174r:m027r', '174r:m029r', '174r:m030r', '174r:m032r', '174r:m033r', '174r:m035r', '174r:m036r', '174r:m037r', '174r:m039r', '174r:m003r', '174r:m040r', '174r:m042l', '174r:m044r', '174r:m045r', '174r:m046r', '174r:m047r', '174r:m048r', '174r:m004r', '174r:m050r', '145l:m102l', '145l:m103l', '145l:m104l', '145l:m105l', '145l:m010l', '145l:m013l', '145l:m014l', '145l:m015l', '145l:m049l', '145l:m020l', '145l:m021l', '145l:m022l', '145l:m026l', '145l:m028l', '145l:m002l', '145l:m031l', '145l:m034l', '145l:m038l', '145l:m041l', '145l:m042l', '145l:m043l', '145l:m018l', '145l:m056l', '145l:m058l', '145l:m055l', '145l:m061l', '145l:m062l', '145l:m063l', '145l:m064l', '145l:m051l', '179r:m100r', '179r:m101r', '179r:m011r', '179r:m012r', '179r:m016r', '179r:m017r', '179r:m019r', '179r:m001r', '179r:m023r', '179r:m024r', '179r:m025r', '179r:m027r', '179r:m029r', '179r:m030r', '179r:m032r', '179r:m033r', '179r:m035r', '179r:m036r', '179r:m037r', '179r:m039r', '179r:m003r', '179r:m040r', '179r:m042l', '179r:m044r', '179r:m045r', '179r:m046r', '179r:m047r', '179r:m048r', '179r:m004r', '179r:m050r', '146l:m105l', '146l:m102l', '146l:m103l', '146l:m104l', '146l:m014l', '146l:m015l', '146l:m018l', '146l:m020l', '146l:m021l', '146l:m038l', '146l:m013l', '146l:m022l', '146l:m026l', '146l:m028l', '146l:m002l', '146l:m031l', '146l:m034l', '146l:m010l', '146l:m043l', '146l:m049l', '146l:m051l', '146l:m055l', '146l:m056l', '146l:m058l', '146l:m061l', '146l:m042l', '146l:m062l', '146l:m063l', '146l:m064l', '146l:m041l', '180r:m100r', '180r:m101r', '180r:m011r', '180r:m012r', '180r:m016r', '180r:m017r', '180r:m019r', '180r:m001r', '180r:m023r', '180r:m024r', '180r:m025r', '180r:m027r', '180r:m029r', '180r:m030r', '180r:m032r', '180r:m033r', '180r:m035r', '180r:m036r', '180r:m037r', '180r:m039r', '180r:m003r', '180r:m040r', '180r:m042l', '180r:m044r', '180r:m045r', '180r:m046r', '180r:m047r', '180r:m048r', '180r:m004r', '180r:m050r', '181r:m100r', '181r:m101r', '181r:m011r', '181r:m012r', '181r:m016r', '181r:m017r', '181r:m019r', '181r:m001r', '181r:m023r', '181r:m024r', '181r:m025r', '181r:m027r', '181r:m029r', '181r:m030r', '181r:m032r', '181r:m033r', '181r:m035r', '181r:m036r', '181r:m037r', '181r:m039r', '181r:m003r', '181r:m040r', '181r:m042l', '181r:m044r', '181r:m045r', '181r:m046r', '181r:m047r', '181r:m048r', '181r:m004r', '181r:m050r', '185r:m100r', '185r:m101r', '185r:m011r', '185r:m012r', '185r:m016r', '185r:m017r', '185r:m019r', '185r:m001r', '185r:m023r', '185r:m024r', '185r:m025r', '185r:m027r', '185r:m029r', '185r:m030r', '185r:m032r', '185r:m033r', '185r:m035r', '185r:m036r', '185r:m037r', '185r:m039r', '185r:m003r', '185r:m040r', '185r:m042l', '185r:m044r', '185r:m045r', '185r:m046r', '185r:m047r', '185r:m048r', '185r:m004r', '185r:m050r', '147l:m102l', '147l:m104l', '147l:m105l', '147l:m010l', '147l:m013l', '147l:m014l', '147l:m015l', '147l:m018l', '147l:m020l', '147l:m021l', '147l:m022l', '147l:m028l', '147l:m026l', '147l:m103l', '147l:m031l', '147l:m034l', '147l:m038l', '147l:m041l', '147l:m042l', '147l:m043l', '147l:m049l', '147l:m051l', '147l:m055l', '147l:m056l', '147l:m058l', '147l:m061l', '147l:m002l', '147l:m063l', '147l:m064l', '147l:m062l', '195r:m100r', '195r:m101r', '195r:m011r', '195r:m012r', '195r:m016r', '195r:m017r', '195r:m019r', '195r:m001r', '195r:m023r', '195r:m024r', '195r:m025r', '195r:m027r', '195r:m029r', '195r:m030r', '195r:m032r', '195r:m033r', '195r:m035r', '195r:m036r', '195r:m037r', '195r:m039r', '195r:m003r', '195r:m040r', '195r:m042l', '195r:m044r', '195r:m045r', '195r:m046r', '195r:m047r', '195r:m048r', '195r:m004r', '195r:m050r', '149l:m018l', '149l:m102l', '149l:m103l', '149l:m104l', '149l:m105l', '149l:m010l', '149l:m013l', '149l:m014l', '149l:m015l', '149l:m022l', '149l:m026l', '149l:m051l', '149l:m021l', '149l:m028l', '149l:m002l', '149l:m031l', '149l:m034l', '149l:m038l', '149l:m041l', '149l:m042l', '149l:m043l', '149l:m049l', '149l:m020l', '149l:m058l', '149l:m061l', '149l:m062l', '149l:m063l', '149l:m064l', '149l:m056l', '149l:m055l', '198r:m100r', '198r:m101r', '198r:m011r', '198r:m012r', '198r:m016r', '198r:m017r', '198r:m019r', '198r:m001r', '198r:m023r', '198r:m024r', '198r:m025r', '198r:m027r', '198r:m029r', '198r:m030r', '198r:m032r', '198r:m033r', '198r:m035r', '198r:m036r', '198r:m037r', '198r:m039r', '198r:m003r', '198r:m040r', '198r:m042l', '198r:m044r', '198r:m045r', '198r:m046r', '198r:m047r', '198r:m048r', '198r:m004r', '198r:m050r', '204r:m100r', '204r:m101r', '204r:m011r', '204r:m012r', '204r:m016r', '204r:m017r', '204r:m019r', '204r:m001r', '204r:m023r', '204r:m024r', '204r:m025r', '204r:m027r', '204r:m029r', '204r:m030r', '204r:m032r', '204r:m033r', '204r:m035r', '204r:m036r', '204r:m037r', '204r:m039r', '204r:m003r', '204r:m040r', '204r:m042l', '204r:m044r', '204r:m045r', '204r:m046r', '204r:m047r', '204r:m048r', '204r:m004r', '204r:m050r', '152l:m010l', '152l:m102l', '152l:m103l', '152l:m104l', '152l:m105l', '152l:m015l', '152l:m018l', '152l:m020l', '152l:m021l', '152l:m022l', '152l:m026l', '152l:m028l', '152l:m041l', '152l:m014l', '152l:m002l', '152l:m031l', '152l:m034l', '152l:m038l', '152l:m013l', '152l:m049l', '152l:m051l', '152l:m055l', '152l:m056l', '152l:m058l', '152l:m061l', '152l:m062l', '152l:m063l', '152l:m064l', '152l:m043l', '152l:m042l', '205r:m100r', '205r:m101r', '205r:m011r', '205r:m012r', '205r:m016r', '205r:m017r', '205r:m019r', '205r:m001r', '205r:m023r', '205r:m024r', '205r:m025r', '205r:m027r', '205r:m029r', '205r:m030r', '205r:m032r', '205r:m033r', '205r:m035r', '205r:m036r', '205r:m037r', '205r:m039r', '205r:m003r', '205r:m040r', '205r:m042l', '205r:m044r', '205r:m045r', '205r:m046r', '205r:m047r', '205r:m048r', '205r:m004r', '205r:m050r', '209r:m100r', '209r:m101r', '209r:m011r', '209r:m012r', '209r:m016r', '209r:m017r', '209r:m019r', '209r:m001r', '209r:m023r', '209r:m024r', '209r:m025r', '209r:m027r', '209r:m029r', '209r:m030r', '209r:m032r', '209r:m033r', '209r:m035r', '209r:m036r', '209r:m037r', '209r:m039r', '209r:m003r', '209r:m040r', '209r:m042l', '209r:m044r', '209r:m045r', '209r:m046r', '209r:m047r', '209r:m048r', '209r:m004r', '209r:m050r', '158l:m103l', '158l:m102l', '158l:m104l', '158l:m105l', '158l:m010l', '158l:m013l', '158l:m014l', '158l:m015l', '158l:m018l', '158l:m020l', '158l:m021l', '158l:m022l', '158l:m026l', '158l:m028l', '158l:m002l', '158l:m031l', '158l:m034l', '158l:m038l', '158l:m041l', '158l:m042l', '158l:m043l', '158l:m049l', '158l:m051l', '158l:m055l', '158l:m056l', '158l:m058l', '158l:m061l', '158l:m062l', '158l:m063l', '158l:m064l', '210r:m100r', '210r:m101r', '210r:m011r', '210r:m012r', '210r:m016r', '210r:m017r', '210r:m019r', '210r:m001r', '210r:m023r', '210r:m024r', '210r:m025r', '210r:m027r', '210r:m029r', '210r:m030r', '210r:m032r', '210r:m033r', '210r:m035r', '210r:m036r', '210r:m037r', '210r:m039r', '210r:m003r', '210r:m040r', '210r:m042l', '210r:m044r', '210r:m045r', '210r:m046r', '210r:m047r', '210r:m048r', '210r:m004r', '210r:m050r', '162l:m102l', '162l:m103l', '162l:m104l', '162l:m105l', '162l:m010l', '162l:m013l', '162l:m014l', '162l:m015l', '162l:m018l', '162l:m020l', '162l:m021l', '162l:m022l', '162l:m026l', '162l:m028l', '162l:m002l', '162l:m031l', '162l:m034l', '162l:m038l', '162l:m041l', '162l:m042l', '162l:m043l', '162l:m049l', '162l:m051l', '162l:m055l', '162l:m056l', '162l:m058l', '162l:m061l', '162l:m062l', '162l:m063l', '162l:m064l', '164l:m102l', '164l:m103l', '164l:m104l', '164l:m105l', '164l:m010l', '164l:m013l', '164l:m014l', '164l:m015l', '164l:m018l', '164l:m020l', '164l:m021l', '164l:m022l', '164l:m026l', '164l:m028l', '164l:m002l', '164l:m031l', '164l:m034l', '164l:m038l', '164l:m041l', '164l:m042l', '164l:m043l', '164l:m049l', '164l:m051l', '164l:m055l', '164l:m056l', '164l:m058l', '164l:m061l', '164l:m062l', '164l:m063l', '164l:m064l', '175l:m102l', '175l:m103l', '175l:m104l', '175l:m105l', '175l:m010l', '175l:m013l', '175l:m014l', '175l:m015l', '175l:m018l', '175l:m020l', '175l:m021l', '175l:m022l', '175l:m026l', '175l:m028l', '175l:m002l', '175l:m031l', '175l:m034l', '175l:m038l', '175l:m041l', '175l:m042l', '175l:m043l', '175l:m049l', '175l:m051l', '175l:m055l', '175l:m056l', '175l:m058l', '175l:m061l', '175l:m062l', '175l:m063l', '175l:m064l', '177l:m102l', '177l:m103l', '177l:m104l', '177l:m105l', '177l:m010l', '177l:m013l', '177l:m014l', '177l:m015l', '177l:m018l', '177l:m020l', '177l:m021l', '177l:m022l', '177l:m026l', '177l:m028l', '177l:m002l', '177l:m031l', '177l:m034l', '177l:m038l', '177l:m041l', '177l:m042l', '177l:m043l', '177l:m049l', '177l:m051l', '177l:m055l', '177l:m056l', '177l:m058l', '177l:m061l', '177l:m062l', '177l:m063l', '177l:m064l', '178l:m102l', '178l:m103l', '178l:m104l', '178l:m105l', '178l:m010l', '178l:m013l', '178l:m014l', '178l:m015l', '178l:m018l', '178l:m020l', '178l:m021l', '178l:m022l', '178l:m026l', '178l:m028l', '178l:m002l', '178l:m031l', '178l:m034l', '178l:m038l', '178l:m041l', '178l:m042l', '178l:m043l', '178l:m049l', '178l:m051l', '178l:m055l', '178l:m056l', '178l:m058l', '178l:m061l', '178l:m062l', '178l:m063l', '178l:m064l', '183l:m102l', '183l:m103l', '183l:m104l', '183l:m105l', '183l:m010l', '183l:m013l', '183l:m014l', '183l:m015l', '183l:m018l', '183l:m020l', '183l:m021l', '183l:m022l', '183l:m026l', '183l:m028l', '183l:m002l', '183l:m031l', '183l:m034l', '183l:m038l', '183l:m041l', '183l:m042l', '183l:m043l', '183l:m049l', '183l:m051l', '183l:m055l', '183l:m056l', '183l:m058l', '183l:m061l', '183l:m062l', '183l:m063l', '183l:m064l', '187l:m102l', '187l:m103l', '187l:m104l', '187l:m105l', '187l:m010l', '187l:m013l', '187l:m014l', '187l:m015l', '187l:m018l', '187l:m020l', '187l:m021l', '187l:m022l', '187l:m026l', '187l:m028l', '187l:m002l', '187l:m031l', '187l:m034l', '187l:m038l', '187l:m041l', '187l:m042l', '187l:m043l', '187l:m049l', '187l:m051l', '187l:m055l', '187l:m056l', '187l:m058l', '187l:m061l', '187l:m062l', '187l:m063l', '187l:m064l', '188l:m102l', '188l:m103l', '188l:m104l', '188l:m105l', '188l:m010l', '188l:m013l', '188l:m014l', '188l:m015l', '188l:m018l', '188l:m020l', '188l:m021l', '188l:m022l', '188l:m026l', '188l:m028l', '188l:m002l', '188l:m031l', '188l:m034l', '188l:m038l', '188l:m041l', '188l:m042l', '188l:m043l', '188l:m049l', '188l:m051l', '188l:m055l', '188l:m056l', '188l:m058l', '188l:m061l', '188l:m062l', '188l:m063l', '188l:m064l', '189l:m102l', '189l:m103l', '189l:m104l', '189l:m105l', '189l:m010l', '189l:m013l', '189l:m014l', '189l:m015l', '189l:m018l', '189l:m020l', '189l:m021l', '189l:m022l', '189l:m026l', '189l:m028l', '189l:m002l', '189l:m031l', '189l:m034l', '189l:m038l', '189l:m041l', '189l:m042l', '189l:m043l', '189l:m049l', '189l:m051l', '189l:m055l', '189l:m056l', '189l:m058l', '189l:m061l', '189l:m062l', '189l:m063l', '189l:m064l', '190l:m102l', '190l:m103l', '190l:m104l', '190l:m105l', '190l:m010l', '190l:m013l', '190l:m014l', '190l:m015l', '190l:m018l', '190l:m020l', '190l:m021l', '190l:m022l', '190l:m026l', '190l:m028l', '190l:m002l', '190l:m031l', '190l:m034l', '190l:m038l', '190l:m041l', '190l:m042l', '190l:m043l', '190l:m049l', '190l:m051l', '190l:m055l', '190l:m056l', '190l:m058l', '190l:m061l', '190l:m062l', '190l:m063l', '190l:m064l', '192l:m102l', '192l:m103l', '192l:m104l', '192l:m105l', '192l:m010l', '192l:m013l', '192l:m014l', '192l:m015l', '192l:m018l', '192l:m020l', '192l:m021l', '192l:m022l', '192l:m026l', '192l:m028l', '192l:m002l', '192l:m031l', '192l:m034l', '192l:m038l', '192l:m041l', '192l:m042l', '192l:m043l', '192l:m049l', '192l:m051l', '192l:m055l', '192l:m056l', '192l:m058l', '192l:m061l', '192l:m062l', '192l:m063l', '192l:m064l', '199l:m102l', '199l:m103l', '199l:m104l', '199l:m105l', '199l:m010l', '199l:m013l', '199l:m014l', '199l:m015l', '199l:m018l', '199l:m020l', '199l:m021l', '199l:m022l', '199l:m026l', '199l:m028l', '199l:m002l', '199l:m031l', '199l:m034l', '199l:m038l', '199l:m041l', '199l:m042l', '199l:m043l', '199l:m049l', '199l:m051l', '199l:m055l', '199l:m056l', '199l:m058l', '199l:m061l', '199l:m062l', '199l:m063l', '199l:m064l', '200l:m102l', '200l:m103l', '200l:m104l', '200l:m105l', '200l:m010l', '200l:m013l', '200l:m014l', '200l:m015l', '200l:m018l', '200l:m020l', '200l:m021l', '200l:m022l', '200l:m026l', '200l:m028l', '200l:m002l', '200l:m031l', '200l:m034l', '200l:m038l', '200l:m041l', '200l:m042l', '200l:m043l', '200l:m049l', '200l:m051l', '200l:m055l', '200l:m056l', '200l:m058l', '200l:m061l', '200l:m062l', '200l:m063l', '200l:m064l', '202l:m102l', '202l:m103l', '202l:m104l', '202l:m105l', '202l:m010l', '202l:m013l', '202l:m014l', '202l:m015l', '202l:m018l', '202l:m020l', '202l:m021l', '202l:m022l', '202l:m026l', '202l:m028l', '202l:m002l', '202l:m031l', '202l:m034l', '202l:m038l', '202l:m041l', '202l:m042l', '202l:m043l', '202l:m049l', '202l:m051l', '202l:m055l', '202l:m056l', '202l:m058l', '202l:m061l', '202l:m062l', '202l:m063l', '202l:m064l']\n",
      "0 0 1800 1800\n"
     ]
    }
   ],
   "source": [
    "train_3d_ids = training_dataset.switch_3d_identifiers(train_3d_idxs)\n",
    "val_label_paths = {_id: training_dataset.img_paths[_id] for _id in val_3d_ids}\n",
    "val_image_paths = {_id: training_dataset.img_paths[_id] for _id in val_3d_ids}\n",
    "train_label_paths = {_id: training_dataset.label_paths[_id] for _id in train_3d_ids}\n",
    "train_image_paths = {_id: training_dataset.img_paths[_id] for _id in train_3d_ids}\n",
    "print(train_3d_ids)\n",
    "\n",
    "\n",
    "path_dict = {}\n",
    "path_dict['val_label_paths'] = val_label_paths\n",
    "path_dict['val_image_paths'] = val_image_paths\n",
    "path_dict['train_label_paths'] = train_label_paths\n",
    "path_dict['train_image_paths'] = train_image_paths\n",
    "print(len(val_label_paths), len(val_image_paths), len(train_label_paths), len(train_image_paths))\n",
    "torch.save(path_dict, f'network_dataset_path_dict_train_{len(train_label_paths)}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2bb163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
