{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3175666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name      Util    Mem free  Cuda             User(s)\n",
      "----  -----------  ------  ----------  ---------------  ---------\n",
      "   0  TITAN RTX       0 %   24199 MiB  11.2(460.73.01)  root\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name        torch\n",
      "----  -----------  --  -------\n",
      "   0  TITAN RTX    ->  cuda:0\n",
      "1.9.0a0+gitdfbd030\n",
      "8200\n",
      "TITAN RTX\n",
      "Running in: /share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "import glob\n",
    "import pickle\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import functools\n",
    "from enum import Enum, auto\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "import scipy\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from deep_staple.metrics import dice3d, dice2d\n",
    "from deep_staple.visualization import visualize_seg\n",
    "from deep_staple.mindssc import mindssc\n",
    "from deep_staple.CrossmodaHybridIdLoader import CrossmodaHybridIdLoader, get_crossmoda_data_load_closure\n",
    "from deep_staple.MobileNet_LR_ASPP_3D import MobileNet_LRASPP_3D, MobileNet_ASPP_3D\n",
    "from deep_staple.utils.torch_utils import get_batch_dice_per_class, get_batch_dice_over_all, get_2d_stack_batch_size, \\\n",
    "    make_2d_stack_from_3d, make_3d_from_2d_stack, interpolate_sample, dilate_label_class, ensure_dense, get_module, set_module, save_model, reset_determinism\n",
    "from deep_staple.utils.common_utils import DotDict, DataParamMode, LabelDisturbanceMode, in_notebook, get_script_dir\n",
    "from deep_staple.utils.log_utils import get_global_idx, log_data_parameter_stats, log_class_dices\n",
    "\n",
    "print(torch.__version__)\n",
    "try:\n",
    "    print(torch.backends.cudnn.version())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "except AssertionError as err:\n",
    "    print(f\"Cuda is not available. {err}\")\n",
    "\n",
    "THIS_SCRIPT_DIR = get_script_dir(__file__)\n",
    "print(f\"Running in: {THIS_SCRIPT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5750fa2f-e706-4c65-ac09-c3ab8050b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,                # If true do not contiue with training after the first fold\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "\n",
    "    'use_mind': False,                      # If true use MIND features (https://pubmed.ncbi.nlm.nih.gov/22722056/)\n",
    "    'epochs': 40,\n",
    "\n",
    "    'batch_size': 8,\n",
    "    'val_batch_size': 1,\n",
    "    'use_2d_normal_to': None,               # Can be None or 'D', 'H', 'W'. If not None 2D slices will be selected for training\n",
    "\n",
    "    'num_val_images': 20,\n",
    "    'atlas_count': 1,                       # If three (noisy) labels per image are used specify three\n",
    "\n",
    "    'dataset': 'crossmoda',                 # The dataset prepared with our preprocessing scripts\n",
    "    'dataset_directory': Path(THIS_SCRIPT_DIR, \"data/crossmoda_dataset\"),\n",
    "    'reg_state': \"acummulate_every_third_deeds_FT2_MT1\", # Registered (noisy) labels used in training. See prepare_data() for valid reg_states\n",
    "    'train_set_max_len': None,              # Length to cut of dataloader sample count\n",
    "    'crop_3d_w_dim_range': (45, 95),        # W-dimension range in which 3D samples are cropped\n",
    "    'crop_2d_slices_gt_num_threshold': 0,   # Drop 2D slices if less than threshold pixels are positive\n",
    "\n",
    "    'lr': 0.01,\n",
    "    'use_scheduling': True,\n",
    "\n",
    "    # Data parameter config\n",
    "    'data_param_mode': DataParamMode.INSTANCE_PARAMS, # or DataParamMode.DISABLED\n",
    "    'init_inst_param': 0.0,                           # Init value of data parameters\n",
    "    'lr_inst_param': 0.1,\n",
    "    'use_risk_regularization': True,                  # See paper\n",
    "    'use_fixed_weighting': True,                      # See paper\n",
    "    'use_ool_dp_loss': True,                          # See paper\n",
    "\n",
    "    # Extended config for loading pretrained data\n",
    "    'fixed_weight_file': None,                        # Specify path to a ./data/output/<training_run>/train_label_snapshot.pth to load pretrained data parameters\n",
    "    'fixed_weight_min_quantile': None,                # If .8 drop 80% of the noisiest samples\n",
    "    'fixed_weight_min_value': None,                   # If 2.3 drop every sample with a data parameter less than that value\n",
    "    'override_embedding_weights': False,              # Fix the data parameters values in training (do not optimize)\n",
    "\n",
    "    'save_every': 200,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'debug': False,\n",
    "    'wandb_mode': 'disabled',                         # e.g. online, disabled. Use weights and biases online logging\n",
    "    'do_sweep': False,                                # Run multiple trainings with varying config values defined in sweep_config_dict below\n",
    "\n",
    "    # For a snapshot file: dummy-a2p2z76CxhCtwLJApfe8xD_fold0_epx0\n",
    "    'checkpoint_name': None,                          # Training snapshot name, e.g. dummy-a2p2z76CxhCtwLJApfe8xD\n",
    "    'fold_override': None,                            # Training fold, e.g. 0\n",
    "    'checkpoint_epx': None,                           # Training epx, e.g. 0\n",
    "\n",
    "    'do_plot': False,                                 # Generate plots (debugging purpose)\n",
    "    'save_dp_figures': False,                         # Plot data parameter value distribution\n",
    "    'save_labels': True,                              # Store training labels alongside data parameter values inside the training snapshot\n",
    "\n",
    "    # Disturbance settings\n",
    "    'disturbance_mode': None,                         # e.g. LabelDisturbanceMode.FLIP_ROLL, LabelDisturbanceMode.AFFINE\n",
    "    'disturbance_strength': 0.,                       # Strength of how a severe label is distorted if artificial disturbance is used\n",
    "    'disturbed_percentage': 0.,                       # Sercentage of the dataset labels to be disturbed\n",
    "\n",
    "    'device': 'cpu'\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de18b0cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_data(config):\n",
    "\n",
    "    assert os.path.isdir(config.dataset_directory), \"Dataset directory does not exist.\"\n",
    "\n",
    "    reset_determinism()\n",
    "    if config.reg_state:\n",
    "        print(\"Loading registered data.\")\n",
    "\n",
    "        if config.reg_state == \"mix_combined_best\":\n",
    "            config.atlas_count = 1\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220113_crossmoda_optimal/optimal_reg_left.pth\"), map_location=config.device)\n",
    "            label_data_right = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220113_crossmoda_optimal/optimal_reg_right.pth\"), map_location=config.device)\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "\n",
    "            perm = np.random.permutation(len(loaded_identifier))\n",
    "            _clen = int(.5*len(loaded_identifier))\n",
    "            best_choice = perm[:_clen]\n",
    "            combined_choice = perm[_clen:]\n",
    "\n",
    "            best_label_data = torch.cat([label_data_left['best_all'].to_dense()[:44], label_data_right['best_all'].to_dense()[:63]], dim=0)[best_choice]\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'].to_dense()[:44], label_data_right['combined_all'].to_dense()[:63]], dim=0)[combined_choice]\n",
    "            label_data = torch.zeros([107,128,128,128])\n",
    "            label_data[best_choice] = best_label_data\n",
    "            label_data[combined_choice] = combined_label_data\n",
    "            var_identifier = [\"mBST\" if idx in best_choice else \"mCMB\" for idx in range(len(loaded_identifier))]\n",
    "            loaded_identifier = [f\"{_id}:{var_id}\" for _id, var_id in zip(loaded_identifier, var_identifier)]\n",
    "\n",
    "        elif config.reg_state == \"acummulate_combined_best\":\n",
    "            config.atlas_count = 2\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220113_crossmoda_optimal/optimal_reg_left.pth\"))\n",
    "            label_data_right = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220113_crossmoda_optimal/optimal_reg_right.pth\"))\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            best_label_data = torch.cat([label_data_left['best_all'].to_dense()[:44], label_data_right['best_all'].to_dense()[:63]], dim=0)\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'].to_dense()[:44], label_data_right['combined_all'].to_dense()[:63]], dim=0)\n",
    "            label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            loaded_identifier = [_id+':mBST' for _id in loaded_identifier] + [_id+':mCMB' for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"best\":\n",
    "            config.atlas_count = 1\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220113_crossmoda_optimal/optimal_reg_left.pth\"))\n",
    "            label_data_right = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220113_crossmoda_optimal/optimal_reg_right.pth\"))\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            label_data = torch.cat([label_data_left[config.reg_state+'_all'].to_dense()[:44], label_data_right[config.reg_state+'_all'].to_dense()[:63]], dim=0)\n",
    "            postfix = 'mBST'\n",
    "            loaded_identifier = [_id+':'+postfix for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"combined\":\n",
    "            config.atlas_count = 1\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220113_crossmoda_optimal/optimal_reg_left.pth\"))\n",
    "            label_data_right = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220113_crossmoda_optimal/optimal_reg_right.pth\"))\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            label_data = torch.cat([label_data_left[config.reg_state+'_all'].to_dense()[:44], label_data_right[config.reg_state+'_all'].to_dense()[:63]], dim=0)\n",
    "            postfix = 'mCMB'\n",
    "            loaded_identifier = [_id+':'+postfix for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"acummulate_convex_adam_FT2_MT1\":\n",
    "            config.atlas_count = 10\n",
    "            domain = 'target'\n",
    "            bare_data = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220318_crossmoda_convex_adam_lr/crossmoda_convex_registered_new_convex.pth\"))\n",
    "            label_data = []\n",
    "            loaded_identifier = []\n",
    "            for fixed_id, moving_dict in bare_data.items():\n",
    "                sorted_moving_dict = OrderedDict(moving_dict)\n",
    "                for idx_mov, (moving_id, moving_sample) in enumerate(sorted_moving_dict.items()):\n",
    "                    # Only use every third warped sample\n",
    "                    if idx_mov % 3 == 0:\n",
    "                        label_data.append(moving_sample['warped_label'].cpu())\n",
    "                        loaded_identifier.append(f\"{fixed_id}:m{moving_id}\")\n",
    "\n",
    "        elif config.reg_state == \"acummulate_every_third_deeds_FT2_MT1\":\n",
    "            config.atlas_count = 10\n",
    "            domain = 'target'\n",
    "            bare_data = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220114_crossmoda_multiple_registrations/crossmoda_deeds_registered.pth\"), map_location=config.device)\n",
    "            label_data = []\n",
    "            loaded_identifier = []\n",
    "            for fixed_id, moving_dict in bare_data.items():\n",
    "                sorted_moving_dict = OrderedDict(moving_dict)\n",
    "                for idx_mov, (moving_id, moving_sample) in enumerate(sorted_moving_dict.items()):\n",
    "                    # Only use every third warped sample\n",
    "                    if idx_mov % 3 == 0:\n",
    "                        label_data.append(moving_sample['warped_label'].cpu())\n",
    "                        loaded_identifier.append(f\"{fixed_id}:m{moving_id}\")\n",
    "\n",
    "        elif config.reg_state == \"acummulate_every_deeds_FT2_MT1\":\n",
    "            config.atlas_count = 30\n",
    "            domain = 'target'\n",
    "            bare_data = torch.load(Path(THIS_SCRIPT_DIR, \"./data_artifacts/20220114_crossmoda_multiple_registrations/crossmoda_deeds_registered.pth\"))\n",
    "            label_data = []\n",
    "            loaded_identifier = []\n",
    "            for fixed_id, moving_dict in bare_data.items():\n",
    "                sorted_moving_dict = OrderedDict(moving_dict)\n",
    "                for idx_mov, (moving_id, moving_sample) in enumerate(sorted_moving_dict.items()):\n",
    "                    label_data.append(moving_sample['warped_label'].cpu())\n",
    "                    loaded_identifier.append(f\"{fixed_id}:m{moving_id}\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        modified_3d_label_override = {}\n",
    "        for idx, identifier in enumerate(loaded_identifier):\n",
    "            # Find sth. like 100r:mBST or 100r:m001l\n",
    "            nl_id, lr_id, m_id = re.findall(r'(\\d{1,3})([lr]):m([A-Z0-9a-z]{3,4})$', identifier)[0]\n",
    "            nl_id = int(nl_id)\n",
    "            crossmoda_var_id = f\"{nl_id:03d}{lr_id}:m{m_id}\"\n",
    "            modified_3d_label_override[crossmoda_var_id] = label_data[idx]\n",
    "\n",
    "        prevent_disturbance = True\n",
    "\n",
    "    else:\n",
    "        domain = 'source'\n",
    "        modified_3d_label_override = None\n",
    "        prevent_disturbance = False\n",
    "\n",
    "    if config.dataset == 'crossmoda':\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.5\n",
    "        clsre = get_crossmoda_data_load_closure(\n",
    "            base_dir=str(config.dataset_directory),\n",
    "            domain=domain, state='l4', use_additional_data=False,\n",
    "            size=(128,128,128), resample=True, normalize=True, crop_3d_w_dim_range=config.crop_3d_w_dim_range,\n",
    "            ensure_labeled_pairs=True, modified_3d_label_override=modified_3d_label_override,\n",
    "            debug=config.debug\n",
    "        )\n",
    "        training_dataset = CrossmodaHybridIdLoader(\n",
    "            clsre,\n",
    "            size=(128,128,128), resample=True, normalize=True, crop_3d_w_dim_range=config.crop_3d_w_dim_range,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor,\n",
    "            fixed_weight_file=config.fixed_weight_file, fixed_weight_min_quantile=config.fixed_weight_min_quantile, fixed_weight_min_value=config.fixed_weight_min_value,\n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbc5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot'] and False:\n",
    "    # Plot label voxel W-dim distribution\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    _, all_labels, _ = training_dataset.get_data(use_2d_override=False)\n",
    "    print(all_labels.shape)\n",
    "    sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "    plt.xlabel(\"W\")\n",
    "    plt.ylabel(\"ground truth>0\")\n",
    "    plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0adbde3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def save_parameter_figure(_path, title, text, parameters, reweighted_parameters, dices):\n",
    "    # Show weights and weights with compensation\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12, 4), dpi=80)\n",
    "    sc1 = axs[0].scatter(\n",
    "        range(len(parameters)),\n",
    "        parameters.cpu().detach(), c=dices,s=1, cmap='plasma', vmin=0., vmax=1.)\n",
    "    sc2 = axs[1].scatter(\n",
    "        range(len(reweighted_parameters)),\n",
    "        reweighted_parameters.cpu().detach(), s=1,c=dices, cmap='plasma', vmin=0., vmax=1.)\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.text(0, 0, text)\n",
    "    axs[0].set_title('Bare parameters')\n",
    "    axs[1].set_title('Reweighted parameters')\n",
    "    axs[0].set_ylim(-10, 10)\n",
    "    axs[1].set_ylim(-3, 1)\n",
    "    plt.colorbar(sc2)\n",
    "    plt.savefig(_path)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "\n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "\n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "\n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "\n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a86540",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "\n",
    "    # Print transformed 2D data\n",
    "    training_dataset.train(use_modified=True, augment=False)\n",
    "    # print(training_dataset.disturbed_idxs)\n",
    "\n",
    "    print(\"Displaying 2D training sample\")\n",
    "\n",
    "    img_stack = []\n",
    "    label_stack = []\n",
    "    mod_label_stack = []\n",
    "\n",
    "    for sample in (training_dataset[idx] for idx in [500,590]):\n",
    "        print(sample['id'])\n",
    "        img_stack.append(sample['image'])\n",
    "        label_stack.append(sample['label'])\n",
    "        mod_label_stack.append(sample['modified_label'])\n",
    "\n",
    "    # Change label num == hue shift for display\n",
    "    img_stack = torch.stack(img_stack).unsqueeze(1)\n",
    "    label_stack = torch.stack(label_stack)\n",
    "    mod_label_stack = torch.stack(mod_label_stack)\n",
    "\n",
    "    mod_label_stack*=4\n",
    "\n",
    "    visualize_seg(in_type=\"batch_3D\", reduce_dim=\"W\",\n",
    "        img=img_stack,\n",
    "        # ground_truth=label_stack,\n",
    "        seg=(mod_label_stack-label_stack).abs(),\n",
    "        # crop_to_non_zero_gt=True,\n",
    "        crop_to_non_zero_seg=True,\n",
    "        alpha_seg = .5\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config, dataset_len, num_classes, THIS_SCRIPT_DIR, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        # Use vanilla torch model\n",
    "        lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(\n",
    "            pretrained=False, progress=True, num_classes=num_classes\n",
    "        )\n",
    "        set_module(lraspp, 'backbone.0.0',\n",
    "            torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2),\n",
    "                            padding=(1, 1), bias=False)\n",
    "        )\n",
    "    else:\n",
    "        # Use custom 3d model\n",
    "        lraspp = MobileNet_LRASPP_3D(\n",
    "            in_num=in_channels, num_classes=num_classes,\n",
    "            use_checkpointing=True\n",
    "        )\n",
    "\n",
    "    # lraspp.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "    lraspp.to(device)\n",
    "    print(f\"Param count lraspp: {sum(p.numel() for p in lraspp.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(lraspp.parameters(), lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2)\n",
    "    else:\n",
    "        # Use ExponentialLR in 3D\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.99)\n",
    "\n",
    "    # Add data paramters embedding and optimizer\n",
    "    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len, 1, sparse=True)\n",
    "        embedding = embedding.to(device)\n",
    "\n",
    "        # Init embedding values\n",
    "        #\n",
    "        if config.override_embedding_weights:\n",
    "            fixed_weightdata = torch.load(config.fixed_weight_file, map_location=config.device)\n",
    "            fixed_weights = fixed_weightdata['data_parameters']\n",
    "            fixed_d_ids = fixed_weightdata['d_ids']\n",
    "            if config.use_2d_normal_to is not None:\n",
    "                corresp_dataset_idxs = [training_dataset.get_2d_ids().index(_id) for _id in fixed_d_ids]\n",
    "            else:\n",
    "                corresp_dataset_idxs = [training_dataset.get_3d_ids().index(_id) for _id in fixed_d_ids]\n",
    "            embedding_weight_tensor = torch.zeros_like(embedding.weight)\n",
    "            embedding_weight_tensor[corresp_dataset_idxs] = fixed_weights.view(-1,1).to(device=config.device)\n",
    "            embedding = nn.Embedding(len(training_dataset), 1, sparse=True, _weight=embedding_weight_tensor)\n",
    "\n",
    "        elif _path and _path.is_dir():\n",
    "            embedding.load_state_dict(torch.load(_path.joinpath('embedding.pth'), map_location=device))\n",
    "        else:\n",
    "            torch.nn.init.normal_(embedding.weight.data, mean=config.init_inst_param, std=0.00)\n",
    "\n",
    "        print(f\"Param count embedding: {sum(p.numel() for p in embedding.parameters())}\")\n",
    "\n",
    "        optimizer_dp = torch.optim.SparseAdam(\n",
    "            embedding.parameters(), lr=config.lr_inst_param,\n",
    "            betas=(0.9, 0.999), eps=1e-08)\n",
    "        scaler_dp =  amp.GradScaler()\n",
    "\n",
    "        if _path and _path.is_dir():\n",
    "            print(f\"Loading dp_optimizer and scaler_dp from {_path}\")\n",
    "            optimizer_dp.load_state_dict(torch.load(_path.joinpath('optimizer_dp.pth'), map_location=device))\n",
    "            scaler_dp.load_state_dict(torch.load(_path.joinpath('scaler_dp.pth'), map_location=device))\n",
    "\n",
    "    else:\n",
    "        embedding = None\n",
    "        optimizer_dp = None\n",
    "        scaler_dp = None\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading lr-aspp model, optimizers and grad scalers from {_path}\")\n",
    "        lraspp.load_state_dict(torch.load(_path.joinpath('lraspp.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scheduler.load_state_dict(torch.load(_path.joinpath('scheduler.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "\n",
    "    return (lraspp, optimizer, scheduler, optimizer_dp, embedding, scaler, scaler_dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_wrap(lraspp, img, use_2d, use_mind):\n",
    "    with torch.inference_mode():\n",
    "        b_img = img.unsqueeze(0).unsqueeze(0).float()\n",
    "        if use_2d and use_mind:\n",
    "            # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "            b_img = mindssc(b_img.unsqueeze(0)).squeeze(2)\n",
    "        elif not use_2d and use_mind:\n",
    "            # MIND 3D in Bx1xDxHxW out BxMINDxDxHxW\n",
    "            b_img = mindssc(b_img)\n",
    "        elif use_2d or not use_2d:\n",
    "            # 2D Bx1xHxW\n",
    "            # 3D out Bx1xDxHxW\n",
    "            pass\n",
    "\n",
    "        b_out = lraspp(b_img)['out']\n",
    "        b_out = b_out.argmax(1)\n",
    "        return b_out\n",
    "\n",
    "\n",
    "\n",
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    # kf.get_n_splits(training_dataset.__len__(use_2d_override=False))\n",
    "    fold_iter = enumerate(kf.split(range(training_dataset.__len__(use_2d_override=False))))\n",
    "\n",
    "    if config.get('fold_override', None):\n",
    "        selected_fold = config.get('fold_override', 0)\n",
    "        fold_iter = list(fold_iter)[selected_fold:selected_fold+1]\n",
    "    elif config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "\n",
    "    if config.wandb_mode != 'disabled':\n",
    "        warnings.warn(\"Logging of dataset file paths is disabled.\")\n",
    "        # # Log dataset info\n",
    "        # training_dataset.eval()\n",
    "        # dataset_info = [[smp['dataset_idx'], smp['id'], smp['image_path'], smp['label_path']] \\\n",
    "        #                 for smp in training_dataset]\n",
    "        # wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', 'id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        n_dims = (-2,-1)\n",
    "    else:\n",
    "        n_dims = (-3,-2,-1)\n",
    "\n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        all_3d_ids = training_dataset.get_3d_ids()\n",
    "\n",
    "        if config.debug:\n",
    "            num_val_images = 2\n",
    "            atlas_count = 1\n",
    "        else:\n",
    "            num_val_images = config.num_val_images\n",
    "            atlas_count = config.atlas_count\n",
    "\n",
    "        if config.use_2d_normal_to is not None:\n",
    "            # Override idxs\n",
    "            all_3d_ids = training_dataset.get_3d_ids()\n",
    "\n",
    "            val_3d_idxs = torch.tensor(list(range(0, num_val_images*atlas_count, atlas_count)))\n",
    "            val_3d_ids = training_dataset.switch_3d_identifiers(val_3d_idxs)\n",
    "\n",
    "            train_3d_idxs = list(range(num_val_images*atlas_count, len(all_3d_ids)))\n",
    "\n",
    "            # Get corresponding 2D idxs\n",
    "            train_2d_ids = []\n",
    "            dcts = training_dataset.get_id_dicts()\n",
    "            for id_dict in dcts:\n",
    "                _2d_id = id_dict['2d_id']\n",
    "                _3d_idx = id_dict['3d_dataset_idx']\n",
    "                if _2d_id in training_dataset.label_data_2d.keys() and _3d_idx in train_3d_idxs:\n",
    "                    train_2d_ids.append(_2d_id)\n",
    "\n",
    "            train_2d_idxs = training_dataset.switch_2d_identifiers(train_2d_ids)\n",
    "            train_idxs = torch.tensor(train_2d_idxs)\n",
    "\n",
    "        else:\n",
    "            val_3d_idxs = torch.tensor(list(range(0, num_val_images*atlas_count, atlas_count)))\n",
    "            val_3d_ids = training_dataset.switch_3d_identifiers(val_3d_idxs)\n",
    "\n",
    "            train_3d_idxs = list(range(num_val_images*atlas_count, len(all_3d_ids)))\n",
    "            train_idxs = torch.tensor(train_3d_idxs)\n",
    "\n",
    "        print(f\"Will run validation with these 3D samples (#{len(val_3d_ids)}):\", sorted(val_3d_ids))\n",
    "\n",
    "        _, _, all_modified_segs = training_dataset.get_data()\n",
    "\n",
    "        if config.disturbed_percentage > 0.:\n",
    "            with torch.no_grad():\n",
    "                non_empty_train_idxs = [(all_modified_segs[train_idxs].sum(dim=n_dims) > 0)]\n",
    "\n",
    "            ### Disturb dataset (only non-emtpy idxs)###\n",
    "            proposed_disturbed_idxs = np.random.choice(non_empty_train_idxs, size=int(len(non_empty_train_idxs)*config.disturbed_percentage), replace=False)\n",
    "            proposed_disturbed_idxs = torch.tensor(proposed_disturbed_idxs)\n",
    "            training_dataset.disturb_idxs(proposed_disturbed_idxs,\n",
    "                disturbance_mode=config.disturbance_mode,\n",
    "                disturbance_strength=config.disturbance_strength\n",
    "            )\n",
    "            disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "            disturbed_bool_vect[training_dataset.disturbed_idxs] = 1.\n",
    "\n",
    "        else:\n",
    "            disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "\n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, training_dataset.disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(training_dataset.disturbed_idxs))\n",
    "\n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in training_dataset.disturbed_idxs])},\n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "\n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels = 12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "\n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "            sampler=train_subsampler, pin_memory=False, drop_last=False,\n",
    "            # collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "        )\n",
    "\n",
    "        # training_dataset.set_augment_at_collate(True) # This function does not work as expected. Scores get worse.\n",
    "\n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        if 'checkpoint_epx' in config and config['checkpoint_epx'] is not None:\n",
    "            epx_start = config['checkpoint_epx']\n",
    "        else:\n",
    "            epx_start = 0\n",
    "\n",
    "        if config.checkpoint_name:\n",
    "            # Load from checkpoint\n",
    "            _path = f\"{config.mdl_save_prefix}/{config.checkpoint_name}_fold{fold_idx}_epx{epx_start}\"\n",
    "        else:\n",
    "            _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx_start}\"\n",
    "\n",
    "        (lraspp, optimizer, scheduler, optimizer_dp, embedding, scaler, scaler_dp) = get_model(config, len(training_dataset), len(training_dataset.label_tags),\n",
    "            THIS_SCRIPT_DIR=THIS_SCRIPT_DIR, _path=_path, device=config.device)\n",
    "\n",
    "        t_start = time.time()\n",
    "\n",
    "        dice_func = dice2d if config.use_2d_normal_to is not None else dice3d\n",
    "\n",
    "        bn_count = torch.zeros([len(training_dataset.label_tags)], device=all_modified_segs.device)\n",
    "        wise_dice = torch.zeros([len(training_dataset), len(training_dataset.label_tags)])\n",
    "        gt_num = torch.zeros([len(training_dataset)])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(\"Fetching training metrics for samples.\")\n",
    "            # _, wise_lbls, mod_lbls = training_dataset.get_data()\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            for sample in tqdm((training_dataset[idx] for idx in train_idxs), desc=\"metric:\", total=len(train_idxs)):\n",
    "                d_idxs = sample['dataset_idx']\n",
    "                wise_label, mod_label = sample['label'], sample['modified_label']\n",
    "                mod_label = mod_label.to(device=config.device)\n",
    "                wise_label = wise_label.to(device=config.device)\n",
    "                mod_label, _ = ensure_dense(mod_label)\n",
    "\n",
    "                dsc = dice_func(\n",
    "                    torch.nn.functional.one_hot(wise_label.unsqueeze(0), len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(mod_label.unsqueeze(0), len(training_dataset.label_tags)),\n",
    "                    one_hot_torch_style=True, nan_for_unlabeled_target=False\n",
    "                )\n",
    "                bn_count += torch.bincount(mod_label.reshape(-1).long(), minlength=len(training_dataset.label_tags)).cpu()\n",
    "                wise_dice[d_idxs] = dsc.cpu()\n",
    "                gt_num[d_idxs] = (mod_label > 0).sum(dim=n_dims).float().cpu()\n",
    "\n",
    "            class_weights = 1 / (bn_count).float().pow(.35)\n",
    "            class_weights /= class_weights.mean()\n",
    "\n",
    "            fixed_weighting = (gt_num+np.exp(1)).log()+np.exp(1)\n",
    "\n",
    "        class_weights = class_weights.to(device=config.device)\n",
    "        fixed_weighting = fixed_weighting.to(device=config.device)\n",
    "\n",
    "        autocast_enabled = 'cuda' in config.device\n",
    "\n",
    "        for epx in range(epx_start, config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "\n",
    "            lraspp.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            training_dataset.train(use_modified=True)\n",
    "\n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            class_dices = []\n",
    "\n",
    "            # Load data\n",
    "            for batch_idx, batch in tqdm(enumerate(train_dataloader), desc=\"batch:\", total=len(train_dataloader)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if optimizer_dp:\n",
    "                    optimizer_dp.zero_grad()\n",
    "\n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "                b_img = b_img.float()\n",
    "\n",
    "                b_img = b_img.to(device=config.device)\n",
    "                b_seg_modified = b_seg_modified.to(device=config.device)\n",
    "                b_idxs_dataset = b_idxs_dataset.to(device=config.device)\n",
    "                b_seg = b_seg.to(device=config.device)\n",
    "\n",
    "                if training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "                elif not training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 3D\n",
    "                    b_img = mindssc(b_img.unsqueeze(1))\n",
    "                else:\n",
    "                    b_img = b_img.unsqueeze(1)\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=autocast_enabled):\n",
    "                    assert b_img.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input image for model must be {len(n_dims)+2}D: BxCxSPATIAL but is {b_img.shape}\"\n",
    "                    for param in lraspp.parameters():\n",
    "                        param.requires_grad = True\n",
    "\n",
    "                    lraspp.use_checkpointing = True\n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    ### Calculate loss ###\n",
    "                    assert logits.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input shape for loss must be BxNUM_CLASSESxSPATIAL but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == len(n_dims)+1, \\\n",
    "                        f\"Target shape for loss must be BxSPATIAL but is {b_seg_modified.shape}\"\n",
    "\n",
    "                    ce_loss = nn.CrossEntropyLoss(class_weights)(logits, b_seg_modified)\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.DISABLED) or config.use_ool_dp_loss:\n",
    "                        scaler.scale(ce_loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                        if config.use_ool_dp_loss:\n",
    "                            # Run second consecutive forward pass\n",
    "                            for param in lraspp.parameters():\n",
    "                                param.requires_grad = False\n",
    "                            lraspp.use_checkpointing = False\n",
    "                            dp_logits = lraspp(b_img)['out']\n",
    "\n",
    "                        else:\n",
    "                            # Do not run a second forward pass\n",
    "                            for param in lraspp.parameters():\n",
    "                                param.requires_grad = True\n",
    "                            lraspp.use_checkpointing = True\n",
    "                            dp_logits = logits\n",
    "\n",
    "                        dp_loss = nn.CrossEntropyLoss(reduction='none')(dp_logits, b_seg_modified)\n",
    "                        dp_loss = dp_loss.mean(n_dims)\n",
    "\n",
    "                        bare_weight = embedding(b_idxs_dataset).squeeze()\n",
    "\n",
    "                        weight = torch.sigmoid(bare_weight)\n",
    "                        weight = weight/weight.mean()\n",
    "\n",
    "                        # This improves scores significantly: Reweight with log(gt_numel)\n",
    "                        if config.use_fixed_weighting:\n",
    "                            weight = weight/fixed_weighting[b_idxs_dataset]\n",
    "\n",
    "                        if config.use_risk_regularization:\n",
    "                            p_pred_num = (dp_logits.argmax(1) > 0).sum(dim=n_dims).detach()\n",
    "                            if config.use_2d_normal_to is not None:\n",
    "                                risk_regularization = -weight*p_pred_num/(dp_logits.shape[-2]*dp_logits.shape[-1])\n",
    "                            else:\n",
    "                                risk_regularization = -weight*p_pred_num/(dp_logits.shape[-3]*dp_logits.shape[-2]*dp_logits.shape[-1])\n",
    "\n",
    "                            dp_loss = (dp_loss*weight).sum() + risk_regularization.sum()\n",
    "                        else:\n",
    "                            dp_loss = (dp_loss*weight).sum()\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                    scaler_dp.scale(dp_loss).backward()\n",
    "\n",
    "                    if config.use_ool_dp_loss:\n",
    "                        # LRASPP already stepped.\n",
    "                        if not config.override_embedding_weights:\n",
    "                            scaler_dp.step(optimizer_dp)\n",
    "                            scaler_dp.update()\n",
    "                    else:\n",
    "                        scaler_dp.step(optimizer)\n",
    "                        if not config.override_embedding_weights:\n",
    "                            scaler_dp.step(optimizer_dp)\n",
    "                        scaler_dp.update()\n",
    "\n",
    "                    epx_losses.append(dp_loss.item())\n",
    "                else:\n",
    "                    epx_losses.append(ce_loss.item())\n",
    "\n",
    "                logits_for_score = logits.argmax(1)\n",
    "\n",
    "                # Calculate dice score\n",
    "                b_dice = dice_func(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(b_seg, len(training_dataset.label_tags)), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_over_all(\n",
    "                    b_dice, exclude_bg=True))\n",
    "                class_dices.append(get_batch_dice_per_class(\n",
    "                    b_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                ###  Scheduler management ###\n",
    "                if config.use_scheduling and epx % atlas_count == 0:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED) and batch_idx % 10 == 0 and config.save_dp_figures:\n",
    "                    # Output data parameter figure\n",
    "                    train_params = embedding.weight[train_idxs].squeeze()\n",
    "                    # order = np.argsort(train_params.cpu().detach()) # Order by DP value\n",
    "                    order = torch.arange(len(train_params))\n",
    "                    pearson_corr_coeff = np.corrcoef(train_params.cpu().detach(), wise_dice[train_idxs][:,1].cpu().detach())[0,1]\n",
    "                    dp_figure_path = Path(f\"data/output_figures/{wandb.run.name}_fold{fold_idx}/dp_figure_epx{epx:03d}_batch{batch_idx:03d}.png\")\n",
    "                    dp_figure_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    save_parameter_figure(dp_figure_path, wandb.run.name, f\"corr. coeff. DP vs. dice(expert label, train gt): {pearson_corr_coeff:4f}\",\n",
    "                        train_params[order], train_params[order]/fixed_weighting[train_idxs][order], dices=wise_dice[train_idxs][:,1][order])\n",
    "\n",
    "                if config.debug:\n",
    "                    break\n",
    "\n",
    "            ### Logging ###\n",
    "            print(f\"### Log epoch {epx} @ {time.time()-t_start:.2f}s\")\n",
    "            print(\"### Training\")\n",
    "\n",
    "            ### Log wandb data ###\n",
    "            # Log the epoch idx per fold - so we can recover the diagram by setting\n",
    "            # ref_epoch_idx as x-axis in wandb interface\n",
    "            wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "\n",
    "            mean_loss = torch.tensor(epx_losses).mean()\n",
    "            wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "\n",
    "            mean_dice = np.nanmean(dices)\n",
    "            print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "            wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "\n",
    "            log_class_dices(\"scores/dice_mean_\", f\"_fold{fold_idx}\", class_dices, global_idx)\n",
    "\n",
    "            # Log data parameters of disturbed samples\n",
    "            if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                # Calculate dice score corr coeff (unknown to network)\n",
    "                train_params = embedding.weight[train_idxs].squeeze()\n",
    "                order = np.argsort(train_params.cpu().detach())\n",
    "                pearson_corr_coeff = np.corrcoef(train_params[order].cpu().detach(), wise_dice[train_idxs][:,1][order].cpu().detach())[0,1]\n",
    "                spearman_corr_coeff, spearman_p = scipy.stats.spearmanr(train_params[order].cpu().detach(), wise_dice[train_idxs][:,1][order].cpu().detach())\n",
    "\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/pearson_corr_coeff_fold{fold_idx}': pearson_corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/spearman_corr_coeff_fold{fold_idx}': spearman_corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/spearman_p_fold{fold_idx}': spearman_p},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                print(f'data_parameters/pearson_corr_coeff_fold{fold_idx}', f\"{pearson_corr_coeff:.2f}\")\n",
    "                print(f'data_parameters/spearman_corr_coeff_fold{fold_idx}', f\"{spearman_corr_coeff:.2f}\")\n",
    "                print(f'data_parameters/spearman_p_fold{fold_idx}', f\"{spearman_p:.5f}\")\n",
    "\n",
    "                # Log stats of data parameters and figure\n",
    "                log_data_parameter_stats(f'data_parameters/iter_stats_fold{fold_idx}', global_idx, embedding.weight.data)\n",
    "\n",
    "            if (epx % config.save_every == 0) \\\n",
    "                or (epx+1 == config.epochs):\n",
    "                _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx}\"\n",
    "                save_model(\n",
    "                    Path(THIS_SCRIPT_DIR, _path),\n",
    "                    lraspp=lraspp,\n",
    "                    optimizer=optimizer, optimizer_dp=optimizer_dp,\n",
    "                    scheduler=scheduler,\n",
    "                    embedding=embedding,\n",
    "                    scaler=scaler,\n",
    "                    scaler_dp=scaler_dp)\n",
    "\n",
    "                (lraspp, optimizer, scheduler, optimizer_dp, embedding, scaler, scaler_dp) = \\\n",
    "                    get_model(\n",
    "                        config, len(training_dataset),\n",
    "                        len(training_dataset.label_tags),\n",
    "                        THIS_SCRIPT_DIR=THIS_SCRIPT_DIR,\n",
    "                        _path=_path, device=config.device)\n",
    "\n",
    "            print()\n",
    "            print(\"### Validation\")\n",
    "            lraspp.eval()\n",
    "            training_dataset.eval()\n",
    "\n",
    "            val_dices = []\n",
    "            val_class_dices = []\n",
    "\n",
    "            with amp.autocast(enabled=autocast_enabled):\n",
    "                with torch.no_grad():\n",
    "                    for val_idx in val_3d_idxs:\n",
    "                        val_sample = training_dataset.get_3d_item(val_idx)\n",
    "                        stack_dim = training_dataset.use_2d_normal_to\n",
    "                        # Create batch out of single val sample\n",
    "                        b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                        b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "\n",
    "                        B = b_val_img.shape[0]\n",
    "\n",
    "                        b_val_img = b_val_img.unsqueeze(1).float().to(device=config.device)\n",
    "                        b_val_seg = b_val_seg.to(device=config.device)\n",
    "\n",
    "                        if training_dataset.use_2d():\n",
    "                            b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=training_dataset.use_2d_normal_to)\n",
    "\n",
    "                            if config.use_mind:\n",
    "                                # MIND 2D model, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                                b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                            output_val = lraspp(b_val_img_2d)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "                            # Prepare logits for scoring\n",
    "                            # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                            val_logits_for_score = make_3d_from_2d_stack(\n",
    "                                val_logits_for_score.unsqueeze(1), stack_dim, B\n",
    "                            ).squeeze(1)\n",
    "\n",
    "                        else:\n",
    "                            if config.use_mind:\n",
    "                                # MIND 3D model shape BxMINDxDxHxW\n",
    "                                b_val_img = mindssc(b_val_img)\n",
    "                            else:\n",
    "                                # 3D model shape Bx1xDxHxW\n",
    "                                pass\n",
    "\n",
    "                            output_val = lraspp(b_val_img)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "\n",
    "                        b_val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(val_logits_for_score, len(training_dataset.label_tags)),\n",
    "                            torch.nn.functional.one_hot(b_val_seg, len(training_dataset.label_tags)),\n",
    "                            one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        # Get mean score over batch\n",
    "                        val_dices.append(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=True))\n",
    "\n",
    "                        val_class_dices.append(get_batch_dice_per_class(\n",
    "                            b_val_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                        if config.do_plot:\n",
    "                            print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                            print(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=False))\n",
    "                            # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                            display_seg(in_type=\"single_3D\",\n",
    "                                reduce_dim=\"W\",\n",
    "                                img=val_sample['image'].unsqueeze(0).cpu(),\n",
    "                                seg=val_logits_for_score_3d.squeeze(0).cpu(),\n",
    "                                ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                crop_to_non_zero_seg=True,\n",
    "                                crop_to_non_zero_gt=True,\n",
    "                                alpha_seg=.3,\n",
    "                                alpha_gt=.0\n",
    "                            )\n",
    "\n",
    "                    mean_val_dice = np.nanmean(val_dices)\n",
    "                    print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                    wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                    log_class_dices(\"scores/val_dice_mean_\", f\"_fold{fold_idx}\", val_class_dices, global_idx)\n",
    "\n",
    "            print()\n",
    "            # End of training loop\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        if str(config.data_param_mode) == str(DataParamMode.INSTANCE_PARAMS):\n",
    "            # Write sample data\n",
    "            save_dict = {}\n",
    "\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            all_idxs = torch.tensor(range(len(training_dataset))).to(device=config.device)\n",
    "            train_label_snapshot_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/train_label_snapshot.pth\")\n",
    "            seg_viz_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weighted_samples.png\")\n",
    "\n",
    "            train_label_snapshot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            dp_weights = embedding(all_idxs)\n",
    "            save_data = []\n",
    "            data_generator = zip(\n",
    "                dp_weights[train_idxs], \\\n",
    "                disturbed_bool_vect[train_idxs],\n",
    "                torch.utils.data.Subset(training_dataset, train_idxs)\n",
    "            )\n",
    "\n",
    "            for dp_weight, disturb_flg, sample in data_generator:\n",
    "                data_tuple = ( \\\n",
    "                    dp_weight,\n",
    "                    bool(disturb_flg.item()),\n",
    "                    sample['id'],\n",
    "                    sample['dataset_idx'],\n",
    "                    sample['image_path'],\n",
    "                    # sample['image'],\n",
    "                    sample['label'].to_sparse(),\n",
    "                    sample['label_path'],\n",
    "                    sample['modified_label'].to_sparse(),\n",
    "                    inference_wrap(lraspp, sample['image'].to(device=config.device), use_2d=training_dataset.use_2d(), use_mind=config.use_mind).to_sparse()\n",
    "                )\n",
    "                save_data.append(data_tuple)\n",
    "\n",
    "            save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "            (\n",
    "                dp_weight,\n",
    "                disturb_flags,\n",
    "                d_ids,\n",
    "                dataset_idxs,\n",
    "                image_paths,\n",
    "                #  _imgs,\n",
    "                _labels,\n",
    "                label_paths,\n",
    "                _modified_labels,\n",
    "                _predictions) = zip(*save_data)\n",
    "\n",
    "            dp_weight = torch.stack(dp_weight)\n",
    "            dataset_idxs = torch.stack(dataset_idxs)\n",
    "\n",
    "            save_dict.update(\n",
    "                {\n",
    "                    'data_parameters': dp_weight.cpu(),\n",
    "                    'disturb_flags': disturb_flags,\n",
    "                    'd_ids': d_ids,\n",
    "                    'dataset_idxs': dataset_idxs.cpu(),\n",
    "                    'image_paths': image_paths,\n",
    "                    'label_paths': label_paths\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if config.save_labels:\n",
    "                _labels = torch.stack(_labels)\n",
    "                _modified_labels = torch.stack(_modified_labels)\n",
    "                _predictions = torch.stack(_predictions)\n",
    "                save_dict.update(\n",
    "                    {\n",
    "                        'labels': _labels.cpu(),\n",
    "                        'modified_labels': _modified_labels.cpu(),\n",
    "                        'train_predictions': _predictions.cpu()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            print(f\"Writing data parameters output to '{train_label_snapshot_path}'\")\n",
    "            torch.save(save_dict, train_label_snapshot_path)\n",
    "\n",
    "            if len(training_dataset.disturbed_idxs) > 0:\n",
    "                # Log histogram\n",
    "                separated_params = list(zip(dp_weights[clean_idxs], dp_weights[training_dataset.disturbed_idxs]))\n",
    "                s_table = wandb.Table(columns=['clean_idxs', 'disturbed_idxs'], data=separated_params)\n",
    "                fields = {\"primary_bins\": \"clean_idxs\", \"secondary_bins\": \"disturbed_idxs\", \"title\": \"Data parameter composite histogram\"}\n",
    "                composite_histogram = wandb.plot_table(vega_spec_name=\"rap1ide/composite_histogram\", data_table=s_table, fields=fields)\n",
    "                wandb.log({f\"data_parameters/separated_params_fold_{fold_idx}\": composite_histogram})\n",
    "\n",
    "            # Write out data of modified and un-modified labels and an overview image\n",
    "\n",
    "            if training_dataset.use_2d():\n",
    "                reduce_dim = None\n",
    "                in_type = \"batch_2D\"\n",
    "                skip_writeout = len(training_dataset) > 3000 # Restrict dataset size to be visualized\n",
    "            else:\n",
    "                reduce_dim = \"W\"\n",
    "                in_type = \"batch_3D\"\n",
    "                skip_writeout = len(training_dataset) > 150 # Restrict dataset size to be visualized\n",
    "            skip_writeout = True\n",
    "\n",
    "            if not skip_writeout:\n",
    "                print(\"Writing train sample image.\")\n",
    "                # overlay text example: d_idx=0, dp_i=1.00, dist? False\n",
    "                overlay_text_list = [f\"id:{d_id} dp:{instance_p.item():.2f}\" \\\n",
    "                    for d_id, instance_p, disturb_flg in zip(d_ids, dp_weight, disturb_flags)]\n",
    "\n",
    "                use_2d = training_dataset.use_2d()\n",
    "                scf = 1/training_dataset.pre_interpolation_factor\n",
    "\n",
    "                show_img = interpolate_sample(b_label=_labels.to_dense(), scale_factor=scf, use_2d=use_2d)[1].unsqueeze(1)\n",
    "                show_seg = interpolate_sample(b_label=_predictions.to_dense().squeeze(1), scale_factor=scf, use_2d=use_2d)[1]\n",
    "                show_gt = interpolate_sample(b_label=_modified_labels.to_dense(), scale_factor=scf, use_2d=use_2d)[1]\n",
    "\n",
    "                visualize_seg(in_type=in_type, reduce_dim=reduce_dim,\n",
    "                    img=show_img, # Expert label in BW\n",
    "                    seg=4*show_seg, # Prediction in blue\n",
    "                    ground_truth=show_gt, # Modified label in red\n",
    "                    crop_to_non_zero_seg=False,\n",
    "                    alpha_seg = .5,\n",
    "                    alpha_gt = .5,\n",
    "                    n_per_row=70,\n",
    "                    overlay_text=overlay_text_list,\n",
    "                    annotate_color=(0,255,255),\n",
    "                    frame_elements=disturb_flags,\n",
    "                    file_path=seg_viz_out_path,\n",
    "                )\n",
    "\n",
    "        # End of fold loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779c09b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Config overrides\n",
    "# config_dict['wandb_mode'] = 'disabled'\n",
    "# config_dict['debug'] = True\n",
    "# Model loading\n",
    "# config_dict['checkpoint_name'] = 'ethereal-serenity-1138'\n",
    "# config_dict['fold_override'] = 0\n",
    "# config_dict['checkpoint_epx'] = 39\n",
    "\n",
    "# Define sweep override dict\n",
    "sweep_config_dict = dict(\n",
    "    method='grid',\n",
    "    metric=dict(goal='maximize', name='scores/val_dice_mean_tumour_fold0'),\n",
    "    parameters=dict(\n",
    "        # disturbance_mode=dict(\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        # disturbance_strength=dict(\n",
    "        #     values=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "        # ),\n",
    "        # disturbed_percentage=dict(\n",
    "        #     values=[0.3, 0.6]\n",
    "        # ),\n",
    "        # data_param_mode=dict(\n",
    "        #     values=[\n",
    "        #         DataParamMode.INSTANCE_PARAMS,\n",
    "        #         DataParamMode.DISABLED,\n",
    "        #     ]\n",
    "        # ),\n",
    "        use_risk_regularization=dict(\n",
    "            values=[False, True]\n",
    "        ),\n",
    "        use_fixed_weighting=dict(\n",
    "            values=[False, True]\n",
    "        ),\n",
    "        # fixed_weight_min_quantile=dict(\n",
    "        #     values=[0.9, 0.8, 0.6, 0.4, 0.2, 0.0]\n",
    "        # ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "549637cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrap1ide\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rap1ide/curriculum_deeplab/runs/2tdi1vm1\" target=\"_blank\">prosperous-chrysanthemum-1269</a></strong> to <a href=\"https://wandb.ai/rap1ide/curriculum_deeplab\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normal_run():\n",
    "    with wandb.init(project=\"deep_staple\", group=\"training\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        ) as run:\n",
    "\n",
    "        run_name = run.name\n",
    "        print(\"Running\", run_name)\n",
    "        training_dataset = prepare_data(config_dict)\n",
    "        config = wandb.config\n",
    "\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init() as run:\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "\n",
    "        run_name = run.name\n",
    "        print(\"Running\", run_name)\n",
    "        training_dataset = prepare_data(config)\n",
    "        config = wandb.config\n",
    "\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "if config_dict['do_sweep']:\n",
    "    # Integrate all config_dict entries into sweep_dict.parameters -> sweep overrides config_dict\n",
    "    cp_config_dict = copy.deepcopy(config_dict)\n",
    "    # cp_config_dict.update(copy.deepcopy(sweep_config_dict['parameters']))\n",
    "    for del_key in sweep_config_dict['parameters'].keys():\n",
    "        if del_key in cp_config_dict:\n",
    "            del cp_config_dict[del_key]\n",
    "    merged_sweep_config_dict = copy.deepcopy(sweep_config_dict)\n",
    "    # merged_sweep_config_dict.update(cp_config_dict)\n",
    "    for key, value in cp_config_dict.items():\n",
    "        merged_sweep_config_dict['parameters'][key] = dict(value=value)\n",
    "    # Convert enum values in parameters to string. They will be identified by their numerical index otherwise\n",
    "    for key, param_dict in merged_sweep_config_dict['parameters'].items():\n",
    "        if 'value' in param_dict and isinstance(param_dict['value'], Enum):\n",
    "            param_dict['value'] = str(param_dict['value'])\n",
    "        if 'values' in param_dict:\n",
    "            param_dict['values'] = [str(elem) if isinstance(elem, Enum) else elem for elem in param_dict['values']]\n",
    "\n",
    "        merged_sweep_config_dict['parameters'][key] = param_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(merged_sweep_config_dict, project=\"deep_staple\")\n",
    "    wandb.agent(sweep_id, function=sweep_run)\n",
    "\n",
    "else:\n",
    "    normal_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2244b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading registered data.\n",
      "Loading CrossMoDa hrT2 images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60 images, 60 labels: 100%|| 120/120 [00:09<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding label data with modified_3d_label_override from 60 to 600 labels\n",
      "Postprocessing 3D volumes\n",
      "Removed 0 3D images in postprocessing\n",
      "Equal image and label numbers: True (600)\n",
      "Image shape: torch.Size([600, 128, 128, 50]), mean.: 0.00, std.: 1.00\n",
      "Label shape: torch.Size([600, 128, 128, 50]), max.: 1\n",
      "Data import finished.\n",
      "CrossMoDa loader will yield 3D samples\n"
     ]
    }
   ],
   "source": [
    "if not in_notebook():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a04ac42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run validation with these 3D samples (#20): ['108r:m100r', '112r:m100r', '118r:m100r', '120r:m100r', '123r:m100r', '127r:m100r', '134r:m100r', '135r:m100r', '142r:m100r', '144r:m100r', '148r:m100r', '154r:m100r', '160r:m100r', '165r:m100r', '166r:m100r', '167r:m100r', '168r:m100r', '171r:m100r', '173r:m100r', '174r:m100r']\n"
     ]
    }
   ],
   "source": [
    "# Do any postprocessing / visualization in notebook here"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
