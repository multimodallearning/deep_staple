{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "# from curriculum_deeplab.data_parameters import DataParamMode, DataParamOptim\n",
    "# from curriculum_deeplab.data_parameters import DataParameterManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "import torch\n",
    "\n",
    "from curriculum_deeplab.sparse_sgd import SparseSGD\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "class DataParamMode(Enum):\n",
    "    ONLY_INSTANCE_PARAMS = auto()\n",
    "    ONLY_CLASS_PARAMS = auto()\n",
    "    COMBINED_INSTANCE_CLASS_PARAMS = auto()\n",
    "    SEPARATE_INSTANCE_CLASS_PARAMS = auto()\n",
    "    DISABLED = auto()\n",
    "\n",
    "class DataParamOptim(Enum):\n",
    "    ADAM = auto()\n",
    "    SGD = auto()\n",
    "    SPARSE_SGD = auto()\n",
    "\n",
    "class EmbeddingDict(torch.nn.Embedding):\n",
    "    def __init__(self, inst_keys=None, init_inst_param=1.0, **kwargs):\n",
    "        self.inst_keys = inst_keys\n",
    "        self.device = kwargs.get('device', 'cpu')\n",
    "        super().__init__(len(inst_keys), 1, sparse=True, **kwargs)\n",
    "        self.weight.data = torch.ones_like(self.weight.data, device=self.device)\n",
    "\n",
    "    def __getitem__(self, key_list):\n",
    "        return self(torch.tensor([self.inst_keys.index(key) for key in key_list], device=self.device))\n",
    "\n",
    "\n",
    "class DataParameterManager():\n",
    "\n",
    "    def __init__(self, instance_keys, class_keys, config=None, device='cpu'):\n",
    "\n",
    "        # Make settings available via .property accessor\n",
    "        config = DotDict(config)\n",
    "\n",
    "        self.data_param_mode = config.data_param_mode\n",
    "        self.disabled = False or self.data_param_mode == DataParamMode.DISABLED\n",
    "\n",
    "        self.instance_keys = instance_keys\n",
    "        self.class_keys = class_keys\n",
    "\n",
    "        self.nr_instances = len(instance_keys)\n",
    "        self.nr_classes = len(class_keys)\n",
    "\n",
    "        self.init_inst_param = config.init_inst_param\n",
    "        self.lr_inst_param = config.lr_inst_param\n",
    "\n",
    "        self.init_class_param = config.init_class_param\n",
    "        self.lr_class_param = config.lr_class_param\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Configure weight decay\n",
    "        self.wd_inst_param = config.wd_inst_param\n",
    "        self.wd_class_param = config.wd_class_param\n",
    "\n",
    "        # Configure data parameter clamping\n",
    "        self.skip_clamp_data_param = config.skip_clamp_data_param\n",
    "\n",
    "        self.clamp_sigma_min = config.clamp_sigma_min\n",
    "        self.clamp_sigma_max = config.clamp_sigma_max\n",
    "\n",
    "        if config.optim_algorithm == DataParamOptim.SGD\\\n",
    "            or config.optim_algorithm == DataParamOptim.SPARSE_SGD:\n",
    "            assert 'momentum' in config.optim_options, \\\n",
    "                \"Data parameter optimization with SGD needs momentum > 0 to be specified \"\\\n",
    "                \"otherwise optimization will fail.\"\n",
    "\n",
    "        self.optim_algorithm = config.optim_algorithm\n",
    "        self.optim_options = config.optim_options\n",
    "\n",
    "        # Prepare the data parameters and optimizer\n",
    "        (self.data_parameters_dict,\n",
    "         self.dp_optimizer) = self.get_data_params_n_optimizer(device)\n",
    "\n",
    "    def get_data_params_n_optimizer(self, device):\n",
    "        \"\"\"Returns class and instance level data parameters and their corresponding optimizers.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        nr_instances = self.nr_instances\n",
    "        nr_classes = self.nr_classes\n",
    "\n",
    "        data_parameters_dict = dict()\n",
    "\n",
    "        if self.data_param_mode == DataParamMode.DISABLED:\n",
    "            return (None, None)\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.ONLY_INSTANCE_PARAMS:\n",
    "            # Create nr_instances data parameters\n",
    "            # params = torch.nn.Embedding(len(self.instance_keys), 1, sparse=True)\n",
    "            # params.weight.data = torch.tensor([self.init_inst_param])\n",
    "\n",
    "            # for pinst_idx, inst_key in enumerate(self.instance_keys):\n",
    "            #     # param = torch.ones(1, requires_grad=True, device=device) * self.init_inst_param\n",
    "            #     # param = torch.nn.parameter.Parameter(param)\n",
    "            data_parameters_dict = EmbeddingDict(self.instance_keys, self.init_inst_param, device=self.device)\n",
    "\n",
    "            print(f\"Initialized instance data parameters with: {self.init_inst_param}\")\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.ONLY_CLASS_PARAMS:\n",
    "            # Create nr_classes data parameters\n",
    "            for pcls_idx, class_key in enumerate(self.class_keys):\n",
    "                param = torch.ones(1, requires_grad=True, device=device) * self.init_class_param\n",
    "                param = torch.nn.parameter.Parameter(param)\n",
    "                data_parameters_dict[class_key] = param\n",
    "\n",
    "            print(f\"Initialized class data parameters with: {self.init_class_param}\")\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.COMBINED_INSTANCE_CLASS_PARAMS:\n",
    "            # Create nr_instances * nr_classes data parameters\n",
    "            for pinst_idx, inst_key in enumerate(self.instance_keys):\n",
    "                cls_dict = {}\n",
    "                for pcls_idx, class_key in enumerate(self.class_keys):\n",
    "                    param = torch.ones(1, requires_grad=True, device=device) * (self.init_inst_param + self.init_class_param)\n",
    "                    param = torch.nn.parameter.Parameter(param)\n",
    "                    cls_dict[class_key] = param\n",
    "\n",
    "                data_parameters_dict[inst_key] = cls_dict.copy()\n",
    "\n",
    "            print(f\"Initialized combined data parameters with: {self.init_inst_param + self.init_class_param}\")\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.SEPARATE_INSTANCE_CLASS_PARAMS:\n",
    "            # Create nr_instances + nr_classes data parameters\n",
    "            for p_idx, dp_key \\\n",
    "                in enumerate(list(self.instance_keys)+list(self.class_keys)):\n",
    "\n",
    "                key_prefix='dp_inst:' if p_idx < nr_instances else 'dp_class:'\n",
    "                init_val = self.init_inst_param \\\n",
    "                    if p_idx < nr_instances else self.init_class_param\n",
    "\n",
    "                param = torch.ones(1, requires_grad=True, device=device) * init_val\n",
    "                param = torch.nn.parameter.Parameter(param)\n",
    "                data_parameters_dict[key_prefix+str(dp_key)] = param\n",
    "\n",
    "            print(f\"Initialized instance data parameters with: {self.init_inst_param}\")\n",
    "            print(f\"Initialized class data parameters with: {self.init_class_param}\")\n",
    "\n",
    "        else:\n",
    "            raise(ValueError(f\"Specified mode '{self.data_param_mode}' is not implemented.\"))\n",
    "\n",
    "        # Setup torch.nn.parameter.Parameters\n",
    "        self.data_parameters_dict = data_parameters_dict\n",
    "\n",
    "        # Build parameter groups for optimizer\n",
    "        if self.data_param_mode == DataParamMode.ONLY_INSTANCE_PARAMS:\n",
    "            param_groups = \\\n",
    "                [{'params': data_parameters_dict.parameters(), 'lr': self.lr_inst_param}]\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.ONLY_CLASS_PARAMS:\n",
    "            param_groups = \\\n",
    "                [{'params': self.get_parameter_list(class_keys='all'), 'lr': self.lr_class_param}]\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.COMBINED_INSTANCE_CLASS_PARAMS:\n",
    "            param_groups = \\\n",
    "                [{'params': self.get_parameter_list(inst_keys='all', class_keys='all'), \\\n",
    "                    'lr': max(self.lr_inst_param, self.lr_class_param)}]\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.SEPARATE_INSTANCE_CLASS_PARAMS:\n",
    "            param_groups = \\\n",
    "                [{'params': self.get_parameter_list(inst_keys='all'), 'lr': self.lr_inst_param}] \\\n",
    "                + [{'params': self.get_parameter_list(class_keys='all'), 'lr': self.lr_class_param}]\n",
    "\n",
    "        # Select optimizer\n",
    "        if self.optim_algorithm == DataParamOptim.ADAM:\n",
    "            # dp_optimizer = torch.optim.SparseAdam(data_parameters_dict.parameters(), **self.optim_options)\n",
    "            dp_optimizer = torch.optim.SparseAdam(param_groups, **self.optim_options)\n",
    "\n",
    "        elif self.optim_algorithm == DataParamOptim.SGD:\n",
    "            dp_optimizer = torch.optim.SGD(param_groups, **self.optim_options)\n",
    "\n",
    "        elif self.optim_algorithm == DataParamOptim.SPARSE_SGD:\n",
    "            dp_optimizer = SparseSGD(param_groups, **self.optim_options, skip_update_zero_grad=True)\n",
    "\n",
    "        else:\n",
    "            raise(ValueError(f\"Specified optimizer algorithm '{self.optim_algorithm}' is not implemented.\"))\n",
    "\n",
    "        return data_parameters_dict, dp_optimizer\n",
    "\n",
    "\n",
    "\n",
    "    def parametrify_logits(self, bare_logits, inst_keys=(), reduced_onehot_targets=()):\n",
    "\n",
    "        B, *SPATIAL_DIMS, CLS = bare_logits.shape\n",
    "        num_dims = bare_logits.dim()\n",
    "\n",
    "        if self.class_keys:\n",
    "            assert CLS == self.nr_classes, \\\n",
    "                f\"Logits shape should be BxSPATIAL_DIMSxCLS but got {bare_logits.shape} \"\\\n",
    "                f\"with CLS={CLS} != len(self.class_keys)={self.nr_classes}.\"\n",
    "\n",
    "        if self.data_param_mode == DataParamMode.ONLY_INSTANCE_PARAMS:\n",
    "            assert inst_keys != None, \"Please specify inst_keys.\"\n",
    "            d_params = self.get_parameter_tensor(inst_keys=inst_keys).exp()\n",
    "\n",
    "            l_shape = torch.Size((B,) + (1,)*(num_dims-1))\n",
    "            # Logits have shape BxSPATIAL_DIMSxCLS\n",
    "            # Divide along batch dim\n",
    "            parametrified_logits = bare_logits / d_params.view(l_shape)\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.ONLY_CLASS_PARAMS:\n",
    "\n",
    "            # Now get only class parameters for class labeled in target of instance in batch\n",
    "            # Because all instances share their class parameters loading all class parameters\n",
    "            # also for unlabeled classes will affect untargeted classes\n",
    "            d_params = self.get_sparse_class_params(inst_keys, reduced_onehot_targets).exp()\n",
    "\n",
    "            l_shape = torch.Size((B,) + (1,)*(num_dims-2) + (CLS,))\n",
    "            # Logits have shape BxSPATIAL_DIMSxCLS\n",
    "            # Divide along class onehot dim\n",
    "            parametrified_logits = bare_logits / d_params.view(l_shape)\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.SEPARATE_INSTANCE_CLASS_PARAMS:\n",
    "            inst_params = self.get_parameter_tensor(inst_keys=inst_keys).exp()\n",
    "\n",
    "            # Class params have shape BxCLS\n",
    "            class_params = self.get_sparse_class_params(inst_keys, reduced_onehot_targets).exp()\n",
    "\n",
    "            l_shape_inst = torch.Size((B,) + (1,)*(num_dims-1))\n",
    "            l_shape_class = torch.Size((B,)+ (1,)*(num_dims-2) + (CLS,))\n",
    "            # Logits have shape BxSPATIAL_DIMSxCLS\n",
    "            d_params = (\n",
    "                inst_params.view(l_shape_inst)\n",
    "                + class_params.view(l_shape_class)\n",
    "            )\n",
    "\n",
    "            parametrified_logits = bare_logits / d_params\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.COMBINED_INSTANCE_CLASS_PARAMS:\n",
    "            # OPTION 1: Everytime load all class parameters for every instance. They are not shared.\n",
    "            # d_params = self.get_parameter_tensor(inst_keys=inst_keys, class_keys='all').exp()\n",
    "\n",
    "            # # OPTION 2: Only load the specific class parameters which are labeled in the instance target\n",
    "            d_params = self.get_sparse_class_params(inst_keys, reduced_onehot_targets).exp()\n",
    "\n",
    "            l_shape = torch.Size((B,) + (1,)*(num_dims-2) + (CLS,))\n",
    "            # Logits have shape BxSPATIAL_DIMSxCLS\n",
    "            parametrified_logits = bare_logits / d_params.view(l_shape)\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.DISABLED:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return parametrified_logits\n",
    "\n",
    "\n",
    "\n",
    "    def get_sparse_class_params(self, inst_keys, reduced_onehot_targets):\n",
    "        # Return class params for batch. Only return class params which are\n",
    "        # referenced in target atlas. Returns torch.Size(BxCLS)\n",
    "        d_params = []\n",
    "        for i_key, i_targets in zip(inst_keys, reduced_onehot_targets):\n",
    "            # Convert class indices to corresponding class keys\n",
    "            inst_c_keys = [key for o_h, key in zip(i_targets, self.class_keys) \\\n",
    "                if o_h > 0]\n",
    "\n",
    "            if self.data_param_mode == DataParamMode.SEPARATE_INSTANCE_CLASS_PARAMS:\n",
    "                # As parameters are split it is not possible to get a class param of an instance here\n",
    "                d_inst_c_params = self.get_parameter_tensor(\n",
    "                    class_keys=inst_c_keys, expand_to_full_classes=True\n",
    "                )\n",
    "            else:\n",
    "                d_inst_c_params = self.get_parameter_tensor(\n",
    "                    inst_keys=[i_key], class_keys=inst_c_keys, expand_to_full_classes=True\n",
    "                )\n",
    "\n",
    "            d_params.append(d_inst_c_params)\n",
    "\n",
    "        return torch.stack(d_params, dim=0)\n",
    "\n",
    "\n",
    "    def apply_weight_decay(self, loss, inst_keys):\n",
    "        \"\"\"Applies weight decay on class and instance level data parameters.\n",
    "\n",
    "        We apply weight decay on only those data parameters which participate in a mini-batch.\n",
    "        To apply weight-decay on a subset of data parameters, we explicitly include l2 penalty on these data\n",
    "        parameters in the computational graph. Note, l2 penalty is applied in log domain. This encourages\n",
    "        data parameters to stay close to value 1, and prevents data parameters from obtaining very high or\n",
    "        low values.\n",
    "\n",
    "        Returns:\n",
    "            loss (torch.Tensor): loss augmented with l2 penalty on data parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.data_param_mode == None:\n",
    "            pass\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.ONLY_INSTANCE_PARAMS:\n",
    "            if self.wd_inst_param > .0:\n",
    "                loss += 0.5 * self.wd_inst_param * (self.get_parameter_tensor(inst_keys=inst_keys) ** 2).sum()\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.ONLY_CLASS_PARAMS:\n",
    "            if self.wd_class_param > .0:\n",
    "                loss += 0.5 * self.wd_class_param * (self.get_parameter_tensor(class_keys='all').exp() ** 2).sum()\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.COMBINED_INSTANCE_CLASS_PARAMS:\n",
    "            if self.wd_class_param > .0:\n",
    "                loss += 0.5 * self.wd_class_param * ( self.get_parameter_tensor(inst_keys, 'all').exp() ** 2).sum()\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.SEPARATE_INSTANCE_CLASS_PARAMS:\n",
    "            if self.wd_inst_param > .0:\n",
    "                loss += 0.5 * self.wd_inst_param * (self.get_parameter_tensor(inst_keys=inst_keys) ** 2).sum()\n",
    "\n",
    "            if self.wd_class_param > .0:\n",
    "                loss += 0.5 * self.wd_class_param * (self.get_parameter_tensor(class_keys='all') ** 2).sum()\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def clamp(self):\n",
    "        \"\"\"Clamps class and instance level parameters within specified range.\n",
    "        \"\"\"\n",
    "        if (self.data_param_mode != None) and (not self.skip_clamp_data_param):\n",
    "\n",
    "            for param in self.get_flat_parameter_list():\n",
    "                param.data.clamp_(self.clamp_sigma_min, self.clamp_sigma_max)\n",
    "\n",
    "\n",
    "    def do_basic_train_step(self, loss_fn, logits, target, optimizer, inst_keys=(),\n",
    "                            scaler=None):\n",
    "\n",
    "        assert target.dtype == torch.long, \"target must be one-hot-encoded long.\"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if self.disabled:\n",
    "            loss = loss_fn(logits, target.float())\n",
    "\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            return logits, loss.item()\n",
    "\n",
    "        else:\n",
    "            if self.optim_algorithm == DataParamOptim.ADAM \\\n",
    "                or self.optim_algorithm == DataParamOptim.SPARSE_SGD:\n",
    "                self.dp_optimizer.zero_grad(set_to_none=False)\n",
    "\n",
    "            elif self.optim_algorithm == DataParamOptim.SGD:\n",
    "                self.dp_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Do only sum over spatial dimensions (not batch and one-hot-dimension)\n",
    "            reduction_dims = tuple(range(1,target.dim()-1))\n",
    "            # Get a list of all available class indices in target (inverse one-hot)\n",
    "            if reduction_dims != ():\n",
    "                reduced_onehot_targets = target.sum(reduction_dims).clip(0,1)\n",
    "            else:\n",
    "                reduced_onehot_targets = target.clip(0,1)\n",
    "\n",
    "            dp_logits = self.parametrify_logits(logits, inst_keys, reduced_onehot_targets)\n",
    "\n",
    "            loss = loss_fn(dp_logits.float(), target.float())\n",
    "            loss = self.apply_weight_decay(loss, inst_keys)\n",
    "\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.step(self.dp_optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.dp_optimizer.step()\n",
    "\n",
    "            self.clamp()\n",
    "\n",
    "        return dp_logits, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    def get_data_parameters_dict(self) -> dict:\n",
    "        return self.data_parameters_dict\n",
    "\n",
    "\n",
    "\n",
    "    def get_flat_parameter_list(self):\n",
    "        self.data_parameters_dict.weight.data\n",
    "        return self.data_parameters_dict.weight.data\n",
    "\n",
    "\n",
    "\n",
    "    def get_parameter_tensor(self, inst_keys=(), class_keys=(), expand_to_full_classes=False) -> torch.Tensor:\n",
    "\n",
    "        i_len = self.nr_instances if inst_keys == 'all' else len(inst_keys)\n",
    "        c_len = self.nr_classes if (class_keys == 'all' or expand_to_full_classes) else len(class_keys)\n",
    "\n",
    "        if expand_to_full_classes:\n",
    "            select_cls_idxs = torch.tensor([c_idx for c_idx, key in enumerate(self.class_keys) if key in class_keys]).long()\n",
    "        else:\n",
    "            select_cls_idxs = torch.arange(c_len)\n",
    "\n",
    "        if self.data_param_mode == DataParamMode.ONLY_INSTANCE_PARAMS:\n",
    "            # assert inst_keys != (), \\\n",
    "            # \"Please specifiy instance keys for 'DataParamMode.ONLY_INSTANCE_PARAMS'.\"\n",
    "\n",
    "            # params = self.get_parameter_list(inst_keys=inst_keys, class_keys=class_keys)\n",
    "            # return torch.cat(params)\n",
    "            return self.data_parameters_dict[inst_keys]\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.ONLY_CLASS_PARAMS:\n",
    "            assert class_keys != (), \\\n",
    "            \"Please specifiy class keys for 'DataParamMode.ONLY_CLASS_PARAMS'.\"\n",
    "\n",
    "            # Initialize for sparse class tensor here\n",
    "            tens = torch.ones((c_len), device=self.device) * self.init_class_param\n",
    "            params = self.get_parameter_list(inst_keys=inst_keys, class_keys=class_keys)\n",
    "            tens[select_cls_idxs] = torch.cat(params)\n",
    "            return tens\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.SEPARATE_INSTANCE_CLASS_PARAMS:\n",
    "            assert (inst_keys != ()) != (class_keys != ()), \\\n",
    "            \"Please specify either instance or class keys for \"\\\n",
    "            \"'DataParamMode.SEPARATE_INSTANCE_CLASS_PARAMS'.\"\n",
    "\n",
    "            if inst_keys != ():\n",
    "                params = self.get_parameter_list(inst_keys=inst_keys)\n",
    "                tens = torch.cat(params)\n",
    "            else:\n",
    "                # Initialize for sparse class tensor here\n",
    "                tens = torch.ones((c_len), device=self.device) * self.init_class_param\n",
    "                params = self.get_parameter_list(class_keys=class_keys)\n",
    "                tens[select_cls_idxs] = torch.cat(params)\n",
    "                return tens\n",
    "\n",
    "            return tens\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.COMBINED_INSTANCE_CLASS_PARAMS:\n",
    "            assert (inst_keys != ()) and (class_keys != ()), \\\n",
    "            \"Please specify instance and class keys for \"\\\n",
    "            \"'DataParamMode.COMBINED_INSTANCE_CLASS_PARAMS'.\"\n",
    "\n",
    "            # Initialize for sparse class tensor here\n",
    "            params = self.get_parameter_list(inst_keys=inst_keys, class_keys=class_keys)\n",
    "            if expand_to_full_classes:\n",
    "                params = torch.cat(params).view(i_len, -1)\n",
    "                tens = torch.ones((i_len, self.nr_classes), device=self.device) * (self.init_inst_param + self.init_class_param)\n",
    "            else:\n",
    "                params = torch.cat(params).view(i_len, c_len)\n",
    "                tens = torch.ones((i_len, c_len), device=self.device) * (self.init_inst_param + self.init_class_param)\n",
    "\n",
    "            tens[:,select_cls_idxs] = params\n",
    "            return tens\n",
    "\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "\n",
    "    def get_parameter_list(self, inst_keys=(), class_keys=()) -> list:\n",
    "\n",
    "        if inst_keys == 'all':\n",
    "            inst_keys = self.instance_keys\n",
    "        if class_keys == 'all':\n",
    "            class_keys = self.class_keys\n",
    "\n",
    "        if self.data_param_mode == DataParamMode.ONLY_INSTANCE_PARAMS:\n",
    "            assert inst_keys != (), \\\n",
    "            \"Please specifiy instance keys for 'DataParamMode.ONLY_INSTANCE_PARAMS'.\"\n",
    "            return [self.data_parameters_dict[key] for key in inst_keys]\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.ONLY_CLASS_PARAMS:\n",
    "            assert class_keys != (), \\\n",
    "            \"Please specifiy class keys for 'DataParamMode.ONLY_CLASS_PARAMS'.\"\n",
    "            return[self.data_parameters_dict[key] for key in class_keys]\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.COMBINED_INSTANCE_CLASS_PARAMS:\n",
    "            assert (inst_keys != ()) and (class_keys != ()), \\\n",
    "            \"Please specify instance and class keys for \"\\\n",
    "            \"'DataParamMode.COMBINED_INSTANCE_CLASS_PARAMS'.\"\n",
    "\n",
    "            params = []\n",
    "            for ikey in inst_keys:\n",
    "                for ckey in class_keys:\n",
    "                    params.append(self.data_parameters_dict[ikey][ckey])\n",
    "\n",
    "            return params\n",
    "\n",
    "        elif self.data_param_mode == DataParamMode.SEPARATE_INSTANCE_CLASS_PARAMS:\n",
    "            assert (inst_keys != ()) != (class_keys != ()), \\\n",
    "            \"Please specify either instance or class keys for \"\\\n",
    "            \"'DataParamMode.SEPARATE_INSTANCE_CLASS_PARAMS'.\"\n",
    "\n",
    "            if inst_keys != ():\n",
    "                key_prefix = 'dp_inst:'\n",
    "                dp_keys = inst_keys\n",
    "            else:\n",
    "                key_prefix = 'dp_class:'\n",
    "                dp_keys = class_keys\n",
    "\n",
    "            return [self.data_parameters_dict[key_prefix+str(key)] for key in dp_keys]\n",
    "\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "\n",
    "    def set_enabled(self, enabled=True):\n",
    "        self.disabled = not enabled\n",
    "\n",
    "\n",
    "\n",
    "def get_basic_config_adam():\n",
    "    config = DotDict({\n",
    "        'data_param_mode': DataParamMode.ONLY_INSTANCE_PARAMS,\n",
    "        'init_class_param': 0.01,\n",
    "        'lr_class_param': 0.1,\n",
    "        'init_inst_param': 1.0,\n",
    "        'lr_inst_param': 0.1,\n",
    "        'wd_inst_param': 0.0,\n",
    "        'wd_class_param': 0.0,\n",
    "\n",
    "        'skip_clamp_data_param': False,\n",
    "        'clamp_sigma_min': np.log(1/20),\n",
    "        'clamp_sigma_max': np.log(20),\n",
    "        'optim_algorithm': DataParamOptim.ADAM,\n",
    "        'optim_options': dict(\n",
    "                betas=(0.9, 0.999)\n",
    "            )\n",
    "    })\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "def get_basic_config_sgd():\n",
    "    config = DotDict({\n",
    "        'data_param_mode': DataParamMode.ONLY_INSTANCE_PARAMS,\n",
    "        'init_class_param': 0.01,\n",
    "        'lr_class_param': 0.1,\n",
    "        'init_inst_param': 1.0,\n",
    "        'lr_inst_param': 0.1,\n",
    "        'wd_inst_param': 0.0,\n",
    "        'wd_class_param': 0.0,\n",
    "\n",
    "        'skip_clamp_data_param': False,\n",
    "        'clamp_sigma_min': np.log(1/20),\n",
    "        'clamp_sigma_max': np.log(20),\n",
    "        'optim_algorithm': DataParamOptim.SGD,\n",
    "        'optim_options': dict(\n",
    "                momentum=.9\n",
    "            )\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72629711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.datasets.MNIST(root=\"../data\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515302ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "config = dotdict({\n",
    "    # Data parameter config\n",
    "    'data_param_mode': DataParamMode.ONLY_INSTANCE_PARAMS,\n",
    "    'init_class_param': 1.0, \n",
    "    'lr_class_param': 0.1,\n",
    "    'init_inst_param': 1.0, \n",
    "    'lr_inst_param': 0.1,\n",
    "    'wd_inst_param': 0.0,\n",
    "    'wd_class_param': 0.0,\n",
    "    \n",
    "    'skip_clamp_data_param': False,\n",
    "    'clamp_sigma_min': np.log(1/20),\n",
    "    'clamp_sigma_max': np.log(20),\n",
    "    'optim_algorithm': DataParamOptim.ADAM,\n",
    "    'optim_options': dict(\n",
    "        # momentum=.9\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['background', 'six']\n",
    "dpm = DataParameterManager(instance_keys=range(800), class_keys=CLASSES, config=config, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(scipy.io.loadmat('../data/mnist-six.mat')['data']).float()\n",
    "segment = (data>150).long()\n",
    "segment2 = (data>5).long()\n",
    "\n",
    "\n",
    "image = (data/255 + torch.randn_like(data)*.1).unsqueeze(1)\n",
    "print(data.shape)\n",
    "plt.imshow(image[152,0].data.cpu())\n",
    "plt.show()\n",
    "plt.imshow(segment2[152].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49440b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_flip = torch.rand(800)>0.7#[::2]\n",
    "# do_flip = torch.rand(800)>0.0#[::2]\n",
    "idx_flip = torch.empty(0).long()\n",
    "for i in range(800):\n",
    "    if(do_flip[i]):\n",
    "        idx_flip = torch.cat((idx_flip,torch.tensor([i])))\n",
    "        segment[i] = torch.roll(segment2[i].transpose(-2,-1),(int(torch.randn(1)*5),int(torch.randn(1)*5)),(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(segment[idx_flip[0]].cpu())\n",
    "print(segment[0].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0cd1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(1,32,3,padding=1),nn.BatchNorm2d(32),nn.ReLU(),\\\n",
    "                   nn.Conv2d(32,32,3,padding=1),nn.BatchNorm2d(32),nn.ReLU(),\\\n",
    "                   nn.Conv2d(32,32,3,padding=1),nn.BatchNorm2d(32),nn.ReLU(),\\\n",
    "                   nn.Conv2d(32,2,3,padding=1))\n",
    "\n",
    "\n",
    "embedding = nn.Embedding(800, 1, sparse=True)\n",
    "embedding\n",
    "run_loss_mattias = torch.zeros(500)\n",
    "run_cc_mattias = torch.zeros(500)\n",
    "\n",
    "net = net.cuda()\n",
    "embedding = embedding.cuda()\n",
    "image = image.cuda()\n",
    "segment = segment.cuda()\n",
    "run_loss_mattias = run_loss_mattias.cuda()\n",
    "run_cc_mattias = run_cc_mattias.cuda()\n",
    "do_flip = do_flip.cuda()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08)\n",
    "optimizer_data = torch.optim.SparseAdam(embedding.parameters(), lr=0.1, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "for i in range(500):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    idx = torch.randperm(800)[:64]\n",
    "    idx = idx.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_data.zero_grad()\n",
    "\n",
    "    input = image[idx]\n",
    "    output = net(input)\n",
    "\n",
    "    loss_mattias = nn.CrossEntropyLoss(reduction='none')(output,segment[idx]).mean(-1).mean(-1)\n",
    "    \n",
    "    run_cc_mattias[i] = np.corrcoef(torch.sigmoid(embedding.weight).cpu().data.squeeze().numpy(),do_flip.float().cpu().numpy())[0,1]\n",
    "    run_loss_mattias[i] = (loss_mattias*(1-do_flip[idx].float())).mean().item()\n",
    "\n",
    "    # print(loss)\n",
    "    weight = torch.sigmoid(embedding(idx)).squeeze()\n",
    "    weight = weight/weight.mean()\n",
    "    \n",
    "    (loss_mattias*weight).sum().backward()\n",
    "    optimizer.step()\n",
    "    optimizer_data.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aebddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data parameter manager\n",
    "CLASSES = ['background', 'six']\n",
    "dpm = DataParameterManager(instance_keys=list(range(800)), class_keys=CLASSES, config=config, device='cuda')\n",
    "\n",
    "net2 = nn.Sequential(nn.Conv2d(1,32,3,padding=1),nn.BatchNorm2d(32),nn.ReLU(),\\\n",
    "                   nn.Conv2d(32,32,3,padding=1),nn.BatchNorm2d(32),nn.ReLU(),\\\n",
    "                   nn.Conv2d(32,32,3,padding=1),nn.BatchNorm2d(32),nn.ReLU(),\\\n",
    "                   nn.Conv2d(32,2,3,padding=1))\n",
    "\n",
    "optimizer2 = torch.optim.Adam(net2.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "run_loss2 = torch.zeros(500)\n",
    "run_cc2 = torch.zeros(500)\n",
    "\n",
    "image = image.cuda()\n",
    "segment = segment.cuda()\n",
    "net2 = net2.cuda()\n",
    "run_loss2 = run_loss2.cuda()\n",
    "run_cc2 = run_cc2.cuda()\n",
    "do_flip = do_flip.cuda()\n",
    "\n",
    "for i in range(500):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    idx = torch.randperm(800)[:64]\n",
    "    idx = idx.cuda()\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "\n",
    "    # Alt impl\n",
    "    _input = image[idx]\n",
    "    logits2 = net2(_input)\n",
    "    logits2 = logits2.permute(0,2,3,1)\n",
    "    _, loss = dpm.do_basic_train_step(\n",
    "        nn.BCEWithLogitsLoss(), \n",
    "        logits2, \n",
    "        torch.nn.functional.one_hot(segment[idx].long()), \n",
    "        optimizer2, \n",
    "        inst_keys=idx.tolist(),\n",
    "        scaler=None)\n",
    "\n",
    "    run_cc2[i] = torch.tensor(np.corrcoef(torch.sigmoid(dpm.get_parameter_tensor(inst_keys=list(range(800))).view(-1)).detach().cpu().numpy(), do_flip.float().cpu().numpy()))[0,1]\n",
    "    run_loss2[i] = (loss*(1-do_flip[idx].float())).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800632d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.sigmoid(embedding.weight.data.cpu()))\n",
    "# plt.plot(torch.sigmoid(dpm.get_parameter_tensor(inst_keys='all')).detach().cpu().numpy())\n",
    "plt.plot(torch.sigmoid(dpm.get_parameter_tensor(inst_keys=list(range(800)))).detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.plot(run_loss_mattias.cpu())\n",
    "plt.plot(run_loss2.cpu())\n",
    "plt.show()\n",
    "\n",
    "plt.plot(run_cc_mattias.cpu())\n",
    "plt.plot(run_cc2.cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(torch.sigmoid(embedding.weight).cpu().data.squeeze().numpy(),do_flip.float().numpy(),'.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
