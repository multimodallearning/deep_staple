{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -3 -4\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import get_overlay_grid\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "\n",
    "import curriculum_deeplab.ml_data_parameters_utils as ml_data_parameters_utils\n",
    "import wandb\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca60d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            # nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        # x = F.adaptive_avg_pool3d(x, (1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')  # , align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = [nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())]\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    " \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "# Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "def create_model(output_classes: int = 14,input_channels: int = 1):\n",
    "    # in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "    in_channels = torch.Tensor([input_channels, 24, 24, 32, 48, 48, 48, 64]).long()\n",
    "    mid_channels = torch.Tensor([64, 128, 192, 192, 256, 256, 256, 384]).long()\n",
    "    out_channels = torch.Tensor([24, 24, 32, 48, 48, 48, 64, 64]).long()\n",
    "    mid_stride = torch.Tensor([1, 1, 1, 2, 1, 1, 1, 1])\n",
    "    net = [nn.Identity()]\n",
    "    for i in range(8):\n",
    "        inc = int(in_channels[i])\n",
    "        midc = int(mid_channels[i])\n",
    "        outc = int(out_channels[i])\n",
    "        strd = int(mid_stride[i])\n",
    "        layer = nn.Sequential(nn.Conv3d(inc, midc, 1, bias=False), nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, midc, 3, stride=strd, padding=1, bias=False, groups=midc),\n",
    "                              nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, outc, 1, bias=False), nn.BatchNorm3d(outc))\n",
    "        if i == 0:\n",
    "            layer[0] = nn.Conv3d(inc, midc, 3, padding=1, stride=2, bias=False)\n",
    "        if (inc == outc) & (strd == 1):\n",
    "            net.append(ResBlock(layer))\n",
    "        else:\n",
    "            net.append(layer)\n",
    "\n",
    "    backbone = nn.Sequential(*net)\n",
    "\n",
    "    count = 0\n",
    "    # weight initialization\n",
    "    for m in backbone.modules():\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            count += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    print('#CNN layer', count)\n",
    "    # complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "    # newer model (one more stride, no groups in head)\n",
    "    aspp = ASPP(64, (2, 4, 8, 16, 32), 128)\n",
    "    head = nn.Sequential(nn.Conv3d(128 + 24, 64, 1, padding=0, groups=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, 64, 3, groups=1, padding=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, output_classes, 1))\n",
    "    return backbone, aspp, head\n",
    "\n",
    "\n",
    "def apply_model(backbone, aspp, head, img, checkpointing=True, return_intermediate=False):\n",
    "    if checkpointing:\n",
    "        x1 = checkpoint(backbone[:3], img)\n",
    "        x2 = checkpoint(backbone[3:], x1)\n",
    "        y = checkpoint(aspp, x2)\n",
    "        y1 = torch.cat((x1, F.interpolate(y, scale_factor=2)), 1)\n",
    "        output_j = checkpoint(head, y1)\n",
    "    else:\n",
    "        x1 = backbone[:3](img)\n",
    "        x2 = backbone[3:](x1)\n",
    "        y = aspp(x2)\n",
    "        y1 = torch.cat((x1, F.interpolate(y, scale_factor=2)), 1)\n",
    "        output_j = head(y1)\n",
    "    if return_intermediate:\n",
    "        return y1,output_j\n",
    "    else:\n",
    "        return output_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a979a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class CrossMoDa_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        ensure_labeled_pairs=True, use_additional_data=False, resample=True,\n",
    "        size:tuple=(96,96,60), normalize:bool=True, max_load_num=None):\n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                ensure_labeled_pairs (bool): Only images with corresponding labels will be loaded (default: True)\n",
    "                \n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "                max_load_num (int): maximum number of pairs to load (uses first max_load_num samples for either images and labels found)\n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        \n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "        \n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "        \n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "        \n",
    "        path = base_dir + state_dir\n",
    "        \n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "            \n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "            \n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "        \n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "        \n",
    "        if domain.lower() == \"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "        \n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "        # First read filepaths\n",
    "        self.img_paths = {}\n",
    "        self.label_paths = {}\n",
    "        \n",
    "        for _path in files:\n",
    "            numeric_id = int(re.findall(r'\\d+', os.path.basename(_path))[0])\n",
    "            if \"_l.nii.gz\" in _path or \"_l_Label.nii.gz\" in _path:\n",
    "                lr_id = ('l',)\n",
    "            elif \"_r.nii.gz\" in _path or \"_r_Label.nii.gz\" in _path:\n",
    "                lr_id = ('r',)\n",
    "            else:\n",
    "                lr_id = tuple()\n",
    "\n",
    "            if \"Label\" in _path:\n",
    "                self.label_paths[(numeric_id,) + lr_id] = _path\n",
    "                    \n",
    "            elif domain in _path:\n",
    "                self.img_paths[(numeric_id,) + lr_id] = _path\n",
    "        \n",
    "        if ensure_labeled_pairs:\n",
    "            pair_idxs = set(self.img_paths).intersection(set(self.label_paths))\n",
    "            if max_load_num:\n",
    "                pair_idxs = list(pair_idxs)[:max_load_num]\n",
    "            self.label_paths = {_id: _path for _id, _path in self.label_paths.items() if _id in pair_idxs}\n",
    "            self.img_paths = {_id: _path for _id, _path in self.img_paths.items() if _id in pair_idxs}\n",
    "        \n",
    "        elif max_load_num:\n",
    "            cut_img_idxs = list(self.img_paths.keys())[:max_load_num]\n",
    "            cut_label_idxs = list(self.label_paths.keys())[:max_load_num]\n",
    "            self.label_paths = {_id: _path for _id, _path in self.label_paths.items() if _id in cut_label_idxs}\n",
    "            self.img_paths = {_id: _path for _id, _path in self.img_paths.items() if _id in cut_img_idxs}\n",
    "            \n",
    "        # Populate data\n",
    "        self.img_data = {}\n",
    "        self.label_data = {}\n",
    "        \n",
    "        #load data\n",
    "        \n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "        id_paths_to_load = list(self.label_paths.items()) + list(self.img_paths.items())\n",
    "        \n",
    "        description = f\"{len(self.img_paths)} images, {len(self.label_paths)} labels\"\n",
    "        \n",
    "        for crossmoda_id, f in tqdm(id_paths_to_load, desc=description):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in f:\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                    \n",
    "                self.label_data[crossmoda_id] = tmp.long()\n",
    "                    \n",
    "            elif domain in f:\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "                \n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "                \n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                    \n",
    "                self.img_data[crossmoda_id] = tmp\n",
    "        \n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(set(self.img_data)==set(self.label_data)))\n",
    "        \n",
    "        img_stack = torch.stack(list(self.img_data.values()), dim=0)\n",
    "        img_mean, img_std = img_stack.mean(), img_stack.std()\n",
    "        \n",
    "        label_stack = torch.stack(list(self.label_data.values()), dim=0)\n",
    "\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(img_stack.shape, img_mean, img_std))\n",
    "        print(\"Label shape: {}, max.: {}\".format(label_stack.shape,torch.max(label_stack)))\n",
    "        print(\"Data import finished. Elapsed time: {:.1f} s\".format(time.time()-t0 ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "\n",
    "    def __getitem__(self, dataset_idx):\n",
    "        map_crossmoda_idx_from_continuos = list(self.img_data.keys())\n",
    "        c_idx = map_crossmoda_idx_from_continuos[dataset_idx]\n",
    "        \n",
    "        image = self.img_data.get(c_idx, torch.tensor([]))\n",
    "        label = self.label_data.get(c_idx, torch.tensor([]))\n",
    "        image_path = self.img_paths.get(c_idx, \"\")\n",
    "        label_path = self.label_paths.get(c_idx, \"\")\n",
    "        \n",
    "        return image, label, dataset_idx, c_idx, image_path, label_path\n",
    "\n",
    "    def get_data(self):\n",
    "        img_stack = torch.stack(list(self.img_data.values()), dim=0)\n",
    "        label_stack = torch.stack(list(self.label_data.values()), dim=0)\n",
    "        \n",
    "        return img_stack, label_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training routine\n",
    "def display_nonempty_seg_slices(img_slices, seg_slices, alpha=.5):\n",
    "    assert img_slices.dim() == 4, \"Image slices need to have dimensions SxCxHxW\"\n",
    "    assert seg_slices.dim() == 3, \"Segmentation slices need to have dimensions SxHxW\"\n",
    "    \n",
    "    color_map = {\n",
    "        0: None, \n",
    "        1: (255,0,0), #ONEHOT id and RGB color\n",
    "        2: (0,255,0)\n",
    "    }\n",
    "\n",
    "    idx_dept_with_segs, *_ = torch.nonzero(seg_slices > 0, as_tuple=True)\n",
    "    \n",
    "    if idx_dept_with_segs.nelement() > 0:\n",
    "        idx_dept_with_segs = idx_dept_with_segs.unique()\n",
    "\n",
    "        img_slices = img_slices[idx_dept_with_segs]\n",
    "        seg_slices = seg_slices[idx_dept_with_segs]\n",
    "\n",
    "        pil_ov, _ = get_overlay_grid(\n",
    "            img_slices, \n",
    "            torch.nn.functional.one_hot(seg_slices, 3), \n",
    "            color_map, n_per_row=10, alpha=alpha\n",
    "        )\n",
    "        display(pil_ov)\n",
    "    else:\n",
    "        print(\"No empty slices to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"source\", state=\"l4\", ensure_labeled_pairs=True, max_load_num=100)\n",
    "validation_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "# target_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "#     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ed00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = torch.utils.data.Subset(training_dataset,range(2))\n",
    "for img, seg, sample_idx, *_ in train_subset:\n",
    "    print(f\"Sample {sample_idx}:\")\n",
    "    img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "    seg_slices = seg.permute(2,0,1)\n",
    "    print(\"With ground-truth overlay\")\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "\n",
    "    print(\"W/o ground-truth overlay\")\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, seg, *_ in validation_dataset:\n",
    "    img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "    seg_slices = seg.permute(2,0,1)\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "    # display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd260b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readd when target dataset is used\n",
    "# img, seg, _ = target_dataset[30]\n",
    "# img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "# seg_slices = seg.permute(2,0,1)\n",
    "# display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "# display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentAffine(img_in, seg_in, strength=0.05):\n",
    "    \"\"\"\n",
    "    3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: img_in batch (torch.cuda.FloatTensor), seg_in batch (torch.cuda.LongTensor)\n",
    "    :return: augmented BxCxTxHxW image batch (torch.cuda.FloatTensor), augmented BxTxHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    B,C,D,H,W = img_in.size()\n",
    "    affine_matrix = (torch.eye(3,4).unsqueeze(0) + torch.randn(B, 3, 4) * strength).to(img_in.device)\n",
    "\n",
    "    meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)), align_corners=False)\n",
    "\n",
    "    img_out = F.grid_sample(img_in, meshgrid, padding_mode='border')\n",
    "    seg_out = F.grid_sample(seg_in.float(), meshgrid, mode='nearest')\n",
    "\n",
    "    return img_out, seg_out.long()\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(img_in,strength=0.05):\n",
    "    return img_in + strength*torch.randn_like(img_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(backbone, aspp, head, inst_parameters, class_parameters, \n",
    "    optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "    scaler, name):\n",
    "    \n",
    "    torch.save(backbone.state_dict(), name + '_backbone.pth')\n",
    "    torch.save(aspp.state_dict(), name + '_aspp.pth')\n",
    "    torch.save(head.state_dict(), name + '_head.pth')\n",
    "    \n",
    "    torch.save(inst_parameters, name + '_inst_parameters.pth')\n",
    "    torch.save(class_parameters, name + '_class_parameters.pth')\n",
    "    \n",
    "    torch.save(optimizer.state_dict(), name + '_optimizer.pth')\n",
    "    torch.save(optimizer_inst_param.state_dict(), name + '_optimizer_inst_param.pth')\n",
    "    torch.save(optimizer_class_param.state_dict(), name + '_optimizer_class_param.pth')\n",
    "    \n",
    "    torch.save(scaler.state_dict(), name + '_grad_scaler.pth')\n",
    "\n",
    "def load_model(name, config, dataset_len):\n",
    "    if config.use_mind:\n",
    "        input_channels = 12\n",
    "    else:\n",
    "        input_channels = 1\n",
    "\n",
    "    backbone, aspp, head = create_model(output_classes=config.num_classes, input_channels=input_channels)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(backbone.parameters()) + list(aspp.parameters()) + list(head.parameters()),\n",
    "        lr=config.lr\n",
    "    )\n",
    "    \n",
    "    (_, _, optimizer_class_param, optimizer_inst_param) = \\\n",
    "        ml_data_parameters_utils.get_class_inst_data_params_n_optimizer(\n",
    "            config.init_class_param, config.learn_class_parameters, config.lr_class_param,\n",
    "            config.init_inst_param, config.learn_inst_parameters, config.lr_inst_param,\n",
    "            nr_classes=config.num_classes,\n",
    "            nr_instances=dataset_len,\n",
    "            device='cuda'\n",
    "        )\n",
    "    \n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    backbone.load_state_dict(torch.load(name + '_backbone.pth'))\n",
    "    aspp.load_state_dict(torch.load(name + '_aspp.pth'))\n",
    "    head.load_state_dict(torch.load(name + '_head.pth'))\n",
    "    \n",
    "    inst_parameters = torch.load(name + '_inst_parameters.pth')\n",
    "    class_parameters = torch.load(name + '_class_parameters.pth')\n",
    "    \n",
    "    optimizer.load_state_dict(torch.load(name + '_optimizer.pth'))\n",
    "    optimizer_inst_param.load_state_dict(torch.load(name + '_optimizer_inst_param.pth'))\n",
    "    optimizer_class_param.load_state_dict(torch.load(name + '_optimizer_class_param.pth'))\n",
    "    \n",
    "    scaler.load_state_dict(torch.load(name + '_grad_scaler.pth'))\n",
    "                                          \n",
    "    return (backbone, aspp, head, inst_parameters, class_parameters, \n",
    "        optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "        scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd4c44-6225-4ed6-bcdd-6b9fc6b30034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "            \n",
    "            \n",
    "def log_n_largest_data_parameters(parameters, log_path, n=10):\n",
    "    data = [[inst_idx, val] for (inst_idx, val) in zip(range(len(parameters)), torch.exp(parameters).tolist())\n",
    "               if val != 1.]\n",
    "\n",
    "    data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    data = data[:n]\n",
    "    table = wandb.Table(data=data, columns = [\"parameter_idx\", \"value\"])\n",
    "    wandb.log({log_path : wandb.plot.bar(table, \"parameter_idx\", \"value\", title=f\"{n} largest parameters\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c902fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disturb_seg(seg):  \n",
    "    return torch.flip(seg, dims=(-3,-2,-1))\n",
    "    # disturbed = (\n",
    "    #     seg.roll(np.random.randint(5,40), dims=-1)\n",
    "    #     .roll(np.random.randint(5,40), dims=-2)\n",
    "    #     .roll(np.random.randint(5,40), dims=-3)\n",
    "    # )\n",
    "    # return disturbed\n",
    "\n",
    "def train_DL(run_name, config, training_dataset):\n",
    "    \n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "    \n",
    "    fold_means_no_bg = []\n",
    "    \n",
    "    fold_iter = enumerate(kf.split(training_dataset))\n",
    "    \n",
    "    if config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "    dataset_info = list(zip(training_dataset.img_paths.values(),training_dataset.label_paths.values()))\n",
    "    wandb.log({'datasets/training_dataset':wandb.Table(columns=['image', 'labels'], data=dataset_info)}, step=0)\n",
    "        \n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "\n",
    "        disturbed_idxs = torch.tensor(np.random.choice(train_idxs, config.disturbed_flipped_num))\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in disturbed_idxs])}, \n",
    "                  step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "        print(\"Will disturb these indexes:\", disturbed_idxs.tolist())\n",
    "        \n",
    "        if config.do_plot:\n",
    "            print(\"Disturbed samples:\")\n",
    "            for d_idx in disturbed_idxs:\n",
    "                img_slices = training_dataset[d_idx][0].permute(2,0,1).unsqueeze(1)\n",
    "                seg_slices = disturb_seg(training_dataset[d_idx][1]).permute(2,0,1).to(dtype=torch.int64)\n",
    "                display_nonempty_seg_slices(img_slices, seg_slices)\n",
    "                        \n",
    "        if config.use_mind:\n",
    "            C =12\n",
    "        else:\n",
    "            C = 1\n",
    "        _, all_segs = training_dataset.get_data()\n",
    "\n",
    "        class_weight = torch.sqrt(1.0/(torch.bincount(all_segs.long().view(-1)).float()))\n",
    "        class_weight = class_weight/class_weight.mean()\n",
    "        class_weight[0] = 0.15\n",
    "        class_weight = class_weight.cuda()\n",
    "        print('inv sqrt class_weight', class_weight)\n",
    "        \n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size, shuffle=False, sampler=train_subsampler, pin_memory=True)\n",
    "        val_dataloader = DataLoader(training_dataset, batch_size=config.batch_size, shuffle=False, sampler=val_subsampler, pin_memory=True)\n",
    "\n",
    "        backbone, aspp, head = create_model(output_classes=config.num_classes, input_channels=C)\n",
    "        optimizer = torch.optim.Adam(list(backbone.parameters())+list(aspp.parameters())+list(head.parameters()),\n",
    "            lr=config.lr)\n",
    "\n",
    "        # Initialize class and instance based temperature\n",
    "        (class_parameters, inst_parameters, optimizer_class_param, optimizer_inst_param) = \\\n",
    "            ml_data_parameters_utils.get_class_inst_data_params_n_optimizer(\n",
    "                config.init_class_param, config.learn_class_parameters, config.lr_class_param,\n",
    "                config.init_inst_param, config.learn_inst_parameters, config.lr_inst_param,\n",
    "                nr_classes=config.num_classes,\n",
    "                nr_instances=len(train_dataloader.dataset),\n",
    "                device='cuda'\n",
    "            )\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(class_weight)\n",
    "        scaler = amp.GradScaler()\n",
    "\n",
    "        top1 = ml_data_parameters_utils.AverageMeter('Acc@1', ':6.2f')\n",
    "        top5 = ml_data_parameters_utils.AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "        backbone.cuda() \n",
    "        backbone.train()\n",
    "        aspp.cuda() \n",
    "        aspp.train()\n",
    "        head.cuda() \n",
    "        head.train()\n",
    "        t0 = time.time()\n",
    "\n",
    "        for epx in range(config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "            backbone.train()\n",
    "            aspp.train()\n",
    "            head.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if config.learn_class_parameters:\n",
    "                optimizer_class_param.zero_grad()\n",
    "            if config.learn_inst_parameters:\n",
    "                optimizer_inst_param.zero_grad()\n",
    "\n",
    "            # Load datta\n",
    "            b_img, b_seg, b_idxs_dataset, *_ = next(iter(train_dataloader))\n",
    "            \n",
    "            for batch_idx, seg_dataset_idx in enumerate(b_idxs_dataset):\n",
    "                # Add disturbance (flipping)\n",
    "                if seg_dataset_idx.item() in disturbed_idxs:\n",
    "                    b_seg[batch_idx] = disturb_seg(b_seg[batch_idx])\n",
    "                    if config.debug and config.do_plot:\n",
    "                        dis_img_slices = b_img[batch_idx].permute(2,0,1).unsqueeze(1)\n",
    "                        dis_seg_slices = b_seg[batch_idx].permute(2,0,1).to(dtype=torch.int64)\n",
    "                        display_nonempty_seg_slices(dis_img_slices, dis_seg_slices)\n",
    "                                \n",
    "            b_img = b_img.unsqueeze(1)\n",
    "            b_seg = b_seg.unsqueeze(1)\n",
    "\n",
    "            b_img, b_seg = b_img.float().cuda(), b_seg.cuda()\n",
    "            b_img, b_seg = augmentAffine(b_img, b_seg, strength=0.1)\n",
    "            \n",
    "            b_img = augmentNoise(b_img, strength=0.02)\n",
    "            \n",
    "            if config.use_mind:\n",
    "                b_img = mindssc(b_img)\n",
    "\n",
    "            b_interpolated_seg = F.interpolate(b_seg.float(), scale_factor=0.5, mode='nearest').long()\n",
    "            b_interpolated_seg = b_interpolated_seg.squeeze(1)\n",
    "            \n",
    "            b_img.requires_grad = True\n",
    "            \n",
    "            #img_mr.requires_grad = True\n",
    "            with amp.autocast(enabled=True):\n",
    "                logits = apply_model(backbone, aspp, head, b_img, checkpointing=True)\n",
    "\n",
    "                if config.learn_class_parameters or config.learn_inst_parameters:\n",
    "                    # Compute data parameters for instances in the minibatch\n",
    "                    class_parameter_minibatch = torch.tensor([0])\n",
    "                    # class_parameter_minibatch = class_parameters[b_seg] TODO: Readd that again\n",
    "                    inst_parameter_minibatch = inst_parameters[b_idxs_dataset]\n",
    "                    data_parameter_minibatch = ml_data_parameters_utils.get_data_param_for_minibatch(\n",
    "                                                    learn_class_parameters=config.learn_class_parameters, \n",
    "                                                    learn_inst_parameters=config.learn_inst_parameters,\n",
    "                                                    class_param_minibatch=class_parameter_minibatch,\n",
    "                                                    inst_param_minibatch=inst_parameter_minibatch)\n",
    "\n",
    "                    # Compute logits scaled by data parameters\n",
    "                    logits = logits / data_parameter_minibatch.view([-1] + [1]*(logits.dim()-1))\n",
    "  \n",
    "                loss = criterion(logits, b_interpolated_seg)\n",
    "                # Apply weight decay on data parameters\n",
    "                if config.learn_class_parameters or config.learn_inst_parameters:\n",
    "                    loss = ml_data_parameters_utils.apply_weight_decay_data_parameters(\n",
    "                        config.learn_inst_parameters, config.wd_inst_param,\n",
    "                        config.learn_class_parameters, config.wd_class_param,\n",
    "                        loss,\n",
    "                        class_parameter_minibatch=class_parameter_minibatch,\n",
    "                        inst_parameter_minibatch=inst_parameter_minibatch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            if config.learn_class_parameters:\n",
    "                scaler.step(optimizer_class_param)\n",
    "            if config.learn_inst_parameters:\n",
    "                scaler.step(optimizer_inst_param)\n",
    "\n",
    "            scaler.update()\n",
    "\n",
    "            # Clamp class and instance level parameters within certain bounds\n",
    "            if config.learn_class_parameters or config.learn_inst_parameters:\n",
    "                ml_data_parameters_utils.clamp_data_parameters(\n",
    "                    config.skip_clamp_data_param, config.learn_inst_parameters, config.learn_class_parameters,\n",
    "                    class_parameters, inst_parameters,\n",
    "                    config.clamp_inst_sigma_config, config.clamp_cls_sigma_config)\n",
    "\n",
    "            # # Measure accuracy and record loss # TODO add again\n",
    "            # acc1, acc5 = ml_data_parameters_utils.compute_topk_accuracy(logits, b_interpolated_seg, topk=(1, 1))\n",
    "            # top1.update(acc1[0], b_img.size(0))\n",
    "            # top5.update(acc5[0], b_img.size(0))\n",
    "            \n",
    "            if epx % config.log_every == 0:\n",
    "                dice = dice3d(\n",
    "                    torch.nn.functional.one_hot(logits.argmax(1), 3),\n",
    "                    torch.nn.functional.one_hot(b_interpolated_seg, 3), one_hot_torch_style=True\n",
    "                )\n",
    "                # Log data parameters\n",
    "                ml_data_parameters_utils.log_intermediate_iteration_stats(\n",
    "                    f\"_fold{fold_idx}\",\n",
    "                    global_idx,\n",
    "                    config.learn_class_parameters, config.learn_inst_parameters,\n",
    "                    class_parameters, inst_parameters, top1, top5)\n",
    "\n",
    "                with amp.autocast(enabled=True):\n",
    "                    backbone.eval()\n",
    "                    aspp.eval()\n",
    "                    head.eval()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        b_val_img, b_val_seg, *_ = next(iter(val_dataloader))\n",
    "                        \n",
    "                        if config.do_plot:\n",
    "                            print(\"Show val img/lbl input\")\n",
    "                            for val_img, val_seg in zip(b_val_img, b_val_seg):\n",
    "                                val_img_slices = val_img.detach().permute(2,0,1).unsqueeze(1)\n",
    "                                val_seg_slices = val_seg.detach().permute(2,0,1).to(dtype=torch.int64)\n",
    "                                display_nonempty_seg_slices(val_img_slices, val_seg_slices)\n",
    "\n",
    "                        b_val_img, b_val_seg = (\n",
    "                            b_val_img.unsqueeze(1).float().cuda(), \n",
    "                            b_val_seg.unsqueeze(1).float().cuda()\n",
    "                        )\n",
    "\n",
    "                        if config.use_mind:\n",
    "                            b_val_img = mindssc(b_val_img)\n",
    "\n",
    "                        b_interpolated_val_seg = F.interpolate(b_val_seg, scale_factor=0.5, mode='nearest').long()\n",
    "                        b_interpolated_val_seg = b_interpolated_val_seg.squeeze(1)\n",
    "                        \n",
    "                        output_val = apply_model(backbone, aspp, head, b_val_img, checkpointing=False)\n",
    "                    \n",
    "                        val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(output_val.argmax(1), 3),\n",
    "                            torch.nn.functional.one_hot(b_interpolated_val_seg, 3), one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        if config.do_plot:\n",
    "                            print(\"Show val lbl/prediction\")\n",
    "                            for val_seg, pred_val_seg in zip(b_val_seg, output_val.argmax(1)):\n",
    "                                val_seg = val_seg.squeeze(0).permute(2,0,1).unsqueeze(1)\n",
    "                                pred_seg_slices = pred_val_seg.permute(2,0,1)\n",
    "                                \n",
    "                                pred_seg_slices = F.upsample_nearest(pred_seg_slices.unsqueeze(0).unsqueeze(0).float(), scale_factor=2).squeeze(0).squeeze(0).long()\n",
    "                                display_nonempty_seg_slices(val_seg, pred_seg_slices)\n",
    "\n",
    "                dice_mean_no_bg = round(dice.mean(dim=0)[1:].mean().item(),4)\n",
    "                val_dice_mean_no_bg = round(val_dice.mean(dim=0)[1:].mean().item(),4)\n",
    "                \n",
    "                print(\n",
    "                    f'epx_fold{fold_idx}', epx, round(time.time()-t0,2),'s',\n",
    "                    f'loss_fold{fold_idx}', round(loss.item(),6),\n",
    "                    f'dice_tensor_fold{fold_idx}', dice, \n",
    "                    f'dice_mean_nobg_fold{fold_idx}', dice_mean_no_bg,\n",
    "                    f'val_dice_mean_nobg_fold{fold_idx}', val_dice_mean_no_bg\n",
    "                )\n",
    "                # Log the epoch idx per fold - so you can recover the diagram by setting \n",
    "                # ref_epoch_idx as x-axis in wandb interface\n",
    "                wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "                \n",
    "                wandb.log({f'losses/loss_fold{fold_idx}': loss}, step=global_idx)\n",
    "                # wandb.log({f'scores/dice_tensor': dice}, step=global_idx)\n",
    "                # wandb.log({f'scores/val_dice_tensor': val_dice}, step=global_idx)\n",
    "                wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': dice_mean_no_bg}, step=global_idx)\n",
    "                wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': round(val_dice_mean_no_bg, 4)}, step=global_idx)\n",
    "                \n",
    "                # print(\"Class parameters: \", class_parameters)\n",
    "                # print(\"Instance parameters: \", inst_parameters)\n",
    "            \n",
    "            if config.debug:\n",
    "                break\n",
    "                \n",
    "        # End of fold loop        \n",
    "        log_n_largest_data_parameters(inst_parameters, \n",
    "            f\"data_parameters/largest_instance_parameters_fold{fold_idx}\", n=30\n",
    "        )\n",
    "        \n",
    "        log_n_largest_data_parameters(inst_parameters, \n",
    "            f\"data_parameters/all_instance_parameters_fold{fold_idx}\", n=len(training_dataset)\n",
    "        )\n",
    "    \n",
    "        # TODO log instance parameters and disturbed instance parameters here\n",
    "        backbone.cpu()\n",
    "        aspp.cpu() \n",
    "        head.cpu()\n",
    "        \n",
    "        save_model(backbone, aspp, head, inst_parameters, class_parameters, \n",
    "            optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "            scaler, f\"{config.mdl_save_prefix}_fold{fold_idx}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': False,\n",
    "    \n",
    "    'num_classes': 3,\n",
    "    'use_mind': True,\n",
    "    'epochs': 2000,\n",
    "    'batch_size': 4,\n",
    "    'lr': 0.001,\n",
    "    # Data parameter config\n",
    "    'init_class_param': 1.0, \n",
    "    'learn_class_parameters': False, \n",
    "    'lr_class_param': 0.1,\n",
    "    'init_inst_param': 1.0, \n",
    "    'learn_inst_parameters': True, \n",
    "    'lr_inst_param': 0.1,\n",
    "    'wd_inst_param': 0.0,\n",
    "    'wd_class_param': 0.0,\n",
    "    \n",
    "    'skip_clamp_data_param': False,\n",
    "    'clamp_inst_sigma_config': {\n",
    "        'min': np.log(1/20),\n",
    "        'max': np.log(20)\n",
    "    },\n",
    "    'clamp_cls_sigma_config': {\n",
    "        'min': np.log(1/20),\n",
    "        'max': np.log(20)\n",
    "    },\n",
    "\n",
    "    'log_every': 50,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "    \n",
    "    'do_plot': False,\n",
    "    'debug': False,\n",
    "    'wandb_mode': \"online\",\n",
    "\n",
    "    'disturbed_flipped_num': 40,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f476a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "    config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "    mode=config_dict['wandb_mode']\n",
    ")\n",
    "run_name = run.name\n",
    "config = wandb.config\n",
    "\n",
    "train_DL(run_name, config, training_dataset)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7118959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_DL(run_name, config, inf_dataset):\n",
    "\n",
    "    score_dicts = []\n",
    "    \n",
    "    fold_iter = range(config.num_folds)\n",
    "    if config_dict['only_first_fold']:\n",
    "        fold_iter = fold_iter[0:1]\n",
    "        \n",
    "    for fold_idx in fold_iter:\n",
    "        backbone, aspp, head, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "        backbone.eval()\n",
    "        aspp.eval()\n",
    "        head.eval()\n",
    "        \n",
    "        for img, seg, sample_idx in inf_dataset:\n",
    "            global_idx = get_global_idx(fold_idx, sample_idx, config.epochs)\n",
    "\n",
    "            img, seg = (\n",
    "                img.unsqueeze(0).unsqueeze(0).float(), \n",
    "                seg.unsqueeze(0).unsqueeze(0)\n",
    "            )\n",
    "\n",
    "            if config.use_mind:\n",
    "                img = mindssc(img)\n",
    "\n",
    "            if config.do_plot:\n",
    "                img_slices = img[0:1].permute(0,1,4,2,3).squeeze().unsqueeze(1)\n",
    "                seg_slices = seg[0:1].permute(0,1,4,2,3).squeeze().to(dtype=torch.int64)\n",
    "                display_nonempty_seg_slices(img_slices, seg_slices)\n",
    "\n",
    "            interpolated_seg = F.interpolate(seg.float(), scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "\n",
    "            img.requires_grad = True\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    interpolated_seg = F.interpolate(seg.float(), scale_factor=0.5, mode='nearest').long()\n",
    "                    interpolated_seg = interpolated_seg.squeeze(1)\n",
    "                    \n",
    "                    output_val = apply_model(backbone, aspp, head, img, checkpointing=False)\n",
    "\n",
    "                    inf_dice = dice3d(\n",
    "                        torch.nn.functional.one_hot(output_val.argmax(1), 3),\n",
    "                        torch.nn.functional.one_hot(interpolated_seg, 3), one_hot_torch_style=True\n",
    "                    )\n",
    "                if config.do_plot:\n",
    "                    lbl_slices = seg.detach().squeeze(0).permute(3,0,1,2)\n",
    "                    pred_seg_slices = output_val.argmax(1).squeeze(0).permute(2,0,1)\n",
    "                    pred_seg_slices = F.upsample_nearest(pred_seg_slices.unsqueeze(0).unsqueeze(0).float(), scale_factor=2).squeeze(0).squeeze(0).long()\n",
    "                    display_nonempty_seg_slices(lbl_slices, pred_seg_slices)\n",
    "\n",
    "                for class_idx, class_dice in enumerate(inf_dice.tolist()[0]):\n",
    "                    score_dicts.append(\n",
    "                        {\n",
    "                            'fold_idx': fold_idx,\n",
    "                            'sample_idx': sample_idx,\n",
    "                            'class_idx': class_idx,\n",
    "                            'dice': class_dice,\n",
    "                        }\n",
    "                    )\n",
    "                # Mean over all classes (w/o background)\n",
    "                dice_mean_no_bg = inf_dice.mean(dim=0)[1:].mean()\n",
    "                wandb.log({f'scores/dice_fold_{fold_idx}': dice_mean_no_bg, 'sample_idx': sample_idx}, step=global_idx)\n",
    "                print(f\"Dice of validation sample {sample_idx} @(fold={fold_idx}): {dice_mean_no_bg.item():.2f}\")\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "    mean_inf_dice = torch.tensor([score['dice'] for score in score_dicts if score['class_idx'] != 0]).mean()\n",
    "    print(f\"Mean dice over all folds, classes and samples: {mean_inf_dice.item()*100:.2f}%\")\n",
    "    wandb.log({'scores/mean_dice_all_folds_samples_classes': mean_inf_dice}, step=global_idx)\n",
    "\n",
    "    return score_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_scores = []\n",
    "run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"testing\", job_type=\"test\",\n",
    "        config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "        mode=config_dict['wandb_mode']\n",
    ")\n",
    "config = wandb.config\n",
    "score_dicts = inference_DL(run_name, config, validation_dataset)\n",
    "folds_scores.append(score_dicts)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352df74b-35a6-4c6f-b986-4dd60ac1e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemberDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        for arg in args:\n",
    "            if isinstance(arg, dict):\n",
    "                for k, v in arg.items():\n",
    "                    self[k] = v\n",
    "\n",
    "        if kwargs:\n",
    "            for k, v in kwargs.items():\n",
    "                self[k] = v\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return self.get(attr)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self.__setitem__(key, value)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        super().__setitem__(key, value)\n",
    "        self.__dict__.update({key: value})\n",
    "\n",
    "    def __delattr__(self, item):\n",
    "        self.__delitem__(item)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        super().__delitem__(key)\n",
    "        del self.__dict__[key]\n",
    "\n",
    "def log_n_largest_data_parameters(inst_parameters, n=10):\n",
    "    data = [[inst_idx, val] for (inst_idx, val) in zip(range(len(inst_parameters)), torch.exp(inst_parameters).tolist())\n",
    "               if val != 1.]\n",
    "\n",
    "    data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    data = data[:n]\n",
    "    table = wandb.Table(data=data, columns = [\"instance_parameter_idx\", \"value\"])\n",
    "    wandb.log({\"data_parameters/instance_parameters\" : wandb.plot.bar(table, \"instance_parameter_idx\", \"value\", title=\"Instance parameters\")})\n",
    "\n",
    "log_n_largest_data_parameters(inst_parameters, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a43fd-16ac-4ef2-becb-753bd99ce191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
