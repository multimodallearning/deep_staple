{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -3 -4\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import get_overlay_grid\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "\n",
    "import curriculum_deeplab.ml_data_parameters_utils as ml_data_parameters_utils\n",
    "import wandb\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca60d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            # nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        # x = F.adaptive_avg_pool3d(x, (1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')  # , align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = [nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())]\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    " \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "# Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "def create_model(output_classes: int = 14,input_channels: int = 1):\n",
    "    # in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "    in_channels = torch.Tensor([input_channels, 24, 24, 32, 48, 48, 48, 64]).long()\n",
    "    mid_channels = torch.Tensor([64, 128, 192, 192, 256, 256, 256, 384]).long()\n",
    "    out_channels = torch.Tensor([24, 24, 32, 48, 48, 48, 64, 64]).long()\n",
    "    mid_stride = torch.Tensor([1, 1, 1, 2, 1, 1, 1, 1])\n",
    "    net = [nn.Identity()]\n",
    "    for i in range(8):\n",
    "        inc = int(in_channels[i])\n",
    "        midc = int(mid_channels[i])\n",
    "        outc = int(out_channels[i])\n",
    "        strd = int(mid_stride[i])\n",
    "        layer = nn.Sequential(nn.Conv3d(inc, midc, 1, bias=False), nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, midc, 3, stride=strd, padding=1, bias=False, groups=midc),\n",
    "                              nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, outc, 1, bias=False), nn.BatchNorm3d(outc))\n",
    "        if i == 0:\n",
    "            layer[0] = nn.Conv3d(inc, midc, 3, padding=1, stride=2, bias=False)\n",
    "        if (inc == outc) & (strd == 1):\n",
    "            net.append(ResBlock(layer))\n",
    "        else:\n",
    "            net.append(layer)\n",
    "\n",
    "    backbone = nn.Sequential(*net)\n",
    "\n",
    "    count = 0\n",
    "    # weight initialization\n",
    "    for m in backbone.modules():\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            count += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    print('#CNN layer', count)\n",
    "    # complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "    # newer model (one more stride, no groups in head)\n",
    "    aspp = ASPP(64, (2, 4, 8, 16, 32), 128)\n",
    "    head = nn.Sequential(nn.Conv3d(128 + 24, 64, 1, padding=0, groups=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, 64, 3, groups=1, padding=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, output_classes, 1))\n",
    "    return backbone, aspp, head\n",
    "\n",
    "\n",
    "def apply_model(backbone, aspp, head, img, checkpointing=True, return_intermediate=False):\n",
    "    if checkpointing:\n",
    "        x1 = checkpoint(backbone[:3], img)\n",
    "        x2 = checkpoint(backbone[3:], x1)\n",
    "        y = checkpoint(aspp, x2)\n",
    "        y1 = torch.cat((x1, F.interpolate(y, scale_factor=2)), 1)\n",
    "        output_j = checkpoint(head, y1)\n",
    "    else:\n",
    "        x1 = backbone[:3](img)\n",
    "        x2 = backbone[3:](x1)\n",
    "        y = aspp(x2)\n",
    "        y1 = torch.cat((x1, F.interpolate(y, scale_factor=2)), 1)\n",
    "        output_j = head(y1)\n",
    "    if return_intermediate:\n",
    "        return y1,output_j\n",
    "    else:\n",
    "        return output_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a979a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class CrossMoDa_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        ensure_labeled_pairs=True, use_additional_data=False, resample = True,\n",
    "        size:tuple = (96,96,60), normalize:bool = True):\n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                ensure_labeled_pairs (bool): Only images with corresponding labels will be loaded (default: True)\n",
    "                \n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "\n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "        path = base_dir + state_dir\n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "        if domain.lower() ==\"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "        if ensure_labeled_pairs:\n",
    "            def get_bare_basename(_path):\n",
    "                return str(Path(_path.replace('.nii.gz', '')).stem)\n",
    "\n",
    "            labeled_files = [\n",
    "                _path for _path in files \\\n",
    "                    if '_Label' in get_bare_basename(_path) \\\n",
    "                    or get_bare_basename(_path)+'_Label' in [get_bare_basename(_path) for _path in files]\n",
    "            ]\n",
    "            files = labeled_files\n",
    "            \n",
    "        #initialize variables\n",
    "        self.imgs = torch.zeros(0,size[0],size[1],size[2])\n",
    "        self.labels = torch.zeros(0,size[0],size[1],size[2])\n",
    "        self.img_nums = []\n",
    "        self.label_nums = []\n",
    "        #load data\n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "\n",
    "        for i,f in enumerate(tqdm(files)):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in f:\n",
    "                self.label_nums.append(int(re.findall(r'\\d+', os.path.basename(f))[0]))\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                self.labels = torch.cat((self.labels,tmp.unsqueeze(0)),dim=0)\n",
    "            elif domain in f:\n",
    "                self.img_nums.append(int(re.findall(r'\\d+', os.path.basename(f))[0]))\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                self.imgs = torch.cat((self.imgs,tmp.unsqueeze(0)),dim=0)\n",
    "        self.labels = self.labels.long()\n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(self.img_nums==self.label_nums))\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(self.imgs.shape,self.imgs.mean(),self.imgs.std()))\n",
    "        print(\"Label shape: {}, max.: {}\".format(self.labels.shape,torch.max(self.labels)))\n",
    "        print(\"Data import finished. Elapsed time: {:.1f} s\".format(time.time()-t0 ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.imgs.size(0))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label, idx \n",
    "\n",
    "    def get_data(self):\n",
    "        return self.imgs,self.labels\n",
    "\n",
    "    def get_image_numbers(self):\n",
    "        return self.img_nums\n",
    "\n",
    "    def get_label_numbers(self):\n",
    "        return self.label_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training routine\n",
    "def display_nonempty_seg_slices(img_slices, seg_slices, alpha=.5):\n",
    "    color_map = {\n",
    "        0: None, \n",
    "        1: (255,0,0), #ONEHOT id and RGB color\n",
    "        2: (0,255,0)\n",
    "    }\n",
    "\n",
    "    idx_dept_with_segs, *_ = torch.nonzero(seg_slices > 0, as_tuple=True)\n",
    "    \n",
    "    if idx_dept_with_segs.nelement() > 0:\n",
    "        idx_dept_with_segs = idx_dept_with_segs.unique()\n",
    "\n",
    "        img_slices = img_slices[idx_dept_with_segs]\n",
    "        seg_slices = seg_slices[idx_dept_with_segs]\n",
    "        \n",
    "        pil_ov, _ = get_overlay_grid(\n",
    "            img_slices, \n",
    "            torch.nn.functional.one_hot(seg_slices, 3), \n",
    "            color_map, n_per_row=10, alpha=alpha\n",
    "        )\n",
    "        display(pil_ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"source\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "validation_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "# target_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "#     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ed00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = torch.utils.data.Subset(training_dataset,range(2))\n",
    "for img, seg, sample_idx in train_subset:\n",
    "    print(f\"Sample {sample_idx}:\")\n",
    "    img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "    seg_slices = seg.permute(2,0,1)\n",
    "    print(\"With ground-truth overlay\")\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "\n",
    "    print(\"W/o ground-truth overlay\")\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, seg, _ in validation_dataset:\n",
    "    img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "    seg_slices = seg.permute(2,0,1)\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "    # display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd260b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, seg, _ = target_dataset[30]\n",
    "img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "seg_slices = seg.permute(2,0,1)\n",
    "display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentAffine(img_in, seg_in, strength=0.05):\n",
    "    \"\"\"\n",
    "    3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: img_in batch (torch.cuda.FloatTensor), seg_in batch (torch.cuda.LongTensor)\n",
    "    :return: augmented BxCxTxHxW image batch (torch.cuda.FloatTensor), augmented BxTxHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    B,C,D,H,W = img_in.size()\n",
    "    affine_matrix = (torch.eye(3,4).unsqueeze(0) + torch.randn(B, 3, 4) * strength).to(img_in.device)\n",
    "\n",
    "    meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)), align_corners=False)\n",
    "\n",
    "    img_out = F.grid_sample(img_in, meshgrid, padding_mode='border')\n",
    "    seg_out = F.grid_sample(seg_in.float(), meshgrid, mode='nearest')\n",
    "\n",
    "    return img_out, seg_out.long()\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(img_in,strength=0.05):\n",
    "    return img_in + strength*torch.randn_like(img_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(backbone, aspp, head, inst_parameters, class_parameters, \n",
    "    optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "    scaler, name):\n",
    "    \n",
    "    torch.save(backbone.state_dict(), name + '_backbone.pth')\n",
    "    torch.save(aspp.state_dict(), name + '_aspp.pth')\n",
    "    torch.save(head.state_dict(), name + '_head.pth')\n",
    "    \n",
    "    torch.save(inst_parameters, name + '_inst_parameters.pth')\n",
    "    torch.save(class_parameters, name + '_class_parameters.pth')\n",
    "    \n",
    "    torch.save(optimizer.state_dict(), name + '_optimizer.pth')\n",
    "    torch.save(optimizer_inst_param.state_dict(), name + '_optimizer_inst_param.pth')\n",
    "    torch.save(optimizer_class_param.state_dict(), name + '_optimizer_class_param.pth')\n",
    "    \n",
    "    torch.save(scaler.state_dict(), name + '_grad_scaler.pth')\n",
    "\n",
    "def load_model(name, config, dataset_len):\n",
    "    if config.use_mind:\n",
    "        input_channels = 12\n",
    "    else:\n",
    "        input_channels = 1\n",
    "\n",
    "    backbone, aspp, head = create_model(output_classes=config.num_classes, input_channels=input_channels)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(backbone.parameters()) + list(aspp.parameters()) + list(head.parameters()),\n",
    "        lr=config.lr\n",
    "    )\n",
    "    \n",
    "    (_, _, optimizer_class_param, optimizer_inst_param) = \\\n",
    "        ml_data_parameters_utils.get_class_inst_data_params_n_optimizer(\n",
    "            config.init_class_param, config.learn_class_parameters, config.lr_class_param,\n",
    "            config.init_inst_param, config.learn_inst_parameters, config.lr_inst_param,\n",
    "            nr_classes=config.num_classes,\n",
    "            nr_instances=dataset_len,\n",
    "            device='cuda'\n",
    "        )\n",
    "    \n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    backbone.load_state_dict(torch.load(name + '_backbone.pth'))\n",
    "    aspp.load_state_dict(torch.load(name + '_aspp.pth'))\n",
    "    head.load_state_dict(torch.load(name + '_head.pth'))\n",
    "    \n",
    "    inst_parameters = torch.load(name + '_inst_parameters.pth')\n",
    "    class_parameters = torch.load(name + '_class_parameters.pth')\n",
    "    \n",
    "    optimizer.load_state_dict(torch.load(name + '_optimizer.pth'))\n",
    "    optimizer_inst_param.load_state_dict(torch.load(name + '_optimizer_inst_param.pth'))\n",
    "    optimizer_class_param.load_state_dict(torch.load(name + '_optimizer_class_param.pth'))\n",
    "    \n",
    "    scaler.load_state_dict(torch.load(name + '_grad_scaler.pth'))\n",
    "                                          \n",
    "    return (backbone, aspp, head, inst_parameters, class_parameters, \n",
    "        optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "        scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3c902fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL(run_name, config_dict, training_dataset):\n",
    "    \n",
    "    kf = KFold(n_splits=config_dict['num_folds'])\n",
    "    kf.get_n_splits(training_dataset)\n",
    "    \n",
    "    fold_means_no_bg = []\n",
    "    \n",
    "    for fold_idx, (train_idxs, val_idxs) in enumerate(kf.split(training_dataset)):\n",
    "        run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"fold{fold_idx}\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "        config = wandb.config\n",
    "\n",
    "        disturbed_idxs = train_idxs[:config.disturbed_flipped_num]\n",
    "\n",
    "        if config.use_mind:\n",
    "            C =12\n",
    "        else:\n",
    "            C = 1\n",
    "        _, all_segs = training_dataset.get_data()\n",
    "\n",
    "        class_weight = torch.sqrt(1.0/(torch.bincount(all_segs.long().view(-1)).float()))\n",
    "        class_weight = class_weight/class_weight.mean()\n",
    "        class_weight[0] = 0.15\n",
    "        class_weight = class_weight.cuda()\n",
    "        print('inv sqrt class_weight', class_weight)\n",
    "        \n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size, shuffle=False, sampler=train_subsampler)\n",
    "        val_dataloader = DataLoader(training_dataset, batch_size=config.batch_size, shuffle=False, sampler=val_subsampler)\n",
    "\n",
    "        backbone, aspp, head = create_model(output_classes=config.num_classes, input_channels=C)\n",
    "        optimizer = torch.optim.Adam(list(backbone.parameters())+list(aspp.parameters())+list(head.parameters()),\n",
    "            lr=config.lr)\n",
    "\n",
    "        # Initialize class and instance based temperature\n",
    "        (class_parameters, inst_parameters, optimizer_class_param, optimizer_inst_param) = \\\n",
    "            ml_data_parameters_utils.get_class_inst_data_params_n_optimizer(\n",
    "                config.init_class_param, config.learn_class_parameters, config.lr_class_param,\n",
    "                config.init_inst_param, config.learn_inst_parameters, config.lr_inst_param,\n",
    "                nr_classes=config.num_classes,\n",
    "                nr_instances=len(train_dataloader.dataset),\n",
    "                device='cuda'\n",
    "            )\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(class_weight)\n",
    "        scaler = amp.GradScaler()\n",
    "\n",
    "        top1 = ml_data_parameters_utils.AverageMeter('Acc@1', ':6.2f')\n",
    "        top5 = ml_data_parameters_utils.AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "        backbone.cuda() \n",
    "        backbone.train()\n",
    "        aspp.cuda() \n",
    "        aspp.train()\n",
    "        head.cuda() \n",
    "        head.train()\n",
    "        t0 = time.time()\n",
    "\n",
    "        for epx in range(config.epochs):\n",
    "            backbone.train()\n",
    "            aspp.train()\n",
    "            head.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if config.learn_class_parameters:\n",
    "                optimizer_class_param.zero_grad()\n",
    "            if config.learn_inst_parameters:\n",
    "                optimizer_inst_param.zero_grad()\n",
    "\n",
    "            # Load datta\n",
    "            b_img, b_seg, idxs_dataset = next(iter(train_dataloader))\n",
    "            \n",
    "            if disturbed_idxs:\n",
    "                # 3D flip the label target\n",
    "                b_seg[disturbed_idxs] = torch.flip(b_seg[disturbed_idxs], dims=(-3,-2,-1))\n",
    "\n",
    "            b_img = b_img.unsqueeze(1)\n",
    "            b_seg = b_seg.unsqueeze(1)\n",
    "\n",
    "            b_img, b_seg = b_img.float().cuda(), b_seg.cuda()\n",
    "            b_img, b_seg = augmentAffine(b_img, b_seg, strength=0.1)\n",
    "            \n",
    "            b_img = augmentNoise(b_img, strength=0.02)\n",
    "            \n",
    "            if config.use_mind:\n",
    "                b_img = mindssc(b_img)\n",
    "\n",
    "            b_interpolated_seg = F.interpolate(b_seg.float(), scale_factor=0.5, mode='nearest').long()\n",
    "            b_interpolated_seg = b_interpolated_seg.squeeze(1)\n",
    "            \n",
    "            b_img.requires_grad = True\n",
    "            \n",
    "            #img_mr.requires_grad = True\n",
    "            with amp.autocast(enabled=True):\n",
    "                logits = apply_model(backbone, aspp, head, b_img, checkpointing=True)\n",
    "\n",
    "                if config.learn_class_parameters or config.learn_inst_parameters:\n",
    "                    # Compute data parameters for instances in the minibatch\n",
    "                    class_parameter_minibatch = torch.tensor([0])\n",
    "                    # class_parameter_minibatch = class_parameters[b_seg] TODO: Readd that again\n",
    "                    inst_parameter_minibatch = inst_parameters[idxs_dataset]\n",
    "                    data_parameter_minibatch = ml_data_parameters_utils.get_data_param_for_minibatch(\n",
    "                                                    learn_class_parameters=learn_class_parameters, \n",
    "                                                    learn_inst_parameters=learn_inst_parameters,\n",
    "                                                    class_param_minibatch=class_parameter_minibatch,\n",
    "                                                    inst_param_minibatch=inst_parameter_minibatch)\n",
    "\n",
    "                    # Compute logits scaled by data parameters\n",
    "                    logits = logits / data_parameter_minibatch.view([-1] + [1]*(logits.dim()-1))\n",
    "  \n",
    "                loss = criterion(logits, b_interpolated_seg)\n",
    "                # Apply weight decay on data parameters\n",
    "                if config.learn_class_parameters or config.learn_inst_parameters:\n",
    "                    loss = ml_data_parameters_utils.apply_weight_decay_data_parameters(\n",
    "                        learn_inst_parameters, wd_inst_param,\n",
    "                        learn_class_parameters, wd_class_param,\n",
    "                        loss,\n",
    "                        class_parameter_minibatch=class_parameter_minibatch,\n",
    "                        inst_parameter_minibatch=inst_parameter_minibatch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            if config.learn_class_parameters:\n",
    "                scaler.step(optimizer_class_param)\n",
    "            if config.learn_inst_parameters:\n",
    "                scaler.step(optimizer_inst_param)\n",
    "\n",
    "            scaler.update()\n",
    "\n",
    "            # Clamp class and instance level parameters within certain bounds\n",
    "            if config.learn_class_parameters or config.learn_inst_parameters:\n",
    "                ml_data_parameters_utils.clamp_data_parameters(\n",
    "                    config.skip_clamp_data_param, config.learn_inst_parameters, config.learn_class_parameters,\n",
    "                    class_parameters, inst_parameters,\n",
    "                    config.clamp_inst_sigma_config, config.clamp_cls_sigma_config)\n",
    "\n",
    "            # # Measure accuracy and record loss # TODO add again\n",
    "            # acc1, acc5 = ml_data_parameters_utils.compute_topk_accuracy(logits, b_interpolated_seg, topk=(1, 1))\n",
    "            # top1.update(acc1[0], b_img.size(0))\n",
    "            # top5.update(acc5[0], b_img.size(0))\n",
    "            \n",
    "            if epx % config.log_every == 0:\n",
    "                dice = dice3d(\n",
    "                    torch.nn.functional.one_hot(logits.argmax(1), 3),\n",
    "                    torch.nn.functional.one_hot(b_interpolated_seg, 3), one_hot_torch_style=True\n",
    "                )\n",
    "                # Log data parameters\n",
    "                ml_data_parameters_utils.log_intermediate_iteration_stats(\n",
    "                    epx,\n",
    "                    config.learn_class_parameters, config.learn_inst_parameters,\n",
    "                    class_parameters, inst_parameters, top1, top5)\n",
    "\n",
    "                with amp.autocast(enabled=True):\n",
    "                    backbone.eval()\n",
    "                    aspp.eval()\n",
    "                    head.eval()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        b_val_img, b_val_seg, _ = next(iter(val_dataloader))\n",
    "                        b_val_img, b_val_seg = (\n",
    "                            b_val_img.unsqueeze(1).float().cuda(), \n",
    "                            b_val_seg.unsqueeze(1).float().cuda()\n",
    "                        )\n",
    "                        if config.do_plot:\n",
    "                            print(\"Show val img/lbl\")\n",
    "                            val_img_slices = b_val_img.detach().squeeze(0).permute(3,0,1,2)\n",
    "                            val_seg_slices = b_val_seg.detach().squeeze(0).squeeze(0).permute(2,0,1).to(dtype=torch.int64)\n",
    "                            display_nonempty_seg_slices(val_img_slices, val_seg_slices)\n",
    "\n",
    "                        if config.use_mind:\n",
    "                            b_val_img = mindssc(b_val_img)\n",
    "\n",
    "                        b_interpolated_val_seg = F.interpolate(b_val_seg, scale_factor=0.5, mode='nearest').long()\n",
    "                        b_interpolated_val_seg = b_interpolated_val_seg.squeeze(1)\n",
    "                        \n",
    "                        output_val = apply_model(backbone, aspp, head, b_val_img, checkpointing=False)\n",
    "                    \n",
    "                        val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(output_val.argmax(1), 3),\n",
    "                            torch.nn.functional.one_hot(b_interpolated_val_seg, 3), one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        if config.do_plot:\n",
    "                            print(\"Show val lbl/prediction\")\n",
    "                            pred_seg_slices = output_val.argmax(1).squeeze(0).permute(2,0,1)\n",
    "                            pred_seg_slices = F.upsample_nearest(pred_seg_slices.unsqueeze(0).unsqueeze(0).float(), scale_factor=2).squeeze(0).squeeze(0).long()\n",
    "                            display_nonempty_seg_slices(val_seg_slices.unsqueeze(1), pred_seg_slices)\n",
    "\n",
    "                dice_mean_no_bg = round(dice.mean(dim=0)[1:].mean().item(),4)\n",
    "                val_dice_mean_no_bg = round(val_dice.mean(dim=0)[1:].mean().item(),4)\n",
    "                \n",
    "                print(\n",
    "                    f'fold{fold_idx}_epx', epx,round(time.time()-t0,2),'s',\n",
    "                    f'fold{fold_idx}_loss', round(loss.item(),6),\n",
    "                    f'fold{fold_idx}_dice_tensor', dice, \n",
    "                    f'fold{fold_idx}_dice mean (nobg)', dice_mean_no_bg,\n",
    "                    f'fold{fold_idx}_val_dice_mean (nobg)', val_dice_mean_no_bg\n",
    "                )\n",
    "\n",
    "                wandb.log({f'losses/loss': loss}, step=epx)\n",
    "                # wandb.log({f'scores/dice_tensor': dice}, step=epx)\n",
    "                # wandb.log({f'scores/val_dice_tensor': val_dice}, step=epx)\n",
    "                wandb.log({f'scores/dice_mean_wo_bg': dice_mean_no_bg}, step=epx)\n",
    "                wandb.log({f'scores/val_dice_mean_wo_bg': round(val_dice_mean_no_bg, 4)}, step=epx)\n",
    "                \n",
    "                # print(\"Class parameters: \", class_parameters)\n",
    "                # print(\"Instance parameters: \", inst_parameters)\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        # TODO log instance parameters and disturbed instance parameters here\n",
    "        backbone.cpu()\n",
    "        aspp.cpu() \n",
    "        head.cpu()\n",
    "        \n",
    "        save_model(backbone, aspp, head, inst_parameters, class_parameters, \n",
    "            optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "            scaler, f\"{config.mdl_save_prefix}_fold{fold_idx}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "021d39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'num_folds': 3,\n",
    "    'num_classes': 3,\n",
    "    'use_mind': True,\n",
    "    'epochs': 2000,\n",
    "    'batch_size': 4,\n",
    "    'lr': 0.001,\n",
    "    # Data parameter config\n",
    "    'init_class_param': 1.0, \n",
    "    'learn_class_parameters': False, \n",
    "    'lr_class_param': 0.1,\n",
    "    'init_inst_param': 1.0, \n",
    "    'learn_inst_parameters': False, \n",
    "    'lr_inst_param': 0.1,\n",
    "    'wd_inst_param': 0.0,\n",
    "    'wd_class_param': 0.0,\n",
    "    \n",
    "    'skip_clamp_data_param': False,\n",
    "    'clamp_inst_sigma_config': {\n",
    "        'min': np.log(1/20),\n",
    "        'max': np.log(20)\n",
    "    },\n",
    "    'clamp_cls_sigma_config': {\n",
    "        'min': np.log(1/20),\n",
    "        'max': np.log(20)\n",
    "    },\n",
    "\n",
    "    'log_every': 50,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "    \n",
    "    'do_plot': False,\n",
    "    'debug': False,\n",
    "    'wandb_mode': \"online\",\n",
    "\n",
    "    'disturbed_flipped_num': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e6f476a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2qxsm5u2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 0... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>losses/loss</td><td>▁</td></tr><tr><td>scores/dice_mean_wo_bg</td><td>▁</td></tr><tr><td>scores/val_dice_mean_wo_bg</td><td>▁</td></tr><tr><td>train_iteration_stats/accuracy_top1</td><td>▁</td></tr><tr><td>train_iteration_stats/accuracy_top5</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>losses/loss</td><td>1.02801</td></tr><tr><td>scores/dice_mean_wo_bg</td><td>0.0114</td></tr><tr><td>scores/val_dice_mean_wo_bg</td><td>0.0003</td></tr><tr><td>train_iteration_stats/accuracy_top1</td><td>0</td></tr><tr><td>train_iteration_stats/accuracy_top5</td><td>0</td></tr></table>\n",
       "</div></div>\n",
       "You can sync this run to the cloud by running:<br/>\n",
       "\u001b[33mwandb sync /share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/wandb/offline-run-20211026_170751-2qxsm5u2<br/>\n",
       "\u001b[0mFind logs at: <code>./wandb/offline-run-20211026_170751-2qxsm5u2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2qxsm5u2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rap1ide/curriculum_deeplab/runs/z0fd9rty\" target=\"_blank\">2b14i3k2</a></strong> to <a href=\"https://wandb.ai/rap1ide/curriculum_deeplab\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv sqrt class_weight tensor([0.1500, 0.6111, 2.3484], device='cuda:0')\n",
      "#CNN layer 24\n",
      "Warning: skip_update_zero_grad set to True. We will zero out update to state and momentum buffer for parameters with zero gradient. \n",
      "Warning: skip_update_zero_grad set to True. We will zero out update to state and momentum buffer for parameters with zero gradient. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226366/2289005047.py:77: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if disturbed_idxs:\n",
      "/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3981: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0_epx 0 0.88 s fold0_loss 1.539262 fold0_dice_tensor tensor([[0.0931, 0.0366, 0.0004],\n",
      "        [0.1142, 0.0000, 0.0003],\n",
      "        [0.1047, 0.0000, 0.0005],\n",
      "        [0.0989, 0.0033, 0.0005]]) fold0_dice mean (nobg) 0.0052 fold0_val_dice_mean (nobg) 0.0002\n",
      "fold0_epx 50 33.07 s fold0_loss 0.221999 fold0_dice_tensor tensor([[0.9985, 0.0351, 0.0000],\n",
      "        [0.9969, 0.0096, 0.0000],\n",
      "        [0.9996, 0.0000, 0.0000],\n",
      "        [0.9998, 0.0000, 0.0000]]) fold0_dice mean (nobg) 0.0056 fold0_val_dice_mean (nobg) 0.0206\n",
      "fold0_epx 100 66.35 s fold0_loss 0.097489 fold0_dice_tensor tensor([[0.9998, 0.0000, 0.0000],\n",
      "        [0.9981, 0.0000, 0.0000],\n",
      "        [0.9970, 0.6499, 0.0000],\n",
      "        [0.9973, 0.7398, 0.0000]]) fold0_dice mean (nobg) 0.1737 fold0_val_dice_mean (nobg) 0.1088\n",
      "fold0_epx 150 99.09 s fold0_loss 0.042586 fold0_dice_tensor tensor([[0.9998, 0.0000, 0.5385],\n",
      "        [0.9997, 0.0000, 0.5455],\n",
      "        [0.9941, 0.7193, 0.4490],\n",
      "        [0.9991, 0.0000, 0.6364]]) fold0_dice mean (nobg) 0.3611 fold0_val_dice_mean (nobg) 0.295\n",
      "fold0_epx 200 131.82 s fold0_loss 0.039725 fold0_dice_tensor tensor([[0.9973, 0.7923, 0.4565],\n",
      "        [0.9975, 0.1588, 0.3038],\n",
      "        [0.9998, 0.0000, 0.5479],\n",
      "        [0.9976, 0.5882, 0.5556]]) fold0_dice mean (nobg) 0.4254 fold0_val_dice_mean (nobg) 0.3566\n",
      "fold0_epx 250 165.03 s fold0_loss 0.030618 fold0_dice_tensor tensor([[0.9947, 0.7976, 0.3333],\n",
      "        [0.9959, 0.7265, 0.3059],\n",
      "        [0.9996, 0.0000, 0.4762],\n",
      "        [0.9991, 0.0000, 0.5846]]) fold0_dice mean (nobg) 0.403 fold0_val_dice_mean (nobg) 0.2558\n",
      "fold0_epx 300 198.9 s fold0_loss 0.015061 fold0_dice_tensor tensor([[0.9983, 0.0000, 0.3571],\n",
      "        [0.9995, 0.0000, 0.3297],\n",
      "        [0.9995, 0.0000, 0.3684],\n",
      "        [0.9989, 0.0000, 0.2013]]) fold0_dice mean (nobg) 0.1571 fold0_val_dice_mean (nobg) 0.196\n",
      "fold0_epx 350 232.41 s fold0_loss 0.016813 fold0_dice_tensor tensor([[0.9993, 0.5000, 0.4341],\n",
      "        [0.9989, 0.0000, 0.5217],\n",
      "        [0.9995, 0.0000, 0.4074],\n",
      "        [0.9962, 0.7080, 0.2955]]) fold0_dice mean (nobg) 0.3583 fold0_val_dice_mean (nobg) 0.2042\n",
      "fold0_epx 400 265.48 s fold0_loss 0.016551 fold0_dice_tensor tensor([[0.9954, 0.8518, 0.1842],\n",
      "        [0.9990, 0.0000, 0.3592],\n",
      "        [0.9989, 0.6937, 0.3594],\n",
      "        [0.9994, 0.0000, 0.3150]]) fold0_dice mean (nobg) 0.3454 fold0_val_dice_mean (nobg) 0.2461\n",
      "fold0_epx 450 298.35 s fold0_loss 0.014202 fold0_dice_tensor tensor([[0.9995, 0.0000, 0.3636],\n",
      "        [0.9958, 0.8666, 0.3125],\n",
      "        [0.9995, 0.0000, 0.4333],\n",
      "        [0.9988, 0.8794, 0.3913]]) fold0_dice mean (nobg) 0.4058 fold0_val_dice_mean (nobg) 0.3619\n",
      "fold0_epx 500 331.69 s fold0_loss 0.012709 fold0_dice_tensor tensor([[0.9987, 0.6815, 0.4471],\n",
      "        [0.9985, 0.7842, 0.3611],\n",
      "        [0.9997, 0.0000, 0.5275],\n",
      "        [0.9976, 0.8088, 0.1538]]) fold0_dice mean (nobg) 0.4705 fold0_val_dice_mean (nobg) 0.5055\n",
      "fold0_epx 550 365.08 s fold0_loss 0.019518 fold0_dice_tensor tensor([[0.9972, 0.7158, 0.5063],\n",
      "        [0.9996, 0.0000, 0.5098],\n",
      "        [0.9996, 0.0000, 0.3855],\n",
      "        [0.9959, 0.8362, 0.5217]]) fold0_dice mean (nobg) 0.4344 fold0_val_dice_mean (nobg) 0.2673\n",
      "fold0_epx 600 398.19 s fold0_loss 0.009366 fold0_dice_tensor tensor([[0.9995, 0.0000, 0.3551],\n",
      "        [0.9982, 0.8218, 0.1463],\n",
      "        [0.9991, 0.6880, 0.4368],\n",
      "        [0.9997, 0.0000, 0.4615]]) fold0_dice mean (nobg) 0.3637 fold0_val_dice_mean (nobg) 0.3672\n",
      "fold0_epx 650 431.44 s fold0_loss 0.023898 fold0_dice_tensor tensor([[0.9997, 0.0000, 0.4412],\n",
      "        [0.9982, 0.9052, 0.4167],\n",
      "        [0.9984, 0.8220, 0.4250],\n",
      "        [0.9963, 0.8953, 0.4598]]) fold0_dice mean (nobg) 0.5456 fold0_val_dice_mean (nobg) 0.6314\n"
     ]
    }
   ],
   "source": [
    "run_name = wandb.util.generate_id()\n",
    "train_DL(run_name, config_dict, training_dataset)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7118959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_DL(run_name, config_dict, inf_dataset):\n",
    "    \n",
    "    run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"testing\", job_type=\"test\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "    )\n",
    "    config = wandb.config\n",
    "\n",
    "    debug = config.debug\n",
    "    num_folds = config.num_folds\n",
    "\n",
    "    score_dicts = []\n",
    "    \n",
    "    for fold_idx in range(num_folds):\n",
    "        backbone, aspp, head, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "        backbone.eval()\n",
    "        aspp.eval()\n",
    "        head.eval()\n",
    "        \n",
    "        for img, seg, sample_idx in inf_dataset:\n",
    "\n",
    "            img, seg = (\n",
    "                img.unsqueeze(0).unsqueeze(0).float(), \n",
    "                seg.unsqueeze(0).unsqueeze(0)\n",
    "            )\n",
    "\n",
    "            if config.use_mind:\n",
    "                img = mindssc(img)\n",
    "\n",
    "            if config.do_plot:\n",
    "                img_slices = img[0:1].permute(0,1,4,2,3).squeeze().unsqueeze(1)\n",
    "                seg_slices = seg[0:1].permute(0,1,4,2,3).squeeze().to(dtype=torch.int64)\n",
    "                display_nonempty_seg_slices(img_slices, seg_slices)\n",
    "\n",
    "            interpolated_seg = F.interpolate(seg.float(), scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "\n",
    "            img.requires_grad = True\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    interpolated_seg = F.interpolate(seg.float(), scale_factor=0.5, mode='nearest').long()\n",
    "                    interpolated_seg = interpolated_seg.squeeze(1)\n",
    "                    \n",
    "                    output_val = apply_model(backbone, aspp, head, img, checkpointing=False)\n",
    "\n",
    "                    inf_dice = dice3d(\n",
    "                        torch.nn.functional.one_hot(output_val.argmax(1), 3),\n",
    "                        torch.nn.functional.one_hot(interpolated_seg, 3), one_hot_torch_style=True\n",
    "                    )\n",
    "                if config.do_plot:\n",
    "                    lbl_slices = seg.detach().squeeze(0).permute(3,0,1,2)\n",
    "                    pred_seg_slices = output_val.argmax(1).squeeze(0).permute(2,0,1)\n",
    "                    pred_seg_slices = F.upsample_nearest(pred_seg_slices.unsqueeze(0).unsqueeze(0).float(), scale_factor=2).squeeze(0).squeeze(0).long()\n",
    "                    display_nonempty_seg_slices(lbl_slices, pred_seg_slices)\n",
    "\n",
    "                for class_idx, class_dice in enumerate(inf_dice.tolist()[0]):\n",
    "                    score_dicts.append(\n",
    "                        {\n",
    "                            'fold_idx': fold_idx,\n",
    "                            'sample_idx': sample_idx,\n",
    "                            'class_idx': class_idx,\n",
    "                            'dice': class_dice,\n",
    "                        }\n",
    "                    )\n",
    "                # Mean over all classes (w/o background)\n",
    "                dice_mean_no_bg = inf_dice.mean(dim=0)[1:].mean()\n",
    "                wandb.log({f'scores/dice_fold_{fold_idx}': dice_mean_no_bg, 'sample_idx': sample_idx})\n",
    "                print(f\"Dice of validation sample {sample_idx} @(fold={fold_idx}): {dice_mean_no_bg.item():.2f}\")\n",
    "\n",
    "            if debug:\n",
    "                break\n",
    "\n",
    "    mean_inf_dice = torch.tensor([score['dice'] for score in score_dicts if score['class_idx'] != 0]).mean()\n",
    "    print(f\"Mean dice over all folds, classes and samples: {mean_inf_dice.item()*100:.2f}%\")\n",
    "    wandb.log({'scores/mean_dice_all_folds_samples_classes': mean_inf_dice})\n",
    "    wandb.finish()\n",
    "\n",
    "    return score_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dicts = inference_DL(run_name, config_dict, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352df74b-35a6-4c6f-b986-4dd60ac1e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone, aspp, head, inst_parameters, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "table = wandb.Table(data=inst_parameters.tolist(), columns=[\"instance_parameters\"])\n",
    "wandb.log(\n",
    "    {'data_parameters/instance_parameters': \n",
    "        wandb.plot.histogram(table, \"instance_parameters\", title=\"Instance parameters\")\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
