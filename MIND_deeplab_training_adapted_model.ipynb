{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -3 -4\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "\n",
    "import torchio as tio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import display_seg\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "\n",
    "import wandb\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from curriculum_deeplab.data_parameters import DataParamMode, DataParamOptim\n",
    "from curriculum_deeplab.data_parameters import DataParameterManager\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca60d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            # nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        # x = F.adaptive_avg_pool3d(x, (1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')  # , align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = [nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())]\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    " \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "# Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "def create_model(output_classes: int = 14,input_channels: int = 1):\n",
    "    # in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "    in_channels = torch.Tensor([input_channels, 24, 24, 32, 48, 48, 48, 64]).long()\n",
    "    mid_channels = torch.Tensor([64, 128, 192, 192, 256, 256, 256, 384]).long()\n",
    "    out_channels = torch.Tensor([24, 24, 32, 48, 48, 48, 64, 64]).long()\n",
    "    mid_stride = torch.Tensor([1, 1, 1, 2, 1, 1, 1, 1])\n",
    "    net = [nn.Identity()]\n",
    "    for i in range(8):\n",
    "        inc = int(in_channels[i])\n",
    "        midc = int(mid_channels[i])\n",
    "        outc = int(out_channels[i])\n",
    "        strd = int(mid_stride[i])\n",
    "        layer = nn.Sequential(nn.Conv3d(inc, midc, 1, bias=False), nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, midc, 3, stride=strd, padding=1, bias=False, groups=midc),\n",
    "                              nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, outc, 1, bias=False), nn.BatchNorm3d(outc))\n",
    "        if i == 0:\n",
    "            layer[0] = nn.Conv3d(inc, midc, 3, padding=1, stride=2, bias=False)\n",
    "        if (inc == outc) & (strd == 1):\n",
    "            net.append(ResBlock(layer))\n",
    "        else:\n",
    "            net.append(layer)\n",
    "\n",
    "    backbone = nn.Sequential(*net)\n",
    "\n",
    "    count = 0\n",
    "    # weight initialization\n",
    "    for m in backbone.modules():\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            count += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    print('#CNN layer', count)\n",
    "    # complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "    # newer model (one more stride, no groups in head)\n",
    "    aspp = ASPP(64, (2, 4, 8, 16, 32), 128)\n",
    "    head = nn.Sequential(nn.Conv3d(128 + 24, 64, 1, padding=0, groups=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, 64, 3, groups=1, padding=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, output_classes, 1))\n",
    "    return backbone, aspp, head\n",
    "\n",
    "\n",
    "def apply_model(backbone, aspp, head, img, checkpointing=True, return_intermediate=False):\n",
    "    if checkpointing:\n",
    "        x1 = checkpoint(backbone[:3], img)\n",
    "        x2 = checkpoint(backbone[3:], x1)\n",
    "        y = checkpoint(aspp, x2)\n",
    "        y1 = torch.cat((x1, F.interpolate(y, scale_factor=2)), 1)\n",
    "        output_j = checkpoint(head, y1)\n",
    "    else:\n",
    "        x1 = backbone[:3](img)\n",
    "        x2 = backbone[3:](x1)\n",
    "        y = aspp(x2)\n",
    "        y1 = torch.cat((x1, F.interpolate(y, scale_factor=2)), 1)\n",
    "        output_j = head(y1)\n",
    "    if return_intermediate:\n",
    "        return y1,output_j\n",
    "    else:\n",
    "        return output_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55c32e-bce6-4e35-be76-3bdc306543f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sample(b_image, b_label, scale_factor, yield_2d):\n",
    "    \n",
    "    if yield_2d:\n",
    "        scale = [scale_factor]*2\n",
    "        im_mode = 'bilinear'\n",
    "    else:\n",
    "        scale = [scale_factor]*3\n",
    "        im_mode = 'trilinear'\n",
    "    \n",
    "    b_image = F.interpolate(\n",
    "        b_image.unsqueeze(1), scale_factor=scale, mode=im_mode, align_corners=True\n",
    "    )\n",
    "\n",
    "    b_label = F.interpolate(\n",
    "        b_label.unsqueeze(1).float(), scale_factor=scale, mode='nearest').long()\n",
    "    \n",
    "    return b_image.squeeze(1), b_label.squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "def dilate_label_class(b_label, class_max_idx, class_dilate_idx, \n",
    "                       yield_2d, kernel_sz=3):\n",
    "    \n",
    "    if kernel_sz < 2:\n",
    "        return b_label\n",
    "    \n",
    "    b_dilated_label = b_label\n",
    "    \n",
    "    b_onehot = torch.nn.functional.one_hot(b_label.long(), class_max_idx+1)\n",
    "    class_slice = b_onehot[...,class_dilate_idx]\n",
    "    \n",
    "    if yield_2d:\n",
    "        B, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz]).long()\n",
    "        kernel = kernel.view(1,1,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv2d(\n",
    "            class_slice.view(B,1,H,W), kernel, padding='same')\n",
    "        \n",
    "    else:\n",
    "        B, D, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz,kernel_sz])\n",
    "        kernel = kernel.long().view(1,1,kernel_sz,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv3d(\n",
    "            class_slice.view(B,1,D,H,W), kernel, padding='same')\n",
    "        \n",
    "    dilated_class_slice = torch.clamp(class_slice.squeeze(0), 0, 1)\n",
    "    b_dilated_label[dilated_class_slice.bool()] = class_dilate_idx\n",
    "    \n",
    "    return b_dilated_label\n",
    "\n",
    "\n",
    "def get_batch_dice_wo_bg(b_dice) -> float:\n",
    "    if torch.all(torch.isnan(b_dice[:,1:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,1:]).item()\n",
    "    \n",
    "    \n",
    "\n",
    "def get_batch_dice_tumour(b_dice) -> float: \n",
    "    if torch.all(torch.isnan(b_dice[:,1])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,1]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_batch_dice_cochlea(b_dice) -> float:\n",
    "    if torch.all(torch.isnan(b_dice[:,2])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,2]).item()\n",
    "    \n",
    "    \n",
    "    \n",
    "def map_continuous_from_dataset_idxs(subset_to_map, dataset_idxs):\n",
    "    cont_idxs = torch.tensor([torch.where(dataset_idxs==d_idx) for d_idx in subset_to_map]).reshape(subset_to_map.shape)\n",
    "    return cont_idxs\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "        \n",
    "        \n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "    \n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "        \n",
    "    \n",
    "    \n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594787c-cddd-4817-9a96-6afd52337a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentBspline(b_image, b_label, num_ctl_points=7, strength=0.05, yield_2d=False):\n",
    "    \"\"\"\n",
    "    2D/3D b-spline augmentation on image and segmentation mini-batch on GPU.\n",
    "    :input: b_image batch (torch.cuda.FloatTensor), b_label batch (torch.cuda.LongTensor)\n",
    "    :return: augmented Bx(D)xHxW image batch (torch.cuda.FloatTensor), \n",
    "    augmented Bx(D)xHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "\n",
    "    KERNEL_SIZE = 3\n",
    "\n",
    "    if yield_2d:\n",
    "        assert b_image.dim() == b_label.dim() == 3, \\\n",
    "            f\"Augmenting 2D. Input batch of image and \" \\\n",
    "            f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,H,W = b_image.shape\n",
    "        \n",
    "        bspline = torch.nn.Sequential(\n",
    "            nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2))\n",
    "        )\n",
    "        # Add an extra *.5 factor to dim strength to make strength fit 3D case\n",
    "        dim_strength = (torch.tensor([H,W]).float()*strength*.5).to(b_image.device)\n",
    "        rand_control_points = dim_strength.view(1,2,1,1) * torch.randn(\n",
    "            1, 2, num_ctl_points, num_ctl_points\n",
    "        ).to(b_image.device)\n",
    "        \n",
    "        bspline_disp = bspline(rand_control_points)\n",
    "        bspline_disp = torch.nn.functional.interpolate(\n",
    "            bspline_disp, size=(H,W), mode='bilinear', align_corners=True\n",
    "        ).permute(0,2,3,1)\n",
    "    \n",
    "        identity = torch.eye(2,3).expand(B,2,3).to(b_image.device)\n",
    "        \n",
    "        id_grid = F.affine_grid(identity, torch.Size((B,2,H,W)), \n",
    "            align_corners=False)\n",
    "\n",
    "    else:\n",
    "        assert b_image.dim() == b_label.dim() == 4, \\\n",
    "            f\"Augmenting 3D. Input batch of image and \" \\\n",
    "            f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,D,H,W = b_image.shape\n",
    "        \n",
    "        bspline = torch.nn.Sequential(\n",
    "            nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2))\n",
    "        )\n",
    "        dim_strength = (torch.tensor([D,H,W]).float()*strength).to(b_image.device)\n",
    "        rand_control_points = dim_strength.view(1,3,1,1,1) * torch.randn(\n",
    "            1, 3, num_ctl_points, num_ctl_points, num_ctl_points\n",
    "        ).to(b_image.device)\n",
    "\n",
    "        bspline_disp = bspline(rand_control_points)\n",
    "\n",
    "        bspline_disp = torch.nn.functional.interpolate(\n",
    "            bspline_disp, size=(D,H,W), mode='trilinear', align_corners=True\n",
    "        ).permute(0,2,3,4,1)\n",
    "    \n",
    "        identity = torch.eye(3,4).expand(B,3,4).to(b_image.device)\n",
    "        \n",
    "        id_grid = F.affine_grid(identity, torch.Size((B,3,D,H,W)), \n",
    "            align_corners=False)\n",
    "\n",
    "    b_image_out = F.grid_sample(\n",
    "        b_image.unsqueeze(1).float(), id_grid + bspline_disp, \n",
    "        padding_mode='border', align_corners=False)\n",
    "    b_label_out = F.grid_sample(\n",
    "        b_label.unsqueeze(1).float(), id_grid + bspline_disp, \n",
    "        mode='nearest', align_corners=False)\n",
    "\n",
    "    return b_image_out.squeeze(1), b_label_out.squeeze(1).long()\n",
    "\n",
    "\n",
    "\n",
    "def augmentAffine(b_image, b_label, strength=0.05, yield_2d=False):\n",
    "    \"\"\"\n",
    "    2D/3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: b_image batch (torch.cuda.FloatTensor), b_label batch (torch.cuda.LongTensor)\n",
    "    :return: augmented Bx(D)xHxW image batch (torch.cuda.FloatTensor), \n",
    "    augmented Bx(D)xHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    if yield_2d:\n",
    "        assert b_image.dim() == b_label.dim() == 3, \\\n",
    "            f\"Augmenting 2D. Input batch of image and \" \\\n",
    "            f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,H,W = b_image.shape\n",
    "        \n",
    "        affine_matrix = (torch.eye(2,3).unsqueeze(0) + torch.randn(B,2,3) \\\n",
    "                         * strength).to(b_image.device)\n",
    "\n",
    "        meshgrid = F.affine_grid(affine_matrix, torch.Size((B,1,H,W)), \n",
    "                                 align_corners=False)\n",
    "    else:\n",
    "        assert b_image.dim() == b_label.dim() == 4, \\\n",
    "            f\"Augmenting 3D. Input batch of image and \" \\\n",
    "            f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,D,H,W = b_image.shape\n",
    "        \n",
    "        affine_matrix = (torch.eye(3,4).unsqueeze(0) + torch.randn(B,3,4) \\\n",
    "                         * strength).to(b_image.device)\n",
    "\n",
    "        meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)), \n",
    "                                 align_corners=False)\n",
    "        \n",
    "    b_image_out = F.grid_sample(\n",
    "        b_image.unsqueeze(1).float(), meshgrid, padding_mode='border', align_corners=False)\n",
    "    b_label_out = F.grid_sample(\n",
    "        b_label.unsqueeze(1).float(), meshgrid, mode='nearest', align_corners=False)\n",
    "\n",
    "    return b_image_out.squeeze(1), b_label_out.squeeze(1).long()\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(b_image, strength=0.05):\n",
    "    return b_image + strength*torch.randn_like(b_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a979a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class CrossMoDa_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        ensure_labeled_pairs=True, use_additional_data=False, resample=True,\n",
    "        size:tuple=(96,96,60), normalize:bool=True, \n",
    "        max_load_num=None, crop_w_dim_range=None,\n",
    "        disturbed_idxs=None, yield_2d_normal_to=None, flip_r_samples=True,\n",
    "        dilate_kernel_sz=3):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                ensure_labeled_pairs (bool): Only images with corresponding labels will be loaded (default: True)\n",
    "                \n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "                max_load_num (int): maximum number of pairs to load (uses first max_load_num samples for either images and labels found)\n",
    "                crop_w_dim_range (tuple): Tuple of ints defining the range to which dimension W of (D,H,W) is cropped\n",
    "                yield_2d_normal_to (bool):\n",
    "                \n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.disturbed_idxs = disturbed_idxs\n",
    "        self.yield_2d_normal_to = yield_2d_normal_to\n",
    "        self.do_train = False\n",
    "        self.augment_at_collate = False\n",
    "        self.dilate_kernel_sz = dilate_kernel_sz\n",
    "        \n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "        \n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "        \n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "        \n",
    "        path = base_dir + state_dir\n",
    "        \n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "            \n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "            \n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "        \n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "        \n",
    "        if domain.lower() == \"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "        \n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "        # First read filepaths\n",
    "        self.img_paths = {}\n",
    "        self.label_paths = {}\n",
    "\n",
    "        for _path in files:\n",
    "     \n",
    "            numeric_id = int(re.findall(r'\\d+', os.path.basename(_path))[0])\n",
    "            if \"_l.nii.gz\" in _path or \"_l_Label.nii.gz\" in _path:\n",
    "                lr_id = 'l'\n",
    "            elif \"_r.nii.gz\" in _path or \"_r_Label.nii.gz\" in _path:\n",
    "                lr_id = 'r'\n",
    "            else:\n",
    "                lr_id = \"\"\n",
    "            \n",
    "            # Generate crossmoda id like 004r\n",
    "            crossmoda_id = f\"{numeric_id:03d}{lr_id}\"\n",
    "            \n",
    "            if \"Label\" in _path:\n",
    "                self.label_paths[crossmoda_id] = _path\n",
    "                    \n",
    "            elif domain in _path:\n",
    "                self.img_paths[crossmoda_id] = _path\n",
    "        \n",
    "        if ensure_labeled_pairs:\n",
    "            pair_idxs = set(self.img_paths).intersection(set(self.label_paths))\n",
    "            self.label_paths = {_id: _path for _id, _path in self.label_paths.items() if _id in pair_idxs}\n",
    "            self.img_paths = {_id: _path for _id, _path in self.img_paths.items() if _id in pair_idxs}\n",
    "        \n",
    "            \n",
    "        # Populate data\n",
    "        self.img_data = {}\n",
    "        self.label_data = {}\n",
    "        self.img_data_2d = {}\n",
    "        self.label_data_2d = {}\n",
    "        \n",
    "        #load data\n",
    "        \n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "        id_paths_to_load = list(self.label_paths.items()) + list(self.img_paths.items())\n",
    "        \n",
    "        description = f\"{len(self.img_paths)} images, {len(self.label_paths)} labels\"\n",
    "\n",
    "        for crossmoda_id, f in tqdm(id_paths_to_load, desc=description):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in f:\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "                \n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                    \n",
    "                if crop_w_dim_range:\n",
    "                    tmp = tmp[..., crop_w_dim_range[0]:crop_w_dim_range[1]]    \n",
    "                \n",
    "                self.label_data[crossmoda_id] = tmp.long()\n",
    "                    \n",
    "            elif domain in f:\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "                \n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                \n",
    "                if crop_w_dim_range:\n",
    "                    tmp = tmp[..., crop_w_dim_range[0]:crop_w_dim_range[1]]\n",
    "                    \n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "                    \n",
    "                self.img_data[crossmoda_id] = tmp\n",
    "        \n",
    "        # Postprocessing\n",
    "        for crossmoda_id in list(self.label_data.keys()):\n",
    "            if self.label_data[crossmoda_id].unique().numel() != 3:\n",
    "                del self.img_data[crossmoda_id]\n",
    "                del self.label_data[crossmoda_id]\n",
    "            elif \"r\" in crossmoda_id:\n",
    "                self.img_data[crossmoda_id] = self.img_data[crossmoda_id].flip(dims=(1,))\n",
    "                self.label_data[crossmoda_id] = self.label_data[crossmoda_id].flip(dims=(1,))\n",
    "        \n",
    "        if max_load_num and ensure_labeled_pairs:\n",
    "            for crossmoda_id in list(self.label_data.keys())[max_load_num:]:\n",
    "                del self.img_data[crossmoda_id]\n",
    "                del self.label_data[crossmoda_id]\n",
    "                \n",
    "        elif max_load_num:\n",
    "            for del_key in list(self.image_data.keys())[max_load_num:]:\n",
    "                del self.img_data[crossmoda_id]\n",
    "            for del_key in list(self.label_data.keys())[max_load_num:]:\n",
    "                del self.label_data[crossmoda_id]\n",
    "            \n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(set(self.img_data)==set(self.label_data)))\n",
    "        \n",
    "        img_stack = torch.stack(list(self.img_data.values()), dim=0)\n",
    "        img_mean, img_std = img_stack.mean(), img_stack.std()\n",
    "        \n",
    "        label_stack = torch.stack(list(self.label_data.values()), dim=0)\n",
    "\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(img_stack.shape, img_mean, img_std))\n",
    "        print(\"Label shape: {}, max.: {}\".format(label_stack.shape,torch.max(label_stack)))\n",
    "        \n",
    "        if yield_2d_normal_to:\n",
    "            if yield_2d_normal_to == \"D\":\n",
    "                slice_dim = -3\n",
    "            if yield_2d_normal_to == \"H\":\n",
    "                slice_dim = -2\n",
    "            if yield_2d_normal_to == \"W\":\n",
    "                slice_dim = -1\n",
    "  \n",
    "            for crossmoda_id, image in self.img_data.items():  \n",
    "                for idx, img_slc in [(slice_idx, image.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(image.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.img_data_2d[f\"{crossmoda_id}{yield_2d_normal_to}{idx:03d}\"] = img_slc\n",
    "                    \n",
    "            for crossmoda_id, label in self.label_data.items():   \n",
    "                for idx, lbl_slc in [(slice_idx, label.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(label.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.label_data_2d[f\"{crossmoda_id}{yield_2d_normal_to}{idx:03d}\"] = lbl_slc\n",
    "                    \n",
    "        print(\"Data import finished. Elapsed time: {:.1f} s\".format(time.time()-t0 ))\n",
    "        print(f\"CrossMoDa loader will yield {'2D' if self.yield_2d_normal_to else '3D'} samples\")\n",
    "        \n",
    "    def get_crossmoda_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data.keys())\n",
    "            .union(set(self.label_data.keys()))\n",
    "        ))\n",
    "    \n",
    "    def get_2d_crossmoda_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data_2d.keys())\n",
    "            .union(set(self.label_data_2d.keys()))\n",
    "        ))\n",
    "    \n",
    "    def get_crossmoda_id_dicts(self):\n",
    "        \n",
    "        all_crossmoda_ids = self.get_crossmoda_ids()\n",
    "        id_dicts = []\n",
    "        \n",
    "        for twod_dataset_idx, twod_crossmoda_id in enumerate(self.get_2d_crossmoda_ids()):\n",
    "            crossmoda_id = twod_crossmoda_id[:-4]\n",
    "            id_dicts.append(\n",
    "                {\n",
    "                    '2d_crossmoda_id': twod_crossmoda_id,\n",
    "                    '2d_dataset_idx': twod_dataset_idx,\n",
    "                    'crossmoda_id': crossmoda_id,\n",
    "                    'dataset_idx': all_crossmoda_ids.index(crossmoda_id),                    \n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return id_dicts\n",
    "        \n",
    "    def __len__(self, yield_2d_override=None):\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data length\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "            \n",
    "        if yield_2d:\n",
    "            return len(self.img_data_2d)\n",
    "        \n",
    "        return len(self.img_data)\n",
    "\n",
    "    def __getitem__(self, dataset_idx, yield_2d_override=None):\n",
    "        \n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data can be overridden\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "        \n",
    "        modified_label = []\n",
    "        \n",
    "        if yield_2d:\n",
    "            all_crossmoda_ids = self.get_2d_crossmoda_ids()\n",
    "            c_id = all_crossmoda_ids[dataset_idx]\n",
    "            image = self.img_data_2d.get(c_id, torch.tensor([]))\n",
    "            label = self.label_data_2d.get(c_id, torch.tensor([]))\n",
    "            \n",
    "            # For 2D crossmoda id cut last 4 \"003rW100\"\n",
    "            image_path = self.img_paths[c_id[:-4]]\n",
    "            label_path = self.label_paths[c_id[:-4]]\n",
    "            \n",
    "        else:\n",
    "            all_crossmoda_ids = self.get_crossmoda_ids()\n",
    "            c_id = all_crossmoda_ids[dataset_idx]\n",
    "            image = self.img_data.get(c_id, torch.tensor([]))\n",
    "            label = self.label_data.get(c_id, torch.tensor([]))\n",
    "\n",
    "            image_path = self.img_paths[c_id]\n",
    "            label_path = self.label_paths[c_id]\n",
    "        \n",
    "        if self.do_train:\n",
    "            # In case of training add augmentation, modification and\n",
    "            # disturbance\n",
    "            \n",
    "            if not self.augment_at_collate:\n",
    "                b_image = image.unsqueeze(0)\n",
    "                b_label = label.unsqueeze(0)\n",
    "                b_image, b_label = self.augment_tio(b_image, b_label, yield_2d)\n",
    "                image = b_image.squeeze(0)\n",
    "                label = b_label.squeeze(0)\n",
    "            \n",
    "            # Dilate small cochlea segmentation\n",
    "            COCHLEA_CLASS_IDX = 2\n",
    "            pre_mod = b_label.squeeze(0)\n",
    "            modified_label = dilate_label_class(\n",
    "                b_label.detach().clone(), COCHLEA_CLASS_IDX, COCHLEA_CLASS_IDX, \n",
    "                yield_2d=yield_2d, kernel_sz=self.dilate_kernel_sz\n",
    "            ).squeeze(0)\n",
    "\n",
    "            if self.disturbed_idxs != None and dataset_idx in self.disturbed_idxs:\n",
    "                if yield_2d:\n",
    "                    modified_label = \\\n",
    "                        torch.flip(modified_label, dims=(-2,-1))\n",
    "                else:\n",
    "                    modified_label = \\\n",
    "                        torch.flip(modified_label, dims=(-3,-2,-1))\n",
    "        \n",
    "        if yield_2d:\n",
    "            assert image.dim() == label.dim() == 2\n",
    "        else:\n",
    "            assert image.dim() == label.dim() == 3\n",
    "            \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'modified_label': modified_label,\n",
    "            'dataset_idx': dataset_idx, \n",
    "            'crossmoda_id': c_id, \n",
    "            'image_path': image_path, \n",
    "            'label_path': label_path\n",
    "        }\n",
    "    \n",
    "    def get_crossmoda_3d_item(self, dataset_idx):\n",
    "        return self.__getitem__(dataset_idx, yield_2d_override=False)\n",
    "\n",
    "    def get_data(self):\n",
    "        img_stack = torch.stack(list(self.img_data.values()), dim=0)\n",
    "        label_stack = torch.stack(list(self.label_data.values()), dim=0)\n",
    "        \n",
    "        return img_stack, label_stack\n",
    "    \n",
    "    def set_disturbed_idxs(self, idxs):\n",
    "        self.disturbed_idxs = idxs\n",
    "        \n",
    "    def train(self):\n",
    "        self.do_train = True\n",
    "        \n",
    "    def eval(self):\n",
    "        self.do_train = False\n",
    "        \n",
    "    def set_augment_at_collate(self):\n",
    "        self.augment_at_collate = True\n",
    "    \n",
    "    def unset_augment_at_collate(self):\n",
    "        self.augment_at_collate = False\n",
    "    \n",
    "    def set_dilate_kernel_size(self, sz):\n",
    "        \n",
    "        self.dilate_kernel_sz = max(1,sz)\n",
    "        \n",
    "    def get_dilate_kernel_size(self):\n",
    "        return self.dilate_kernel_sz\n",
    "        \n",
    "    def get_efficient_augmentation_collate_fn(self):\n",
    "        yield_2d = True if self.yield_2d_normal_to else False\n",
    "        \n",
    "        def collate_closure(batch):\n",
    "            batch = torch.utils.data._utils.collate.default_collate(batch)\n",
    "            if self.augment_at_collate:\n",
    "                # Augment the whole batch not just one sample\n",
    "                b_image = batch['image'].cuda()\n",
    "                b_label = batch['label'].cuda()\n",
    "                b_image, b_label = self.augment(b_image, b_label, yield_2d)\n",
    "                batch['image'], batch['label'] = b_image.cpu(), b_label.cpu()\n",
    "\n",
    "            return batch\n",
    "        \n",
    "        return collate_closure\n",
    "    \n",
    "    def augment(self, b_image, b_label, yield_2d):\n",
    "        if yield_2d:\n",
    "            assert b_image.dim() == b_label.dim() == 3, \\\n",
    "                f\"Augmenting 2D. Input batch of image and \" \\\n",
    "                f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        else:\n",
    "            assert b_image.dim() == b_label.dim() == 4, \\\n",
    "                f\"Augmenting 3D. Input batch of image and \" \\\n",
    "                f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "\n",
    "        spatial_aug_selector = np.random.rand()\n",
    "        \n",
    "        b_image, b_label = interpolate_sample(b_image, b_label, 2., yield_2d)\n",
    "        if spatial_aug_selector < .4:\n",
    "            b_image, b_label = augmentAffine(\n",
    "                b_image, b_label, strength=0.05, yield_2d=yield_2d)\n",
    "\n",
    "        elif spatial_aug_selector <.8:\n",
    "            b_image, b_label = augmentBspline(\n",
    "                b_image, b_label, num_ctl_points=7, strength=0.005, yield_2d=yield_2d)\n",
    "        else:\n",
    "            pass\n",
    "        b_image, b_label = interpolate_sample(b_image, b_label, .5, yield_2d)\n",
    "        \n",
    "        b_image = augmentNoise(b_image, strength=0.05)\n",
    "        b_label = b_label.long()\n",
    "        \n",
    "    def augment_tio(self, image, label, yield_2d):\n",
    "        # Prepare dims for torchio: All transformed \n",
    "        # images / labels need to be 4-dim;\n",
    "        # 2D images need to have dims=1xHxWx1 to make transformation work\n",
    "        \n",
    "        if yield_2d:\n",
    "            assert image.dim() == label.dim() == 3, \\\n",
    "                f\"Augmenting 2D. Input batch of image and \" \\\n",
    "                f\"label should be 1xHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        else:\n",
    "            assert image.dim() == label.dim() == 4, \\\n",
    "                f\"Augmenting 3D. Input batch of image and \" \\\n",
    "                f\"label should be 1xDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "\n",
    "            \n",
    "        if self.yield_2d_normal_to:           \n",
    "            self.spatial_transform = tio.OneOf({\n",
    "                tio.transforms.RandomAffine(.05): 0.8,\n",
    "                tio.transforms.RandomElasticDeformation(num_control_points=7, max_displacement=(7.5,7.5,1e-5)): 0.2,\n",
    "                },\n",
    "                p=0.75,\n",
    "            )\n",
    "        else:\n",
    "            self.spatial_transform = tio.OneOf({\n",
    "                tio.transforms.RandomAffine(.05): 0.8,\n",
    "                tio.transforms.RandomElasticDeformation(num_control_points=7, max_displacement=7.5): 0.2,\n",
    "                },\n",
    "                p=0.75,\n",
    "            )\n",
    "\n",
    "        # Transforms can be composed as in torchvision.transforms\n",
    "\n",
    "        self.intensity_transform = tio.OneOf({\n",
    "            tio.transforms.RandomNoise(std=0.05): 0.6,\n",
    "        })\n",
    "\n",
    "        if yield_2d:\n",
    "            image = image.unsqueeze(-1)\n",
    "            label = label.unsqueeze(-1)\n",
    "\n",
    "            \n",
    "        # Run torchio transformation - LabelMap will be secured for intensity\n",
    "        # transformations\n",
    "        subject = tio.Subject(\n",
    "            image=tio.ScalarImage(tensor=image),  \n",
    "            label=tio.LabelMap(tensor=label)\n",
    "        )\n",
    "        subject = self.spatial_transform(subject)\n",
    "        # Transform image intensities apart from subject - spatial transform\n",
    "        # was not applied to label correctly if transformations are stacked.\n",
    "        image = self.intensity_transform(subject.image).data\n",
    "        image = subject.image.data\n",
    "        label = subject.label.data\n",
    "\n",
    "        if yield_2d:\n",
    "            image = image.squeeze(-1)\n",
    "            label = label.squeeze(-1)\n",
    "                \n",
    "        label = label.long()\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb18f481-6804-48ad-8263-7f44b80cd5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    \n",
    "    'num_classes': 3,\n",
    "    'use_mind': True,\n",
    "    'epochs': 120,\n",
    "    \n",
    "    'batch_size': 64,\n",
    "    'val_batch_size': 1,\n",
    "    \n",
    "    'train_set_max_len': 100,\n",
    "    'crop_w_dim_range': (24, 110),\n",
    "    'yield_2d_normal_to': \"W\",\n",
    "    \n",
    "    'lr': 0.0005,\n",
    "    'use_cosine_annealing': False,\n",
    "    \n",
    "    # Data parameter config\n",
    "    'data_parameter_config': DotDict(\n",
    "        data_param_mode=int(DataParamMode.ONLY_INSTANCE_PARAMS),\n",
    "        init_class_param=0.01,\n",
    "        lr_class_param=0.1,\n",
    "        init_inst_param=1.0,\n",
    "        lr_inst_param=0.1,\n",
    "        wd_inst_param=0.0,\n",
    "        wd_class_param=0.0,\n",
    "        skip_clamp_data_param=False,\n",
    "        clamp_sigma_min=np.log(1/20),\n",
    "        clamp_sigma_max=np.log(20),\n",
    "        optim_algorithm=int(DataParamOptim.ADAM),\n",
    "        optim_options=dict(\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    'log_every': 1,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "    \n",
    "    'do_plot': False,\n",
    "    'debug': False,\n",
    "    'wandb_mode': \"online\",\n",
    "\n",
    "    'disturbed_percentage': .4,\n",
    "    'start_disturbing_after_ep': 40,\n",
    "    \n",
    "    'start_dilate_kernel_sz': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"source\", state=\"l4\", size=(128, 128, 128),\n",
    "    ensure_labeled_pairs=True, \n",
    "    max_load_num=config_dict['train_set_max_len'], \n",
    "    crop_w_dim_range=config_dict['crop_w_dim_range'],\n",
    "    yield_2d_normal_to=config_dict['yield_2d_normal_to'],\n",
    "    dilate_kernel_sz=config_dict['start_dilate_kernel_sz']\n",
    ")\n",
    "# validation_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "#     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "# target_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "#     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f25703b-de2d-4422-82cf-249ee741cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128, 128, 86])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, all_labels = training_dataset.get_data()\n",
    "print(all_labels.shape)\n",
    "# D_stack = make_2d_stack_from_3d(all_labels.unsqueeze(1), \"D\")\n",
    "# print(D_stack.shape)\n",
    "sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "plt.xlabel(\"W\")\n",
    "plt.ylabel(\"ground truth>0\")\n",
    "plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21b1edbb-45de-4a35-8524-a4848e50bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    # Print bare 2D data\n",
    "    # print(\"Displaying 2D bare sample\")\n",
    "    # for img, label in zip(training_dataset.img_data_2d.values(), \n",
    "    #                       training_dataset.label_data_2d.values()):\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=img.unsqueeze(0), \n",
    "    #                 ground_truth=label,\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "        \n",
    "    # Print transformed 2D data\n",
    "    training_dataset.train()\n",
    "    print(\"Displaying 2D training sample\")\n",
    "    for sample in training_dataset:\n",
    "        display_seg(in_type=\"single_2D\",\n",
    "                    img=sample['image'].unsqueeze(0), \n",
    "                    ground_truth=sample['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "    \n",
    "#     # Print transformed 3D data\n",
    "#     training_dataset.train()\n",
    "#     print(\"Displaying 3D training sample\")\n",
    "#     leng = 1# training_dataset.__len__(yield_2d_override=False)\n",
    "#     for sample in (training_dataset.get_crossmoda_3d_item(idx) for idx in range(leng)):\n",
    "#         # training_dataset.set_dilate_kernel_size(1)\n",
    "#         display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "#                     img=sample['image'].unsqueeze(0), \n",
    "#                     ground_truth=sample['label'],\n",
    "#                     crop_to_non_zero_gt=True,\n",
    "#                     alpha_gt = .3)\n",
    "        \n",
    "#         # training_dataset.set_dilate_kernel_size(7)\n",
    "#         display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "#                     img=sample['image'].unsqueeze(0), \n",
    "#                     ground_truth=sample['modified_label'],\n",
    "#                     crop_to_non_zero_gt=True,\n",
    "#                     alpha_gt = .3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ad65dae-5a01-4b13-af12-8422b730f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    for sidx in [1,]:\n",
    "        print(f\"Sample {sidx}:\")\n",
    "        \n",
    "        training_dataset.eval()\n",
    "        sample_eval = training_dataset.get_crossmoda_3d_item(sidx)\n",
    "\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=sample_eval['image'].unsqueeze(0), \n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "        \n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=sample_eval['image'].unsqueeze(0), \n",
    "                    ground_truth=sample_eval['label'], \n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "\n",
    "        training_dataset.train()\n",
    "        print(\"Train sample with ground-truth overlay\")\n",
    "        sample_train = training_dataset.get_crossmoda_3d_item(sidx)\n",
    "        print(sample_train['label'].unique())\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=sample_train['image'].unsqueeze(0), \n",
    "                    ground_truth=sample_train['label'], \n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt=.3)\n",
    "\n",
    "        print(\"Eval/train diff with diff overlay\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=(sample_eval['image'] - sample_train['image']).unsqueeze(0), \n",
    "                    ground_truth=(sample_eval['label'] - sample_train['label']).clamp(min=0), \n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f7ed00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_subset = torch.utils.data.Subset(training_dataset,range(2))\n",
    "if config_dict['do_plot']:\n",
    "    train_plotset = (training_dataset.get_crossmoda_3d_item(idx) for idx in (55, 81, 63))\n",
    "    for sample in train_plotset:\n",
    "        print(f\"Sample {sample['dataset_idx']}:\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "            img=sample_eval['image'].unsqueeze(0), \n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .6)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "            img=sample_eval['image'].unsqueeze(0), \n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1eb958f2-0fee-4ee3-b108-2a203bb65f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "\n",
    "import functools\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use get_named_layers_leaves(module) to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24d4986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(lraspp, optimizer, scaler, _path):\n",
    "    \n",
    "    torch.save(lraspp.state_dict(), _path + '_lraspp.pth')    \n",
    "    torch.save(optimizer.state_dict(), _path + '_optimizer.pth')\n",
    "    torch.save(scaler.state_dict(), _path + '_grad_scaler.pth')\n",
    "\n",
    "    # TODO add saving inst/class parameters again\n",
    "    # torch.save(inst_parameters, _path + '_inst_parameters.pth')\n",
    "    # torch.save(class_parameters, _path + '_class_parameters.pth')\n",
    "\n",
    "    \n",
    "    \n",
    "def get_model(config, dataset_len, _path=None):\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=False, progress=True, num_classes=config.num_classes)\n",
    "    set_module(lraspp, 'backbone.0.0', torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False))\n",
    "        \n",
    "    optimizer = torch.optim.Adam(lraspp.parameters(), lr=config.lr)\n",
    "    \n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    if _path and os.path.isfile(_path + '_lraspp.pth'):\n",
    "        print(f\"Loading lr-aspp model, optimizers and grad scalers from {_path}\")\n",
    "        # TODO add loading of data parameters\n",
    "        lraspp.load_state_dict(torch.load(_path + '_lraspp.pth', map_location='cuda'))\n",
    "\n",
    "        optimizer.load_state_dict(torch.load(_path + '_optimizer.pth', map_location='cuda'))\n",
    "        scaler.load_state_dict(torch.load(_path + '_grad_scaler.pth', map_location='cuda'))\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "        \n",
    "    return (lraspp, optimizer, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cabd4c44-6225-4ed6-bcdd-6b9fc6b30034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "        \n",
    "    \n",
    "            \n",
    "def log_data_parameters(log_path, parameter_idxs, parameters):\n",
    "    data = [[idx, param] for (idx, param) in \\\n",
    "        zip(parameter_idxs, torch.exp(parameters).tolist())]\n",
    "\n",
    "    table = wandb.Table(data=data, columns = [\"parameter_idx\", \"value\"])\n",
    "    wandb.log({log_path:wandb.plot.bar(table, \"parameter_idx\", \"value\", title=log_path)})\n",
    "    \n",
    "\n",
    "    \n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "    \n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "    \n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "    \n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "    \n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio\n",
    "    \n",
    "    \n",
    "    \n",
    "# def get_largest_data_parameters_in_target_ratio(parameter_idxs, parameters, target_idxs):\n",
    "#     # print(\"param_idxs\", parameter_idxs)\n",
    "#     # print(\"parameters\", parameters)\n",
    "#     # print(\"target_idxs\", target_idxs)\n",
    "#     data = [[inst_idx, val] for (inst_idx, val) in \\\n",
    "#         zip(parameter_idxs, torch.exp(parameters).tolist())]\n",
    "    \n",
    "#     topk_cont_idxs = torch.argsort(parameters, descending=True)[:len(target_idxs)]\n",
    "#     # print(\"topk_cont_idxs\", topk_cont_idxs)\n",
    "#     topk_dataset_idxs = parameter_idxs[topk_cont_idxs]\n",
    "#     # print(\"topk_dataset_idxs\", topk_dataset_idxs)\n",
    "#     ratio = np.sum(np.in1d(topk_dataset_idxs, target_idxs))/len(target_idxs)\n",
    "    \n",
    "#     return ratio\n",
    "\n",
    "\n",
    "\n",
    "def log_data_parameter_stats(log_path, epx, data_parameters):\n",
    "    \"\"\"Log stats for data parameters on wandb.\"\"\"\n",
    "    data_parameters = data_parameters.exp()\n",
    "    wandb.log({f'{log_path}/highest': torch.max(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/lowest': torch.min(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/mean': torch.mean(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/std': torch.std(data_parameters).item()}, step=epx)\n",
    "\n",
    "    \n",
    "    \n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e9d47e9-fe55-4241-86f1-796e6286e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "    \n",
    "    fold_iter = enumerate(kf.split(training_dataset))\n",
    "\n",
    "    if config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "    \n",
    "    if config.wandb_mode != 'disabled':\n",
    "        # Log dataset info\n",
    "        training_dataset.eval()\n",
    "        dataset_info = [[smp['dataset_idx'], smp['crossmoda_id'], smp['image_path'], smp['label_path']] \\\n",
    "                        for smp in training_dataset]\n",
    "        wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', 'crossmoda_id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    " \n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        \n",
    "        # Training happens in 2D, validation happens in 3D:\n",
    "        # Read 2D dataset idxs which are used for training, \n",
    "        # get their 3D super-ids by 3d dataset length\n",
    "        # and substract these from all 3D ids to get val_3d_idxs\n",
    "        trained_3d_dataset_idxs = {dct['dataset_idx'] \\\n",
    "             for dct in training_dataset.get_crossmoda_id_dicts() if dct['2d_dataset_idx'] in train_idxs.tolist()}\n",
    "        val_3d_idxs = set(range(training_dataset.__len__(yield_2d_override=False))) - trained_3d_dataset_idxs\n",
    "        print(\"Will run validation with these 3D samples:\", val_3d_idxs)\n",
    "        \n",
    "        ### Disturb dataset ###\n",
    "        disturbed_idxs = np.random.choice(train_idxs, size=int(len(train_idxs)*config.disturbed_percentage), replace=False)\n",
    "        disturbed_idxs = torch.tensor(disturbed_idxs)\n",
    "        \n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(disturbed_idxs.tolist()))\n",
    "        \n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in disturbed_idxs])}, \n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "        \n",
    "        ### Visualization ###\n",
    "        if config.do_plot:\n",
    "            print(\"Disturbed samples:\")\n",
    "            for d_idx in disturbed_idxs:\n",
    "                display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=training_dataset[d_idx][0], \n",
    "                    ground_truth=disturb_seg(training_dataset[d_idx][1]),\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "        \n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels =12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "            \n",
    "        _, all_segs = training_dataset.get_data()\n",
    "\n",
    "        # TODO add class weights again\n",
    "        # class_weight = torch.sqrt(1.0/(torch.bincount(all_segs.long().view(-1)).float()))\n",
    "        # class_weight = class_weight/class_weight.mean()\n",
    "        # class_weight[0] = 0.15\n",
    "        # class_weight = class_weight.cuda()\n",
    "        # print('inv sqrt class_weight', class_weight)\n",
    "        \n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size, \n",
    "            sampler=train_subsampler, pin_memory=True, drop_last=False,\n",
    "            collate_fn=training_dataset.get_efficient_augmentation_collate_fn())\n",
    "        training_dataset.unset_augment_at_collate()\n",
    "#         val_dataloader = DataLoader(training_dataset, batch_size=config.val_batch_size, \n",
    "#                                     sampler=val_subsampler, pin_memory=True, drop_last=False)\n",
    "      \n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        (lraspp, optimizer, scaler) = get_model(config, len(train_dataloader), _path=None)#f\"{config.mdl_save_prefix}_fold{fold_idx}\") # TODO start fresh set _path None\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=200, T_mult=2, eta_min=config.lr*.1, last_epoch=- 1, verbose=False)\n",
    "        \n",
    "        lraspp.cuda()\n",
    "        dpm = DataParameterManager(train_idxs.tolist(), ['background', 'tumour', 'cochlea'], \n",
    "            config=config.data_parameter_config, device='cuda')\n",
    "        \n",
    "        # criterion = nn.CrossEntropyLoss(class_weight)\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        for epx in range(config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "        \n",
    "            lraspp.train()\n",
    "            training_dataset.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            do_disturb = epx > config.start_disturbing_after_ep\n",
    "            wandb.log({\"do_disturb\": float(do_disturb)}, step=global_idx)\n",
    "\n",
    "            if do_disturb:\n",
    "                training_dataset.set_disturbed_idxs(disturbed_idxs)\n",
    "            else:\n",
    "                training_dataset.set_disturbed_idxs([])\n",
    "                \n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            dices_tumour = []\n",
    "            dices_cochlea = []\n",
    "            \n",
    "            # Load data\n",
    "            for batch in train_dataloader:\n",
    "                \n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "\n",
    "                b_img = b_img.float().cuda()\n",
    "                b_seg_modified = b_seg_modified.cuda()\n",
    "        \n",
    "                b_seg = b_seg.cuda()\n",
    "    \n",
    "                if config.use_mind:\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=True):\n",
    "                    assert b_img.dim() == 4, \\\n",
    "                        f\"Input image for model must be 4D: BxCxHxW but is {b_img.shape}\"\n",
    "                    \n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    ### Check dimensions ###\n",
    "                    assert logits.dim() == 4, \\\n",
    "                        f\"Logits shape must be BxNUM_CLASSESxHxW but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == 3, \\\n",
    "                        f\"Target shape for loss must be BxHxW but is {b_seg_modified.shape}\"\n",
    "\n",
    "                    dp_logits, loss_value = dpm.do_basic_train_step(\n",
    "                        criterion, logits.permute(0,2,3,1), torch.nn.functional.one_hot(b_seg, 3), \n",
    "                        optimizer, inst_keys=b_idxs_dataset.tolist(), scaler=scaler)\n",
    "    \n",
    "                epx_losses.append(loss_value)\n",
    "                \n",
    "                # Prepare logits for scoring\n",
    "                logits_for_score = logits.argmax(1) # TODO Check logits ok or dp_logits instead?\n",
    "\n",
    "                # Calculate dice score\n",
    "                dice = dice2d(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, 3),\n",
    "                    torch.nn.functional.one_hot(b_seg, 3), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_wo_bg(dice))\n",
    "                dices_tumour.append(get_batch_dice_tumour(dice))\n",
    "                dices_cochlea.append(get_batch_dice_cochlea(dice))\n",
    "                \n",
    "                if config.do_plot:\n",
    "                    print(\"Training 2D stack image label/ground-truth\")\n",
    "                    print(dice)\n",
    "                    \n",
    "                    display_seg(in_type=\"batch_2D\", \n",
    "                        img=batch['image'].unsqueeze(1).cpu(), \n",
    "                        seg=logits_for_score.cpu(),\n",
    "                        ground_truth=b_seg.cpu(),\n",
    "                        crop_to_non_zero_seg=True,\n",
    "                        crop_to_non_zero_gt=True,\n",
    "                        alpha_seg=.1,\n",
    "                        alpha_gt =.2\n",
    "                    )\n",
    "                    \n",
    "                if config.debug:\n",
    "                    break\n",
    "                    \n",
    "                ###  Scheduler management ###\n",
    "                if config.use_cosine_annealing:\n",
    "                    scheduler.step()\n",
    "\n",
    "                # if scheduler.T_cur == 0:\n",
    "                #     sz = training_dataset.get_dilate_kernel_size()\n",
    "                #     training_dataset.set_dilate_kernel_size(sz-1)\n",
    "                #     print(f\"Current dilate kernel size is {training_dataset.get_dilate_kernel_size()}.\")\n",
    "\n",
    "            ### Logging ###\n",
    "            if epx % config.log_every == 0 or (epx+1 == config.epochs):\n",
    "                           \n",
    "                ### Log wandb data ###\n",
    "                # Log the epoch idx per fold - so we can recover the diagram by setting \n",
    "                # ref_epoch_idx as x-axis in wandb interface\n",
    "                print(f'epx_fold{fold_idx}{epx} {time.time()-t0:.2f}s')\n",
    "                wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "                \n",
    "                mean_loss = torch.tensor(epx_losses).mean()\n",
    "                mean_dice = np.nanmean(dices)\n",
    "                mean_dice_tumour = np.nanmean(dices_tumour)\n",
    "                mean_dice_cochlea = np.nanmean(dices_cochlea)\n",
    "                \n",
    "                print()\n",
    "                print(f'loss_fold{fold_idx} {mean_loss:.6f}')\n",
    "                print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "                print(f'dice_mean_tumour_fold{fold_idx}', f\"{mean_dice_tumour*100:.2f}%\")\n",
    "                print(f'dice_mean_cochlea_fold{fold_idx}', f\"{mean_dice_cochlea*100:.2f}%\")\n",
    "                wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "                wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "                wandb.log({f'scores/dice_mean_tumour_fold{fold_idx}': mean_dice_tumour}, step=global_idx)\n",
    "                wandb.log({f'scores/dice_mean_cochlea_fold{fold_idx}': mean_dice_cochlea}, step=global_idx)\n",
    "            \n",
    "            \n",
    "                if (not dpm.disabled) and (dpm.data_param_mode != DataParamMode.ONLY_CLASS_PARAMS):\n",
    "                    # Log instance parameters of all samples\n",
    "                    log_data_parameter_stats(f\"data_parameters/instances/all_fold{fold_idx}\", global_idx, \n",
    "                            dpm.get_parameter_tensor(inst_keys='all').detach()\n",
    "                    )\n",
    "\n",
    "                    # Log instance parameters of disturbed samples\n",
    "                    if disturbed_idxs.numel() > 0:\n",
    "                        param_dict = dpm.get_data_parameters_dict()\n",
    "\n",
    "                        log_data_parameter_stats(f\"data_parameters/instances/disturbed_fold{fold_idx}\", global_idx, \n",
    "                            dpm.get_parameter_tensor(inst_keys=disturbed_idxs.tolist()).detach()\n",
    "                        )\n",
    "\n",
    "                        log_data_parameter_stats(f\"data_parameters/instances/clean_fold{fold_idx}\", global_idx, \n",
    "                            dpm.get_parameter_tensor(inst_keys=clean_idxs.tolist()).detach()\n",
    "                        )\n",
    "\n",
    "                        # Calculate ratio of data parameters in topN disturbed (is 1.0 if every disturbed\n",
    "                        # sample gets the highest data parameter)\n",
    "                        min_param_ratio = calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_idxs.tolist(), 'min')\n",
    "                        max_param_ratio = calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_idxs.tolist(), 'max')\n",
    "         \n",
    "                        print(f'inst_param_ratio_min_fold{fold_idx}', min_param_ratio)\n",
    "                        print(f'inst_param_ratio_max_fold{fold_idx}', max_param_ratio)\n",
    "                        wandb.log(\n",
    "                            {f'data_parameters/inst_param_ratio_min_fold{fold_idx}': min_param_ratio}, \n",
    "                            step=global_idx\n",
    "                        )\n",
    "                        wandb.log(\n",
    "                            {f'data_parameters/inst_param_ratio_max_fold{fold_idx}': max_param_ratio}, \n",
    "                            step=global_idx\n",
    "                        )\n",
    "                \n",
    "                ## Validation ###\n",
    "                                                    \n",
    "                lraspp.eval()\n",
    "                training_dataset.eval()   \n",
    "                # TODO remove saving \n",
    "                # save_model(lraspp, inst_parameters, class_parameters, \n",
    "                #     optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "                #     scaler, f\"{config.mdl_save_prefix}_fold{fold_idx}\")\n",
    "                \n",
    "                with amp.autocast(enabled=True):\n",
    "                    with torch.no_grad():\n",
    "                        val_dices = []\n",
    "                        val_dices_tumour = []\n",
    "                        val_dices_cochlea = []\n",
    "                        \n",
    "                        for val_idx in val_3d_idxs:\n",
    "                            val_sample = training_dataset.get_crossmoda_3d_item(val_idx)\n",
    "                            stack_dim = training_dataset.yield_2d_normal_to\n",
    "                            # Create batch out of single val sample\n",
    "                            b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                            b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "                            \n",
    "                            B = b_val_img.shape[0]\n",
    "\n",
    "                            b_val_img = b_val_img.unsqueeze(1).float().cuda()\n",
    "                            b_val_seg = b_val_seg.cuda()\n",
    "                            b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=stack_dim)\n",
    "                            \n",
    "                            if config.use_mind:\n",
    "                                b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "                                \n",
    "                            output_val = lraspp(b_val_img_2d)['out']\n",
    "\n",
    "                            # Prepare logits for scoring\n",
    "                            # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                            val_logits_for_score = make_3d_from_2d_stack(output_val, stack_dim, B)\n",
    "                            val_logits_for_score = val_logits_for_score.argmax(1)\n",
    "\n",
    "                            val_dice = dice3d(\n",
    "                                torch.nn.functional.one_hot(val_logits_for_score, 3),\n",
    "                                torch.nn.functional.one_hot(b_val_seg, 3), \n",
    "                                one_hot_torch_style=True\n",
    "                            )\n",
    "                            val_dices.append(get_batch_dice_wo_bg(val_dice))\n",
    "                            val_dices_tumour.append(get_batch_dice_tumour(val_dice))\n",
    "                            val_dices_cochlea.append(get_batch_dice_cochlea(val_dice))\n",
    "                            \n",
    "                            if config.do_plot:\n",
    "                                print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                                print(val_dice)\n",
    "                                # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                                display_seg(in_type=\"single_3D\", \n",
    "                                    reduce_dim=\"W\",\n",
    "                                    img=val_sample['image'].unsqueeze(0).cpu(), \n",
    "                                    seg=val_logits_for_score.squeeze(0).cpu(),\n",
    "                                    ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                    crop_to_non_zero_seg=True,\n",
    "                                    crop_to_non_zero_gt=True,\n",
    "                                    alpha_seg=.4,\n",
    "                                    alpha_gt=.2\n",
    "                                )\n",
    "                        mean_val_dice = np.nanmean(val_dices)\n",
    "                        mean_val_dice_tumour = np.nanmean(val_dices_tumour)\n",
    "                        mean_val_dice_cochlea = np.nanmean(val_dices_cochlea)\n",
    "                        \n",
    "                        print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                        print(f'val_dice_mean_tumour_fold{fold_idx}', f\"{mean_val_dice_tumour*100:.2f}%\")\n",
    "                        print(f'val_dice_mean_cochlea_fold{fold_idx}', f\"{mean_val_dice_cochlea*100:.2f}%\")\n",
    "                        wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                        wandb.log({f'scores/val_dice_mean_tumour_fold{fold_idx}': mean_val_dice_tumour}, step=global_idx)\n",
    "                        wandb.log({f'scores/val_dice_mean_cochlea_fold{fold_idx}': mean_val_dice_cochlea}, step=global_idx)\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "        \n",
    "        # End of fold loop\n",
    "        inst_parameters = dpm.get_parameter_tensor(inst_keys=train_idxs.tolist()).detach() # TODO simplify logging\n",
    "        log_data_parameters(f\"data_parameters/instance_parameters_fold{fold_idx}\", train_idxs, inst_parameters)\n",
    "\n",
    "        lraspp.cpu()\n",
    "        \n",
    "        save_model(lraspp,optimizer, scaler, f\"{config.mdl_save_prefix}_fold{fold_idx}\") # TODO save data parameter manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f476a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2jud0uic) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 0... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 4.99MB of 4.99MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>data_parameters/inst_param_ratio_max_fold0</td><td></td></tr><tr><td>data_parameters/inst_param_ratio_min_fold0</td><td></td></tr><tr><td>data_parameters/instances/all_fold0/highest</td><td></td></tr><tr><td>data_parameters/instances/all_fold0/lowest</td><td></td></tr><tr><td>data_parameters/instances/all_fold0/mean</td><td></td></tr><tr><td>data_parameters/instances/all_fold0/std</td><td></td></tr><tr><td>data_parameters/instances/clean_fold0/highest</td><td></td></tr><tr><td>data_parameters/instances/clean_fold0/lowest</td><td></td></tr><tr><td>data_parameters/instances/clean_fold0/mean</td><td></td></tr><tr><td>data_parameters/instances/clean_fold0/std</td><td></td></tr><tr><td>data_parameters/instances/disturbed_fold0/highest</td><td></td></tr><tr><td>data_parameters/instances/disturbed_fold0/lowest</td><td></td></tr><tr><td>data_parameters/instances/disturbed_fold0/mean</td><td></td></tr><tr><td>data_parameters/instances/disturbed_fold0/std</td><td></td></tr><tr><td>do_disturb</td><td></td></tr><tr><td>losses/loss_fold0</td><td></td></tr><tr><td>ref_epoch_idx</td><td></td></tr><tr><td>scores/dice_mean_cochlea_fold0</td><td></td></tr><tr><td>scores/dice_mean_tumour_fold0</td><td></td></tr><tr><td>scores/dice_mean_wo_bg_fold0</td><td></td></tr><tr><td>scores/val_dice_mean_cochlea_fold0</td><td></td></tr><tr><td>scores/val_dice_mean_tumour_fold0</td><td></td></tr><tr><td>scores/val_dice_mean_wo_bg_fold0</td><td></td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>data_parameters/inst_param_ratio_max_fold0</td><td>0.39599</td></tr><tr><td>data_parameters/inst_param_ratio_min_fold0</td><td>0.40384</td></tr><tr><td>data_parameters/instances/all_fold0/highest</td><td>1.60039</td></tr><tr><td>data_parameters/instances/all_fold0/lowest</td><td>0.39957</td></tr><tr><td>data_parameters/instances/all_fold0/mean</td><td>0.45517</td></tr><tr><td>data_parameters/instances/all_fold0/std</td><td>0.1148</td></tr><tr><td>data_parameters/instances/clean_fold0/highest</td><td>1.60039</td></tr><tr><td>data_parameters/instances/clean_fold0/lowest</td><td>0.39957</td></tr><tr><td>data_parameters/instances/clean_fold0/mean</td><td>0.45375</td></tr><tr><td>data_parameters/instances/clean_fold0/std</td><td>0.11228</td></tr><tr><td>data_parameters/instances/disturbed_fold0/highest</td><td>1.60039</td></tr><tr><td>data_parameters/instances/disturbed_fold0/lowest</td><td>0.39957</td></tr><tr><td>data_parameters/instances/disturbed_fold0/mean</td><td>0.45731</td></tr><tr><td>data_parameters/instances/disturbed_fold0/std</td><td>0.11847</td></tr><tr><td>do_disturb</td><td>0.0</td></tr><tr><td>losses/loss_fold0</td><td>0.45106</td></tr><tr><td>ref_epoch_idx</td><td>0</td></tr><tr><td>scores/dice_mean_cochlea_fold0</td><td>5e-05</td></tr><tr><td>scores/dice_mean_tumour_fold0</td><td>0.00575</td></tr><tr><td>scores/dice_mean_wo_bg_fold0</td><td>0.00388</td></tr><tr><td>scores/val_dice_mean_cochlea_fold0</td><td>0.0</td></tr><tr><td>scores/val_dice_mean_tumour_fold0</td><td>0.0</td></tr><tr><td>scores/val_dice_mean_wo_bg_fold0</td><td>0.0</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">woven-moon-276</strong>: <a href=\"https://wandb.ai/rap1ide/curriculum_deeplab/runs/2jud0uic\" target=\"_blank\">https://wandb.ai/rap1ide/curriculum_deeplab/runs/2jud0uic</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211122_190043-2jud0uic/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2jud0uic). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rap1ide/curriculum_deeplab/runs/tluat7tn\" target=\"_blank\">silvery-terrain-277</a></strong> to <a href=\"https://wandb.ai/rap1ide/curriculum_deeplab\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run validation with these 3D samples: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32}\n",
      "Disturbed indexes: [2868, 2871, 2876, 2879, 2881, 2882, 2889, 2890, 2896, 2897, 2898, 2899, 2900, 2901, 2903, 2904, 2905, 2906, 2909, 2911, 2912, 2916, 2920, 2924, 2931, 2932, 2933, 2937, 2939, 2952, 2954, 2955, 2962, 2963, 2965, 2971, 2980, 2984, 2985, 2989, 2991, 2992, 2997, 3001, 3002, 3004, 3005, 3007, 3009, 3010, 3011, 3015, 3019, 3021, 3023, 3025, 3026, 3028, 3029, 3034, 3037, 3040, 3049, 3052, 3055, 3056, 3057, 3059, 3062, 3069, 3070, 3073, 3077, 3081, 3084, 3085, 3087, 3088, 3090, 3091, 3094, 3097, 3100, 3108, 3109, 3111, 3112, 3113, 3114, 3115, 3119, 3120, 3124, 3128, 3130, 3132, 3136, 3138, 3143, 3149, 3150, 3152, 3153, 3154, 3158, 3161, 3162, 3163, 3165, 3169, 3171, 3175, 3178, 3181, 3184, 3189, 3192, 3193, 3200, 3209, 3212, 3213, 3214, 3215, 3216, 3219, 3225, 3231, 3235, 3238, 3243, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3255, 3257, 3261, 3263, 3265, 3268, 3269, 3272, 3273, 3275, 3277, 3282, 3287, 3288, 3295, 3296, 3302, 3307, 3308, 3310, 3311, 3313, 3317, 3318, 3319, 3320, 3324, 3328, 3329, 3330, 3331, 3332, 3334, 3337, 3338, 3339, 3346, 3347, 3348, 3349, 3350, 3352, 3354, 3356, 3358, 3362, 3363, 3365, 3366, 3367, 3368, 3369, 3374, 3376, 3381, 3384, 3386, 3387, 3388, 3393, 3394, 3395, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3405, 3406, 3408, 3410, 3412, 3413, 3415, 3416, 3419, 3420, 3422, 3423, 3425, 3426, 3429, 3432, 3434, 3436, 3439, 3442, 3443, 3445, 3448, 3449, 3451, 3460, 3465, 3466, 3468, 3475, 3480, 3486, 3487, 3488, 3489, 3491, 3495, 3496, 3497, 3500, 3501, 3502, 3503, 3505, 3506, 3508, 3509, 3515, 3516, 3518, 3525, 3527, 3528, 3530, 3533, 3534, 3535, 3542, 3545, 3547, 3550, 3551, 3552, 3554, 3557, 3562, 3571, 3573, 3575, 3579, 3583, 3585, 3587, 3591, 3592, 3593, 3594, 3595, 3596, 3598, 3600, 3602, 3604, 3614, 3615, 3618, 3627, 3630, 3635, 3638, 3640, 3642, 3643, 3647, 3654, 3655, 3659, 3661, 3666, 3670, 3673, 3676, 3678, 3679, 3684, 3685, 3692, 3695, 3696, 3698, 3701, 3703, 3707, 3708, 3709, 3710, 3711, 3714, 3716, 3719, 3721, 3724, 3728, 3729, 3730, 3731, 3733, 3734, 3735, 3738, 3739, 3740, 3742, 3745, 3747, 3749, 3750, 3755, 3757, 3758, 3759, 3760, 3762, 3763, 3765, 3766, 3767, 3771, 3773, 3777, 3781, 3783, 3784, 3785, 3792, 3794, 3796, 3800, 3805, 3806, 3809, 3811, 3816, 3820, 3829, 3832, 3838, 3843, 3844, 3845, 3846, 3847, 3849, 3850, 3852, 3858, 3859, 3861, 3862, 3863, 3868, 3869, 3870, 3872, 3875, 3876, 3878, 3879, 3880, 3881, 3888, 3889, 3896, 3898, 3901, 3909, 3910, 3914, 3917, 3918, 3932, 3933, 3934, 3935, 3936, 3940, 3942, 3944, 3949, 3951, 3952, 3954, 3958, 3962, 3963, 3965, 3971, 3975, 3976, 3981, 3982, 3983, 3985, 3986, 3988, 3989, 3990, 3994, 3999, 4006, 4008, 4009, 4012, 4015, 4017, 4022, 4027, 4028, 4030, 4032, 4037, 4039, 4043, 4044, 4047, 4052, 4056, 4058, 4059, 4063, 4064, 4069, 4070, 4073, 4083, 4087, 4090, 4091, 4097, 4099, 4101, 4104, 4106, 4107, 4112, 4113, 4114, 4115, 4117, 4123, 4126, 4128, 4129, 4132, 4133, 4134, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4146, 4148, 4149, 4151, 4152, 4155, 4157, 4161, 4162, 4165, 4166, 4169, 4170, 4176, 4181, 4186, 4193, 4194, 4195, 4196, 4197, 4199, 4203, 4205, 4210, 4216, 4218, 4224, 4225, 4228, 4229, 4230, 4231, 4233, 4234, 4235, 4236, 4237, 4238, 4243, 4244, 4246, 4247, 4252, 4253, 4254, 4256, 4257, 4258, 4260, 4265, 4266, 4271, 4273, 4277, 4278, 4279, 4281, 4285, 4286, 4287, 4288, 4289, 4290, 4294, 4295, 4297, 4298, 4299, 4300, 4304, 4306, 4309, 4315, 4317, 4319, 4320, 4322, 4323, 4324, 4325, 4327, 4328, 4331, 4332, 4337, 4338, 4341, 4342, 4345, 4347, 4349, 4351, 4354, 4356, 4357, 4358, 4360, 4362, 4363, 4364, 4366, 4367, 4372, 4374, 4376, 4378, 4380, 4381, 4382, 4383, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4396, 4397, 4398, 4400, 4403, 4407, 4409, 4411, 4412, 4414, 4418, 4423, 4426, 4430, 4431, 4434, 4435, 4436, 4438, 4440, 4447, 4448, 4453, 4457, 4460, 4462, 4463, 4464, 4468, 4473, 4474, 4482, 4487, 4489, 4491, 4492, 4499, 4507, 4509, 4510, 4511, 4516, 4518, 4521, 4524, 4525, 4529, 4532, 4533, 4535, 4536, 4537, 4542, 4545, 4547, 4550, 4554, 4555, 4558, 4559, 4562, 4563, 4565, 4567, 4569, 4572, 4573, 4575, 4577, 4579, 4582, 4585, 4588, 4589, 4593, 4603, 4605, 4608, 4609, 4611, 4613, 4616, 4617, 4619, 4620, 4621, 4622, 4624, 4626, 4627, 4628, 4630, 4632, 4633, 4634, 4635, 4638, 4639, 4641, 4645, 4650, 4651, 4653, 4654, 4656, 4657, 4658, 4660, 4667, 4670, 4671, 4680, 4681, 4682, 4683, 4684, 4686, 4691, 4693, 4695, 4696, 4697, 4698, 4700, 4701, 4702, 4704, 4706, 4707, 4710, 4715, 4717, 4719, 4720, 4722, 4726, 4728, 4731, 4732, 4734, 4736, 4737, 4739, 4742, 4746, 4748, 4754, 4755, 4756, 4758, 4759, 4767, 4769, 4773, 4779, 4783, 4786, 4788, 4789, 4793, 4795, 4798, 4800, 4801, 4803, 4806, 4807, 4808, 4809, 4813, 4816, 4818, 4820, 4824, 4825, 4827, 4828, 4832, 4836, 4839, 4841, 4844, 4846, 4849, 4851, 4852, 4853, 4856, 4857, 4862, 4863, 4864, 4866, 4873, 4876, 4878, 4879, 4880, 4881, 4883, 4884, 4893, 4894, 4896, 4897, 4900, 4903, 4905, 4910, 4911, 4915, 4916, 4925, 4927, 4932, 4933, 4934, 4937, 4938, 4939, 4942, 4945, 4946, 4948, 4951, 4954, 4955, 4956, 4957, 4958, 4967, 4969, 4970, 4971, 4973, 4975, 4978, 4985, 4991, 4998, 5000, 5001, 5004, 5007, 5008, 5014, 5015, 5016, 5019, 5021, 5029, 5031, 5032, 5039, 5042, 5044, 5045, 5046, 5049, 5051, 5055, 5056, 5057, 5060, 5061, 5063, 5067, 5068, 5071, 5074, 5078, 5079, 5081, 5082, 5083, 5084, 5090, 5091, 5095, 5096, 5097, 5099, 5101, 5102, 5106, 5107, 5108, 5109, 5110, 5111, 5114, 5117, 5122, 5126, 5127, 5129, 5132, 5133, 5134, 5137, 5139, 5142, 5145, 5146, 5151, 5152, 5160, 5161, 5167, 5168, 5170, 5173, 5175, 5178, 5181, 5182, 5190, 5191, 5192, 5196, 5199, 5204, 5206, 5207, 5212, 5213, 5214, 5217, 5222, 5224, 5226, 5231, 5235, 5236, 5239, 5240, 5241, 5244, 5245, 5246, 5248, 5249, 5252, 5255, 5263, 5267, 5268, 5269, 5271, 5273, 5274, 5288, 5290, 5292, 5293, 5296, 5310, 5314, 5315, 5317, 5320, 5322, 5323, 5324, 5327, 5331, 5332, 5333, 5339, 5342, 5344, 5345, 5348, 5350, 5357, 5360, 5367, 5368, 5372, 5374, 5377, 5380, 5385, 5386, 5388, 5390, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5400, 5401, 5404, 5406, 5409, 5410, 5416, 5417, 5418, 5419, 5420, 5430, 5432, 5436, 5441, 5442, 5444, 5445, 5446, 5447, 5451, 5452, 5453, 5454, 5455, 5457, 5458, 5459, 5461, 5467, 5469, 5471, 5473, 5478, 5479, 5482, 5483, 5484, 5485, 5486, 5487, 5490, 5491, 5492, 5493, 5498, 5502, 5506, 5507, 5508, 5511, 5514, 5515, 5517, 5518, 5519, 5520, 5522, 5524, 5525, 5528, 5531, 5536, 5537, 5545, 5549, 5553, 5556, 5557, 5563, 5567, 5568, 5569, 5573, 5574, 5575, 5576, 5581, 5583, 5584, 5585, 5587, 5591, 5593, 5597, 5598, 5601, 5603, 5605, 5607, 5610, 5614, 5616, 5619, 5620, 5622, 5625, 5627, 5628, 5629, 5631, 5635, 5636, 5638, 5639, 5644, 5649, 5651, 5657, 5660, 5663, 5666, 5672, 5673, 5674, 5676, 5677, 5684, 5686, 5690, 5691, 5695, 5697, 5698, 5699, 5703, 5705, 5706, 5707, 5709, 5711, 5713, 5714, 5715, 5716, 5724, 5725, 5726, 5733, 5735, 5736, 5743, 5745, 5749, 5750, 5754, 5755, 5756, 5757, 5759, 5763, 5766, 5768, 5769, 5772, 5773, 5776, 5780, 5782, 5783, 5788, 5790, 5796, 5800, 5803, 5805, 5806, 5807, 5809, 5810, 5813, 5814, 5815, 5816, 5822, 5825, 5826, 5828, 5829, 5833, 5836, 5838, 5839, 5841, 5842, 5843, 5846, 5847, 5850, 5851, 5853, 5859, 5861, 5863, 5865, 5867, 5868, 5873, 5877, 5880, 5885, 5886, 5888, 5892, 5894, 5899, 5901, 5902, 5904, 5905, 5906, 5907, 5909, 5912, 5913, 5915, 5917, 5919, 5921, 5922, 5925, 5928, 5931, 5932, 5934, 5936, 5938, 5940, 5941, 5942, 5946, 5947, 5948, 5953, 5955, 5957, 5959, 5960, 5962, 5965, 5966, 5968, 5975, 5978, 5980, 5984, 5985, 5986, 5988, 5993, 5998, 6002, 6008, 6011, 6012, 6014, 6017, 6018, 6019, 6021, 6022, 6024, 6025, 6027, 6028, 6029, 6034, 6037, 6038, 6043, 6047, 6049, 6050, 6051, 6055, 6059, 6063, 6066, 6067, 6068, 6069, 6076, 6077, 6079, 6080, 6081, 6087, 6089, 6100, 6101, 6102, 6107, 6108, 6109, 6118, 6120, 6121, 6123, 6127, 6128, 6129, 6130, 6133, 6134, 6136, 6137, 6148, 6149, 6151, 6156, 6162, 6163, 6169, 6172, 6178, 6179, 6184, 6185, 6186, 6187, 6190, 6193, 6194, 6199, 6207, 6208, 6210, 6212, 6215, 6217, 6219, 6221, 6226, 6229, 6230, 6232, 6234, 6237, 6240, 6241, 6242, 6245, 6247, 6248, 6252, 6257, 6259, 6261, 6264, 6270, 6272, 6274, 6278, 6279, 6281, 6284, 6291, 6292, 6295, 6296, 6298, 6299, 6300, 6301, 6302, 6306, 6307, 6316, 6317, 6319, 6320, 6323, 6327, 6328, 6329, 6336, 6338, 6341, 6347, 6348, 6351, 6352, 6354, 6355, 6357, 6358, 6362, 6363, 6364, 6365, 6366, 6368, 6369, 6370, 6375, 6379, 6381, 6382, 6384, 6390, 6391, 6393, 6395, 6396, 6401, 6407, 6409, 6411, 6417, 6418, 6419, 6421, 6422, 6423, 6426, 6428, 6430, 6431, 6432, 6438, 6448, 6449, 6450, 6451, 6453, 6454, 6456, 6459, 6462, 6464, 6473, 6476, 6481, 6483, 6485, 6486, 6493, 6497, 6498, 6500, 6501, 6502, 6503, 6507, 6512, 6513, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6525, 6530, 6534, 6536, 6538, 6539, 6540, 6542, 6544, 6546, 6549, 6550, 6551, 6554, 6555, 6556, 6557, 6558, 6560, 6563, 6576, 6577, 6578, 6579, 6580, 6589, 6591, 6592, 6593, 6595, 6598, 6599, 6600, 6602, 6603, 6605, 6607, 6611, 6613, 6617, 6620, 6621, 6623, 6626, 6627, 6628, 6631, 6638, 6640, 6643, 6644, 6651, 6653, 6654, 6655, 6656, 6657, 6658, 6662, 6665, 6670, 6671, 6674, 6677, 6681, 6683, 6687, 6689, 6690, 6691, 6693, 6695, 6696, 6697, 6699, 6700, 6703, 6704, 6706, 6707, 6711, 6712, 6717, 6720, 6723, 6725, 6729, 6732, 6740, 6741, 6746, 6750, 6752, 6754, 6755, 6756, 6761, 6762, 6767, 6769, 6770, 6772, 6776, 6780, 6781, 6784, 6792, 6793, 6800, 6802, 6803, 6806, 6810, 6811, 6815, 6817, 6820, 6821, 6823, 6824, 6831, 6839, 6842, 6843, 6844, 6845, 6846, 6848, 6853, 6856, 6858, 6860, 6863, 6867, 6870, 6877, 6882, 6883, 6884, 6888, 6891, 6893, 6894, 6895, 6896, 6898, 6899, 6901, 6903, 6904, 6908, 6912, 6913, 6919, 6920, 6922, 6925, 6926, 6927, 6928, 6930, 6931, 6932, 6934, 6937, 6940, 6943, 6944, 6950, 6955, 6961, 6962, 6964, 6965, 6968, 6969, 6972, 6977, 6978, 6980, 6981, 6982, 6985, 6987, 6992, 6993, 6994, 6997, 6999, 7000, 7003, 7005, 7006, 7007, 7008, 7010, 7011, 7012, 7013, 7016, 7017, 7021, 7022, 7023, 7024, 7027, 7029, 7033, 7034, 7035, 7036, 7037, 7041, 7043, 7045, 7046, 7051, 7057, 7060, 7061, 7062, 7064, 7067, 7071, 7082, 7083, 7084, 7085, 7088, 7091, 7092, 7094, 7095, 7097, 7098, 7100, 7107, 7108, 7111, 7112, 7113, 7116, 7119, 7122, 7125, 7131, 7137, 7140, 7142, 7143, 7145, 7146, 7150, 7151, 7153, 7154, 7158, 7159, 7160, 7161, 7166, 7168, 7169, 7174, 7176, 7177, 7179, 7182, 7188, 7195, 7196, 7197, 7198, 7203, 7211, 7212, 7214, 7215, 7217, 7219, 7222, 7225, 7229, 7231, 7234, 7238, 7241, 7244, 7246, 7247, 7248, 7252, 7255, 7256, 7257, 7258, 7259, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7269, 7272, 7275, 7277, 7279, 7289, 7291, 7293, 7295, 7297, 7298, 7301, 7302, 7303, 7306, 7308, 7309, 7310, 7313, 7315, 7320, 7323, 7324, 7329, 7332, 7339, 7341, 7342, 7343, 7347, 7354, 7355, 7356, 7357, 7360, 7366, 7368, 7373, 7374, 7377, 7378, 7379, 7380, 7381, 7385, 7386, 7394, 7396, 7397, 7405, 7406, 7408, 7410, 7411, 7416, 7419, 7421, 7423, 7424, 7430, 7431, 7435, 7436, 7440, 7441, 7444, 7447, 7449, 7451, 7453, 7454, 7456, 7461, 7462, 7463, 7466, 7470, 7476, 7477, 7479, 7484, 7485, 7488, 7490, 7491, 7496, 7499, 7500, 7502, 7506, 7507, 7508, 7510, 7511, 7512, 7513, 7516, 7517, 7522, 7523, 7524, 7527, 7531, 7537, 7538, 7540, 7544, 7545, 7546, 7549, 7552, 7553, 7554, 7556, 7561, 7565, 7568, 7572, 7574, 7578, 7579, 7583, 7586, 7591, 7592, 7595, 7599, 7600, 7601, 7609, 7610, 7613, 7615, 7618, 7619, 7622, 7624, 7625, 7626, 7627, 7628, 7630, 7631, 7634, 7635, 7639, 7640, 7641, 7642, 7648, 7650, 7652, 7653, 7655, 7656, 7659, 7661, 7663, 7664, 7665, 7666, 7669, 7674, 7676, 7677, 7678, 7679, 7680, 7682, 7683, 7687, 7689, 7690, 7694, 7696, 7698, 7700, 7701, 7706, 7711, 7714, 7721, 7722, 7723, 7724, 7727, 7732, 7733, 7734, 7735, 7740, 7742, 7745, 7746, 7747, 7752, 7758, 7761, 7762, 7764, 7766, 7767, 7768, 7769, 7771, 7773, 7774, 7775, 7776, 7777, 7778, 7780, 7785, 7786, 7788, 7789, 7790, 7791, 7792, 7794, 7795, 7800, 7801, 7805, 7807, 7809, 7810, 7811, 7813, 7814, 7815, 7816, 7819, 7821, 7823, 7824, 7827, 7829, 7830, 7834, 7844, 7847, 7849, 7850, 7853, 7854, 7856, 7858, 7859, 7862, 7867, 7869, 7870, 7871, 7873, 7874, 7875, 7876, 7887, 7889, 7891, 7893, 7897, 7898, 7899, 7900, 7902, 7906, 7907, 7911, 7912, 7916, 7918, 7919, 7920, 7923, 7926, 7927, 7928, 7929, 7930, 7931, 7933, 7947, 7950, 7954, 7959, 7963, 7965, 7972, 7976, 7977, 7979, 7980, 7984, 7986, 7988, 7989, 7990, 7996, 8000, 8003, 8004, 8006, 8007, 8013, 8017, 8018, 8019, 8020, 8021, 8022, 8025, 8029, 8031, 8033, 8034, 8038, 8041, 8042, 8043, 8045, 8046, 8047, 8050, 8052, 8053, 8054, 8056, 8061, 8064, 8071, 8072, 8073, 8077, 8078, 8082, 8084, 8085, 8088, 8089, 8090, 8092, 8098, 8101, 8106, 8110, 8113, 8114, 8115, 8117, 8120, 8123, 8127, 8129, 8132, 8133, 8136, 8141, 8143, 8145, 8146, 8147, 8149, 8150, 8153, 8157, 8160, 8162, 8165, 8167, 8171, 8172, 8174, 8175, 8176, 8181, 8182, 8183, 8191, 8192, 8194, 8195, 8200, 8202, 8207, 8211, 8216, 8217, 8218, 8219, 8223, 8227, 8228, 8232, 8233, 8236, 8237, 8238, 8240, 8244, 8245, 8249, 8251, 8254, 8255, 8256, 8257, 8259, 8263, 8264, 8265, 8266, 8274, 8275, 8278, 8281, 8283, 8284, 8291, 8293, 8294, 8298, 8299, 8301, 8304, 8307, 8311, 8314, 8320, 8322, 8324, 8327, 8328, 8330, 8332, 8333, 8334, 8337, 8338, 8339, 8343, 8351, 8353, 8363, 8365, 8367, 8370, 8375, 8376, 8378, 8379, 8382, 8383, 8387, 8389, 8391, 8392, 8397, 8403, 8404, 8406, 8407, 8411, 8414, 8415, 8417, 8419, 8420, 8422, 8424, 8427, 8428, 8435, 8438, 8442, 8445, 8450, 8452, 8453, 8454, 8455, 8456, 8458, 8461, 8462, 8467, 8474, 8476, 8477, 8482, 8483, 8485, 8486, 8487, 8488, 8489, 8490, 8493, 8496, 8498, 8503, 8504, 8508, 8517, 8518, 8522, 8523, 8526, 8527, 8534, 8535, 8536, 8538, 8541, 8546, 8548, 8550, 8552, 8553, 8555, 8559, 8562, 8571, 8575, 8578, 8580, 8581, 8582, 8592, 8593, 8594, 8598]\n",
      "Generating fresh lr-aspp model, optimizer and grad scaler.\n",
      "Initialized instance data parameters with: 1.0\n",
      "epx_fold00 140.59s\n",
      "\n",
      "loss_fold0 0.441941\n",
      "dice_mean_wo_bg_fold0 0.38%\n",
      "dice_mean_tumour_fold0 0.56%\n",
      "dice_mean_cochlea_fold0 0.01%\n",
      "inst_param_ratio_min_fold0 0.4042738770170083\n",
      "inst_param_ratio_max_fold0 0.39598778892280856\n",
      "val_dice_mean_wo_bg_fold0 0.00%\n",
      "val_dice_mean_tumour_fold0 0.00%\n",
      "val_dice_mean_cochlea_fold0 0.00%\n",
      "epx_fold01 310.13s\n",
      "\n",
      "loss_fold0 0.083512\n",
      "dice_mean_wo_bg_fold0 3.63%\n",
      "dice_mean_tumour_fold0 5.38%\n",
      "dice_mean_cochlea_fold0 0.00%\n",
      "inst_param_ratio_min_fold0 0.4077627562145661\n",
      "inst_param_ratio_max_fold0 0.3951155691234191\n",
      "val_dice_mean_wo_bg_fold0 0.00%\n",
      "val_dice_mean_tumour_fold0 0.00%\n",
      "val_dice_mean_cochlea_fold0 0.00%\n",
      "epx_fold02 476.69s\n",
      "\n",
      "loss_fold0 0.014119\n",
      "dice_mean_wo_bg_fold0 34.04%\n",
      "dice_mean_tumour_fold0 50.34%\n",
      "dice_mean_cochlea_fold0 0.00%\n",
      "inst_param_ratio_min_fold0 0.40558220671609246\n",
      "inst_param_ratio_max_fold0 0.3911905800261666\n",
      "val_dice_mean_wo_bg_fold0 0.00%\n",
      "val_dice_mean_tumour_fold0 0.00%\n",
      "val_dice_mean_cochlea_fold0 0.00%\n",
      "epx_fold03 647.46s\n",
      "\n",
      "loss_fold0 0.009323\n",
      "dice_mean_wo_bg_fold0 40.92%\n",
      "dice_mean_tumour_fold0 60.75%\n",
      "dice_mean_cochlea_fold0 0.00%\n",
      "inst_param_ratio_min_fold0 0.40340165721761884\n",
      "inst_param_ratio_max_fold0 0.39555167902311383\n",
      "val_dice_mean_wo_bg_fold0 0.49%\n",
      "val_dice_mean_tumour_fold0 0.97%\n",
      "val_dice_mean_cochlea_fold0 0.00%\n",
      "epx_fold04 816.56s\n",
      "\n",
      "loss_fold0 0.007415\n",
      "dice_mean_wo_bg_fold0 43.61%\n",
      "dice_mean_tumour_fold0 64.72%\n",
      "dice_mean_cochlea_fold0 0.00%\n",
      "inst_param_ratio_min_fold0 0.4060183166157872\n",
      "inst_param_ratio_max_fold0 0.39380723942433493\n",
      "val_dice_mean_wo_bg_fold0 13.43%\n",
      "val_dice_mean_tumour_fold0 26.86%\n",
      "val_dice_mean_cochlea_fold0 0.00%\n"
     ]
    }
   ],
   "source": [
    "config_dict['debug'] = False\n",
    "config_dict['wandb_mode'] = 'online'\n",
    "\n",
    "run = wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "    config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "    mode=config_dict['wandb_mode']\n",
    ")\n",
    "\n",
    "run_name = run.name\n",
    "config = wandb.config\n",
    "\n",
    "train_DL(run_name, config, training_dataset)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fef1bd-ec6a-4bbd-a5d0-a2b006e8813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_DL(run_name, config, inf_dataset):\n",
    "\n",
    "    score_dicts = []\n",
    "    \n",
    "    fold_iter = range(config.num_folds)\n",
    "    if config_dict['only_first_fold']:\n",
    "        fold_iter = fold_iter[0:1]\n",
    "        \n",
    "    for fold_idx in fold_iter:\n",
    "        lraspp, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "        lraspp.eval()\n",
    "        inf_dataset.eval()   \n",
    "        stack_dim = config.yield_2d_normal_to\n",
    "        \n",
    "        inf_dices = []\n",
    "        inf_dices_tumour = []\n",
    "        inf_dices_cochlea = []\n",
    "        \n",
    "        for inf_sample in inf_dataset:\n",
    "            global_idx = get_global_idx(fold_idx, sample_idx, config.epochs)\n",
    "            crossmoda_id = sample['crossmoda_id']\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Create batch out of single val sample\n",
    "                    b_inf_img = inf_sample['image'].unsqueeze(0)\n",
    "                    b_inf_seg = inf_sample['label'].unsqueeze(0)\n",
    "\n",
    "                    B = b_inf_img.shape[0]\n",
    "\n",
    "                    b_inf_img = b_inf_img.unsqueeze(1).float().cuda()\n",
    "                    b_inf_seg = b_inf_seg.cuda()\n",
    "                    b_inf_img_2d = make_2d_stack_from_3d(b_inf_img, stack_dim=stack_dim)\n",
    "\n",
    "                    if config.use_mind:\n",
    "                        b_inf_img_2d = mindssc(b_inf_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                    output_inf = lraspp(b_inf_img_2d)['out']\n",
    "\n",
    "                    # Prepare logits for scoring\n",
    "                    # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                    inf_logits_for_score = make_3d_from_2d_stack(output_inf, stack_dim, B)\n",
    "                    inf_logits_for_score = inf_logits_for_score.argmax(1)\n",
    "\n",
    "                    inf_dice = dice3d(\n",
    "                        torch.nn.functional.one_hot(inf_logits_for_score, 3),\n",
    "                        torch.nn.functional.one_hot(b_inf_seg, 3), \n",
    "                        one_hot_torch_style=True\n",
    "                    )\n",
    "                    inf_dices.append(get_batch_dice_wo_bg(inf_dice))\n",
    "                    inf_dices_tumour.append(get_batch_dice_tumour(inf_dice))\n",
    "                    inf_dices_cochlea.append(get_batch_dice_cochlea(inf_dice))\n",
    "\n",
    "                    if config.do_plot:\n",
    "                        print(\"Inference 3D image label/ground-truth\")\n",
    "                        print(inf_dice)\n",
    "                        # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                        display_seg(in_type=\"single_3D\", \n",
    "                            reduce_dim=\"W\",\n",
    "                            img=inf_sample['image'].unsqueeze(0).cpu(), \n",
    "                            seg=inf_logits_for_score.squeeze(0).cpu(),\n",
    "                            ground_truth=b_inf_seg.squeeze(0).cpu(),\n",
    "                            crop_to_non_zero_seg=True,\n",
    "                            crop_to_non_zero_gt=True,\n",
    "                            alpha_seg=.4,\n",
    "                            alpha_gt=.2\n",
    "                        )\n",
    "                        \n",
    "            if config.debug:\n",
    "                break\n",
    "                \n",
    "        mean_inf_dice = np.nanmean(inf_dices)\n",
    "        mean_inf_dice_tumour = np.nanmean(inf_dices_tumour)\n",
    "        mean_inf_dice_cochlea = np.nanmean(inf_dices_cochlea)\n",
    "\n",
    "        print(f'inf_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_inf_dice*100:.2f}%\")\n",
    "        print(f'inf_dice_mean_tumour_fold{fold_idx}', f\"{mean_inf_dice_tumour*100:.2f}%\")\n",
    "        print(f'inf_dice_mean_cochlea_fold{fold_idx}', f\"{mean_inf_dice_cochlea*100:.2f}%\")\n",
    "        wandb.log({f'scores/inf_dice_mean_wo_bg_fold{fold_idx}': mean_inf_dice}, step=global_idx)\n",
    "        wandb.log({f'scores/inf_dice_mean_tumour_fold{fold_idx}': mean_inf_dice_tumour}, step=global_idx)\n",
    "        wandb.log({f'scores/inf_dice_mean_cochlea_fold{fold_idx}': mean_inf_dice_cochlea}, step=global_idx)\n",
    "\n",
    "        # Store data for inter-fold scoring\n",
    "        class_dice_list = inf_dices.tolist()[0]\n",
    "        for class_idx, class_dice in enumerate(class_dice_list):\n",
    "            score_dicts.append(\n",
    "                {\n",
    "                    'fold_idx': fold_idx,\n",
    "                    'crossmoda_id': crossmoda_id,\n",
    "                    'class_idx': class_idx,\n",
    "                    'class_dice': class_dice,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    mean_oa_inf_dice = np.nanmean(torch.tensor([score['class_dice'] for score in score_dicts]))\n",
    "    print(f\"Mean dice over all folds, classes and samples: {mean_oa_inf_dice*100:.2f}%\")\n",
    "    wandb.log({'scores/mean_dice_all_folds_samples_classes': mean_oa_inf_dice}, step=global_idx)\n",
    "\n",
    "    return score_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_scores = []\n",
    "run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"testing\", job_type=\"test\",\n",
    "        config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "        mode=config_dict['wandb_mode']\n",
    ")\n",
    "config = wandb.config\n",
    "score_dicts = inference_DL(run_name, config, validation_dataset)\n",
    "folds_scores.append(score_dicts)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
