{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fc395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name              Util    Mem free  Cuda             User(s)\n",
      "----  -------------------  ------  ----------  ---------------  -------------\n",
      "   3  Quadro RTX 8000         0 %   38557 MiB  11.1(455.45.01)  falta\n",
      "   2  GeForce RTX 2080 Ti     0 %    9929 MiB  11.1(455.45.01)  grossbroehmer\n",
      "   1  Tesla T4                0 %    8855 MiB  11.1(455.45.01)  hempe\n",
      "   5  GeForce RTX 2080 Ti     1 %   11019 MiB  11.1(455.45.01)\n",
      "   4  GeForce RTX 2080 Ti    15 %     985 MiB  11.1(455.45.01)  falta\n",
      "   0  Tesla T4               52 %    5137 MiB  11.1(455.45.01)  falta\n",
      "   6  GeForce RTX 2080 Ti    52 %     985 MiB  11.1(455.45.01)  falta\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                torch\n",
      "----  -------------------  --  -------\n",
      "   2  GeForce RTX 2080 Ti  ->  cuda:0\n",
      "1.9.1+cu102\n",
      "7605\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -3 -4\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import get_overlay_grid\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "import wandb\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca60d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            # nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        # x = F.adaptive_avg_pool3d(x, (1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')  # , align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = [nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())]\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    " \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "# Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "def create_model(output_classes: int = 14,input_channels: int = 1):\n",
    "    # in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "    in_channels = torch.Tensor([input_channels, 24, 24, 32, 48, 48, 48, 64]).long()\n",
    "    mid_channels = torch.Tensor([64, 128, 192, 192, 256, 256, 256, 384]).long()\n",
    "    out_channels = torch.Tensor([24, 24, 32, 48, 48, 48, 64, 64]).long()\n",
    "    mid_stride = torch.Tensor([1, 1, 1, 2, 1, 1, 1, 1])\n",
    "    net = [nn.Identity()]\n",
    "    for i in range(8):\n",
    "        inc = int(in_channels[i])\n",
    "        midc = int(mid_channels[i])\n",
    "        outc = int(out_channels[i])\n",
    "        strd = int(mid_stride[i])\n",
    "        layer = nn.Sequential(nn.Conv3d(inc, midc, 1, bias=False), nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, midc, 3, stride=strd, padding=1, bias=False, groups=midc),\n",
    "                              nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, outc, 1, bias=False), nn.BatchNorm3d(outc))\n",
    "        if i == 0:\n",
    "            layer[0] = nn.Conv3d(inc, midc, 3, padding=1, stride=2, bias=False)\n",
    "        if (inc == outc) & (strd == 1):\n",
    "            net.append(ResBlock(layer))\n",
    "        else:\n",
    "            net.append(layer)\n",
    "\n",
    "    backbone = nn.Sequential(*net)\n",
    "\n",
    "    count = 0\n",
    "    # weight initialization\n",
    "    for m in backbone.modules():\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            count += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    print('#CNN layer', count)\n",
    "    # complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "    # newer model (one more stride, no groups in head)\n",
    "    aspp = ASPP(64, (2, 4, 8, 16, 32), 128)\n",
    "    head = nn.Sequential(nn.Conv3d(128 + 24, 64, 1, padding=0, groups=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, 64, 3, groups=1, padding=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, output_classes, 1))\n",
    "    return backbone, aspp, head\n",
    "\n",
    "\n",
    "def apply_model(backbone, aspp, head, img, checkpointing=True, return_intermediate=False):\n",
    "    if checkpointing:\n",
    "        x1 = checkpoint(backbone[:3], img)\n",
    "        x2 = checkpoint(backbone[3:], x1)\n",
    "        y = checkpoint(aspp, x2)\n",
    "        y1 = torch.cat((x1[0:1], F.interpolate(y[0:1], scale_factor=2)), 1)\n",
    "        output_j = checkpoint(head, y1)\n",
    "    else:\n",
    "        x1 = backbone[:3](img)\n",
    "        x2 = backbone[3:](x1)\n",
    "        y = aspp(x2)\n",
    "        y1 = torch.cat((x1[0:1], F.interpolate(y[0:1], scale_factor=2)), 1)\n",
    "        output_j = head(y1)\n",
    "    if return_intermediate:\n",
    "        return y1,output_j\n",
    "    else:\n",
    "        return output_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class CrossMoDa_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        ensure_labeled_pairs=True, use_additional_data=False, resample = True,\n",
    "        size:tuple = (96,96,60), normalize:bool = True):\n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                ensure_labeled_pairs (bool): Only images with corresponding labels will be loaded (default: True)\n",
    "                \n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "\n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "        path = base_dir + state_dir\n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "        if domain.lower() ==\"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "        if ensure_labeled_pairs:\n",
    "            def get_bare_basename(_path):\n",
    "                return str(Path(_path.replace('.nii.gz', '')).stem)\n",
    "\n",
    "            labeled_files = [\n",
    "                _path for _path in files \\\n",
    "                    if '_Label' in get_bare_basename(_path) \\\n",
    "                    or get_bare_basename(_path)+'_Label' in [get_bare_basename(_path) for _path in files]\n",
    "            ]\n",
    "            files = labeled_files\n",
    "            \n",
    "        #initialize variables\n",
    "        self.imgs = torch.zeros(0,size[0],size[1],size[2])\n",
    "        self.labels = torch.zeros(0,size[0],size[1],size[2])\n",
    "        self.img_nums = []\n",
    "        self.label_nums = []\n",
    "        #load data\n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "\n",
    "        for i,f in enumerate(tqdm(files)):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in f:\n",
    "                self.label_nums.append(int(re.findall(r'\\d+', os.path.basename(f))[0]))\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                self.labels = torch.cat((self.labels,tmp.unsqueeze(0)),dim=0)\n",
    "            elif domain in f:\n",
    "                self.img_nums.append(int(re.findall(r'\\d+', os.path.basename(f))[0]))\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                self.imgs = torch.cat((self.imgs,tmp.unsqueeze(0)),dim=0)\n",
    "        self.labels = self.labels.long()\n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(self.img_nums==self.label_nums))\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(self.imgs.shape,self.imgs.mean(),self.imgs.std()))\n",
    "        print(\"Label shape: {}, max.: {}\".format(self.labels.shape,torch.max(self.labels)))\n",
    "        print(\"Data import finished. Elapsed time: {:.1f} s\".format(time.time()-t0 ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.imgs.size(0))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label.long()\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.imgs,self.labels\n",
    "\n",
    "    def get_image_numbers(self):\n",
    "        return self.img_nums\n",
    "\n",
    "    def get_label_numbers(self):\n",
    "        return self.label_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training routine\n",
    "def display_nonempty_seg_slices(img_slices, seg_slices, alpha=.5):\n",
    "    color_map = {\n",
    "        0: None, \n",
    "        1: (255,0,0), #ONEHOT id and RGB color\n",
    "        2: (0,255,0)\n",
    "    }\n",
    "\n",
    "    idx_dept_with_segs, *_ = torch.nonzero(seg_slices > 0, as_tuple=True)\n",
    "    \n",
    "    if idx_dept_with_segs.nelement() > 0:\n",
    "        idx_dept_with_segs = idx_dept_with_segs.unique()\n",
    "\n",
    "        img_slices = img_slices[idx_dept_with_segs]\n",
    "        seg_slices = seg_slices[idx_dept_with_segs]\n",
    "        \n",
    "        pil_ov, _ = get_overlay_grid(\n",
    "            img_slices, \n",
    "            torch.nn.functional.one_hot(seg_slices, 3), \n",
    "            color_map, n_per_row=10, alpha=alpha\n",
    "        )\n",
    "        display(pil_ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CrossMoDa ceT1 images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 418/418 [06:56<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal image and label numbers: True\n",
      "Image shape: torch.Size([209, 96, 96, 60]), mean.: 0.00, std.: 1.00\n",
      "Label shape: torch.Size([209, 96, 96, 60]), max.: 2\n",
      "Data import finished. Elapsed time: 419.0 s\n",
      "Loading CrossMoDa validation images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:13<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal image and label numbers: True\n",
      "Image shape: torch.Size([19, 96, 96, 60]), mean.: -0.00, std.: 1.00\n",
      "Label shape: torch.Size([19, 96, 96, 60]), max.: 2\n",
      "Data import finished. Elapsed time: 13.3 s\n",
      "Loading CrossMoDa hrT2 images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [01:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal image and label numbers: True\n",
      "Image shape: torch.Size([60, 96, 96, 60]), mean.: 0.00, std.: 1.00\n",
      "Label shape: torch.Size([60, 96, 96, 60]), max.: 2\n",
      "Data import finished. Elapsed time: 61.2 s\n"
     ]
    }
   ],
   "source": [
    "training_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"source\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "validation_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "target_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = torch.utils.data.Subset(training_dataset,range(2))\n",
    "for sample_idx, (img, seg) in enumerate(train_subset):\n",
    "    print(f\"Sample {sample_idx}:\")\n",
    "    img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "    seg_slices = seg.permute(2,0,1)\n",
    "    print(\"With ground-truth overlay\")\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "\n",
    "    print(\"W/o ground-truth overlay\")\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, seg in validation_dataset:\n",
    "    img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "    seg_slices = seg.permute(2,0,1)\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "    # display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, seg = target_dataset[30]\n",
    "img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "seg_slices = seg.permute(2,0,1)\n",
    "display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "684c5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentAffine(img_in, seg_in, strength=0.05):\n",
    "    \"\"\"\n",
    "    3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: img_in batch (torch.cuda.FloatTensor), seg_in batch (torch.cuda.LongTensor)\n",
    "    :return: augmented BxCxTxHxW image batch (torch.cuda.FloatTensor), augmented BxTxHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    B,C,D,H,W = img_in.size()\n",
    "    affine_matrix = (torch.eye(3,4).unsqueeze(0) + torch.randn(B, 3, 4) * strength).to(img_in.device)\n",
    "\n",
    "    meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)), align_corners=False)\n",
    "\n",
    "    img_out = F.grid_sample(img_in, meshgrid,padding_mode='border')\n",
    "    seg_out = F.grid_sample(seg_in.float(), meshgrid, mode='nearest')\n",
    "\n",
    "    return img_out, seg_out\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(img_in,strength=0.05):\n",
    "    return img_in + strength*torch.randn_like(img_in)\n",
    "\n",
    "\n",
    "    \n",
    "def pad_center_plane(img,size):\n",
    "    s0,s1,s2 = img.size()\n",
    "    i0 = (size[0]-s0)//2\n",
    "    i1 = (size[1]-s1)//2\n",
    "    i2 = 0\n",
    "    pd = (i2,size[2]-s2-i2,i1,size[1]-s1-i1,i0,size[0]-s0-i0)\n",
    "    #print('pad',pd)\n",
    "    img = F.pad(img, pd, \"constant\", 0)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def crop_center_plane(img, size):\n",
    "    s0,s1,s2 = img.size()\n",
    "    i0 = (s0-size[0])//2\n",
    "    i1 = (s1-size[1])//2\n",
    "    i2 = 0\n",
    "    img = img[i0:i0+size[0],i1:i1+size[1],i2:i2+size[2]]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c902fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL(training_dataset, epochs=500, batch_size=1, fold_splits=3, \n",
    "    update_epx = 50, use_mind = True, do_plot=False, debug=False):\n",
    "    if use_mind:\n",
    "        C =12\n",
    "    else:\n",
    "        C = 1\n",
    "    _, all_segs = training_dataset.get_data()\n",
    "\n",
    "    num_class = int(torch.max(all_segs).item()+1)\n",
    "    class_weight = torch.sqrt(1.0/(torch.bincount(all_segs.long().view(-1)).float()))\n",
    "    class_weight = class_weight/class_weight.mean()\n",
    "    class_weight[0] = 0.15\n",
    "    class_weight = class_weight.cuda()\n",
    "    print('inv sqrt class_weight', class_weight)\n",
    "\n",
    "    kf = KFold(n_splits=3)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "    train_idx, val_idx = next(kf.split(training_dataset)) # Only uses one fold\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=False, sampler=train_subsampler)\n",
    "    val_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=False, sampler=val_subsampler)\n",
    "\n",
    "    backbone, aspp, head = create_model(output_classes=num_class,input_channels=C)\n",
    "    optimizer = torch.optim.Adam(list(backbone.parameters())+list(aspp.parameters())+list(head.parameters()),lr=0.001)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(class_weight)\n",
    "    scaler = amp.GradScaler()\n",
    "    backbone.cuda() \n",
    "    backbone.train()\n",
    "    aspp.cuda() \n",
    "    aspp.train()\n",
    "    head.cuda() \n",
    "    head.train()\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epx in range(epochs):\n",
    "        backbone.train()\n",
    "        aspp.train()\n",
    "        head.train()\n",
    "        optimizer.zero_grad()\n",
    "        b_img, b_seg = next(iter(train_dataloader))\n",
    "        \n",
    "        b_img = b_img.unsqueeze(1)\n",
    "        b_seg = b_seg.unsqueeze(1)\n",
    "\n",
    "        b_img, b_seg = b_img.float().cuda(), b_seg.float().cuda()\n",
    "\n",
    "        b_img, b_seg = augmentAffine(b_img, b_seg, strength=0.1)\n",
    "        b_img = augmentNoise(b_img, strength=0.02)\n",
    "        \n",
    "        if use_mind:\n",
    "            b_img = mindssc(b_img)\n",
    "\n",
    "        b_interpolated_seg = F.interpolate(b_seg, scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "\n",
    "        b_img.requires_grad = True\n",
    "        #img_mr.requires_grad = True\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,b_img,checkpointing=True)\n",
    "            loss = criterion(output_j, b_interpolated_seg)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        if epx%update_epx==update_epx-1 or epx == 0:\n",
    "            dice = dice3d(\n",
    "                torch.nn.functional.one_hot(output_j.argmax(1), 3),\n",
    "                torch.nn.functional.one_hot(b_interpolated_seg, 3), one_hot_torch_style=True\n",
    "            )\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                backbone.eval()\n",
    "                aspp.eval()\n",
    "                head.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    b_val_img, b_val_seg = next(iter(val_dataloader))\n",
    "                    b_val_img, b_val_seg = (\n",
    "                        b_val_img.unsqueeze(1).float().cuda(), \n",
    "                        b_val_seg.unsqueeze(1).float().cuda()\n",
    "                    )\n",
    "                    if do_plot:\n",
    "                        print(\"Show val img/lbl\")\n",
    "                        val_img_slices = b_val_img.detach().squeeze(0).permute(3,0,1,2)\n",
    "                        val_seg_slices = b_val_seg.detach().squeeze(0).squeeze(0).permute(2,0,1).to(dtype=torch.int64)\n",
    "                        display_nonempty_seg_slices(val_img_slices, val_seg_slices)\n",
    "\n",
    "                    if use_mind:\n",
    "                        b_val_img = mindssc(b_val_img)\n",
    "\n",
    "                    b_interpolated_val_seg = F.interpolate(b_val_seg, scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "                    output_val = apply_model(backbone,aspp,head,b_val_img,checkpointing=False)\n",
    "                \n",
    "                    val_dice = dice3d(\n",
    "                        torch.nn.functional.one_hot(output_val.argmax(1), 3),\n",
    "                        torch.nn.functional.one_hot(b_interpolated_val_seg, 3), one_hot_torch_style=True\n",
    "                    )\n",
    "\n",
    "                    if do_plot:\n",
    "                        print(\"Show val lbl/prediction\")\n",
    "                        pred_seg_slices = output_val.argmax(1).squeeze(0).permute(2,0,1)\n",
    "                        pred_seg_slices = F.upsample_nearest(pred_seg_slices.unsqueeze(0).unsqueeze(0).float(), scale_factor=2).squeeze(0).squeeze(0).long()\n",
    "                        display_nonempty_seg_slices(val_seg_slices.unsqueeze(1), pred_seg_slices)\n",
    "\n",
    "            dice_mean_no_bg = round(dice.mean(dim=0)[1:].mean().item(),4)\n",
    "            val_dice_mean_no_bg = round(val_dice.mean(dim=0)[1:].mean().item(),4)\n",
    "            print(\n",
    "                'epx',epx,round(time.time()-t0,2),'s',\n",
    "                'loss',round(loss.item(),6),\n",
    "                'dice_tensor',dice, \n",
    "                'dice mean (nobg)', dice_mean_no_bg,\n",
    "                'val_dice_mean (nobg)', val_dice_mean_no_bg\n",
    "            )\n",
    "\n",
    "            wandb.log({'losses/loss': loss}, step=epx)\n",
    "            wandb.log({'scores/dice_tensor': dice}, step=epx)\n",
    "            wandb.log({'scores/val_dice_tensor': val_dice}, step=epx)\n",
    "            wandb.log({'scores/dice_mean_wo_bg': dice_mean_no_bg}, step=epx)\n",
    "            wandb.log({'scores/val_dice_mean_wo_bg': round(val_dice.mean(dim=0)[1:].mean().item(),4)}, step=epx)\n",
    "        \n",
    "        if debug:\n",
    "            break\n",
    "#    stat_cuda('Visceral training')\n",
    "    backbone.cpu()\n",
    "    aspp.cpu() \n",
    "    head.cpu()\n",
    "\n",
    "    return backbone,aspp,head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(backbone,aspp,head,name):\n",
    "    torch.save(backbone.state_dict(), name + '_backbone.pth')\n",
    "    torch.save(aspp.state_dict(), name + '_aspp.pth')\n",
    "    torch.save(head.state_dict(), name + '_head.pth')\n",
    "    return None\n",
    "\n",
    "def load_model(name,output_classes,use_mind):\n",
    "    if use_mind:\n",
    "        input_channels = 12\n",
    "    else:\n",
    "        input_channels = 1\n",
    "\n",
    "    backbone, aspp, head = create_model(output_classes=output_classes,input_channels=input_channels)\n",
    "    backbone.load_state_dict(torch.load( name + '_backbone.pth'))\n",
    "    aspp.load_state_dict(torch.load(name + '_aspp.pth'))\n",
    "    head.load_state_dict(torch.load(name + '_head.pth'))\n",
    "    return backbone,aspp,head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'epochs': 2000,\n",
    "    'update_every': 50,\n",
    "    'use_mind': True,\n",
    "    'debug': False,\n",
    "    'do_plot': False,\n",
    "    'fold_splits': 3,\n",
    "    'batch_size': 1,\n",
    "    'mdl_save_prefix': 'data/models'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f476a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:14ditxmr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 0... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">silver-valley-3</strong>: <a href=\"https://wandb.ai/rap1ide/curriculum_deeplab/runs/14ditxmr\" target=\"_blank\">https://wandb.ai/rap1ide/curriculum_deeplab/runs/14ditxmr</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211022_110334-14ditxmr/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:14ditxmr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rap1ide/curriculum_deeplab/runs/2lbpkhfg\" target=\"_blank\">fresh-deluge-4</a></strong> to <a href=\"https://wandb.ai/rap1ide/curriculum_deeplab\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv sqrt class_weight tensor([0.1500, 0.6111, 2.3484], device='cuda:0')\n",
      "#CNN layer 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3981: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epx 0 0.31 s loss 1.047148 dice_tensor tensor([[6.4788e-01, 9.9657e-03, 2.8810e-04]]) dice mean (nobg) 0.0051 val_dice_mean (nobg) 0.0194\n",
      "epx 49 13.53 s loss 0.158697 dice_tensor tensor([[0.9986, 0.0000, 0.0000]]) dice mean (nobg) 0.0 val_dice_mean (nobg) 0.0\n",
      "epx 99 26.77 s loss 0.049364 dice_tensor tensor([[0.9994, 0.0000, 0.2759]]) dice mean (nobg) 0.1379 val_dice_mean (nobg) 0.3714\n",
      "epx 149 78.51 s loss 0.081221 dice_tensor tensor([[0.9959, 0.7684, 0.3243]]) dice mean (nobg) 0.5464 val_dice_mean (nobg) 0.046\n",
      "epx 199 90.83 s loss 0.03967 dice_tensor tensor([[0.9958, 0.4218, 0.4000]]) dice mean (nobg) 0.4109 val_dice_mean (nobg) 0.0435\n",
      "epx 249 112.5 s loss 0.020957 dice_tensor tensor([[0.9984, 0.0000, 0.2317]]) dice mean (nobg) 0.1159 val_dice_mean (nobg) 0.1007\n",
      "epx 299 123.25 s loss 0.012848 dice_tensor tensor([[0.9995, 0.0000, 0.4160]]) dice mean (nobg) 0.208 val_dice_mean (nobg) 0.4652\n",
      "epx 349 138.19 s loss 0.020503 dice_tensor tensor([[0.9976, 0.0000, 0.2703]]) dice mean (nobg) 0.1351 val_dice_mean (nobg) 0.1727\n",
      "epx 399 149.21 s loss 0.00826 dice_tensor tensor([[0.9998, 0.0000, 0.5797]]) dice mean (nobg) 0.2899 val_dice_mean (nobg) 0.3591\n",
      "epx 449 202.61 s loss 0.011934 dice_tensor tensor([[0.9988, 0.0000, 0.2597]]) dice mean (nobg) 0.1299 val_dice_mean (nobg) 0.2877\n",
      "epx 499 215.07 s loss 0.018697 dice_tensor tensor([[0.9983, 0.7385, 0.3151]]) dice mean (nobg) 0.5268 val_dice_mean (nobg) 0.1117\n",
      "epx 549 226.57 s loss 0.017791 dice_tensor tensor([[0.9971, 0.3984, 0.2500]]) dice mean (nobg) 0.3242 val_dice_mean (nobg) 0.1059\n",
      "epx 599 237.87 s loss 0.008956 dice_tensor tensor([[0.9994, 0.0000, 0.4593]]) dice mean (nobg) 0.2296 val_dice_mean (nobg) 0.5011\n",
      "epx 649 255.85 s loss 0.005527 dice_tensor tensor([[0.9994, 0.0000, 0.3876]]) dice mean (nobg) 0.1938 val_dice_mean (nobg) 0.2892\n",
      "epx 699 277.19 s loss 0.00811 dice_tensor tensor([[0.9988, 0.0000, 0.3704]]) dice mean (nobg) 0.1852 val_dice_mean (nobg) 0.1705\n",
      "epx 749 298.44 s loss 0.024771 dice_tensor tensor([[0.9946, 0.7392, 0.3363]]) dice mean (nobg) 0.5378 val_dice_mean (nobg) 0.1839\n",
      "epx 799 315.96 s loss 0.011794 dice_tensor tensor([[0.9988, 0.7248, 0.2887]]) dice mean (nobg) 0.5067 val_dice_mean (nobg) 0.0746\n",
      "epx 849 325.3 s loss 0.015694 dice_tensor tensor([[0.9996, 0.0000, 0.5128]]) dice mean (nobg) 0.2564 val_dice_mean (nobg) 0.3014\n",
      "epx 899 344.97 s loss 0.018448 dice_tensor tensor([[0.9965, 0.8317, 0.2560]]) dice mean (nobg) 0.5439 val_dice_mean (nobg) 0.5134\n",
      "epx 949 385.46 s loss 0.024798 dice_tensor tensor([[0.9984, 0.6038, 0.4510]]) dice mean (nobg) 0.5274 val_dice_mean (nobg) 0.1176\n",
      "epx 999 400.2 s loss 0.004695 dice_tensor tensor([[0.9995, 0.0000, 0.3673]]) dice mean (nobg) 0.1837 val_dice_mean (nobg) 0.4967\n",
      "epx 1049 432.86 s loss 0.005712 dice_tensor tensor([[0.9991, 0.7703, 0.4632]]) dice mean (nobg) 0.6167 val_dice_mean (nobg) 0.188\n",
      "epx 1099 451.58 s loss 0.015811 dice_tensor tensor([[0.9978, 0.8695, 0.3529]]) dice mean (nobg) 0.6112 val_dice_mean (nobg) 0.5163\n",
      "epx 1149 489.4 s loss 0.003991 dice_tensor tensor([[0.9994, 0.0000, 0.3846]]) dice mean (nobg) 0.1923 val_dice_mean (nobg) 0.186\n",
      "epx 1199 507.78 s loss 0.003555 dice_tensor tensor([[0.9997, 0.0000, 0.5814]]) dice mean (nobg) 0.2907 val_dice_mean (nobg) 0.2877\n",
      "epx 1249 521.33 s loss 0.003334 dice_tensor tensor([[0.9998, 0.0000, 0.5152]]) dice mean (nobg) 0.2576 val_dice_mean (nobg) 0.5287\n",
      "epx 1299 532.03 s loss 0.010935 dice_tensor tensor([[0.9988, 0.5575, 0.4179]]) dice mean (nobg) 0.4877 val_dice_mean (nobg) 0.6548\n",
      "epx 1349 552.35 s loss 0.023562 dice_tensor tensor([[0.9955, 0.7300, 0.3333]]) dice mean (nobg) 0.5317 val_dice_mean (nobg) 0.1698\n",
      "epx 1399 590.26 s loss 0.006955 dice_tensor tensor([[0.9988, 0.6235, 0.3818]]) dice mean (nobg) 0.5027 val_dice_mean (nobg) 0.6543\n",
      "epx 1449 600.09 s loss 0.050036 dice_tensor tensor([[0.9969, 0.8529, 0.4384]]) dice mean (nobg) 0.6456 val_dice_mean (nobg) 0.5656\n",
      "epx 1499 612.81 s loss 0.005772 dice_tensor tensor([[0.9992, 0.7978, 0.3333]]) dice mean (nobg) 0.5656 val_dice_mean (nobg) 0.7174\n",
      "epx 1549 622.67 s loss 0.00501 dice_tensor tensor([[0.9998, 0.0000, 0.6462]]) dice mean (nobg) 0.3231 val_dice_mean (nobg) 0.6456\n",
      "epx 1599 633.29 s loss 0.006715 dice_tensor tensor([[0.9991, 0.7173, 0.3404]]) dice mean (nobg) 0.5289 val_dice_mean (nobg) 0.6598\n",
      "epx 1649 644.3 s loss 0.003065 dice_tensor tensor([[0.9998, 0.0000, 0.5455]]) dice mean (nobg) 0.2727 val_dice_mean (nobg) 0.1798\n",
      "epx 1699 655.78 s loss 0.003026 dice_tensor tensor([[0.9995, 0.0000, 0.4522]]) dice mean (nobg) 0.2261 val_dice_mean (nobg) 0.1428\n",
      "epx 1749 677.73 s loss 0.003171 dice_tensor tensor([[0.9997, 0.0000, 0.3478]]) dice mean (nobg) 0.1739 val_dice_mean (nobg) 0.6078\n",
      "epx 1799 690.07 s loss 0.002582 dice_tensor tensor([[0.9997, 0.0000, 0.5909]]) dice mean (nobg) 0.2955 val_dice_mean (nobg) 0.6123\n",
      "epx 1849 702.87 s loss 0.004389 dice_tensor tensor([[0.9993, 0.0000, 0.3486]]) dice mean (nobg) 0.1743 val_dice_mean (nobg) 0.2105\n",
      "epx 1899 721.34 s loss 0.006882 dice_tensor tensor([[0.9999, 0.0000, 0.6364]]) dice mean (nobg) 0.3182 val_dice_mean (nobg) 0.2018\n",
      "epx 1949 735.64 s loss 0.004597 dice_tensor tensor([[0.9997, 0.0000, 0.4722]]) dice mean (nobg) 0.2361 val_dice_mean (nobg) 0.1681\n",
      "epx 1999 749.68 s loss 0.002405 dice_tensor tensor([[0.9997, 0.0000, 0.5000]]) dice mean (nobg) 0.25 val_dice_mean (nobg) 0.6198\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"curriculum_deeplab\", group=\"training\", config=config_dict,\n",
    "    settings=wandb.Settings(start_method=\"thread\") \n",
    "    # mode=\"disabled\"\n",
    ")\n",
    "config = wandb.config\n",
    "\n",
    "\n",
    "# imgs = torch.cat((imgs_train_source,imgs_train_target),dim=0)\n",
    "# label = torch.cat((labels_train_source,labels_train_target),dim=0)\n",
    "backbone,aspp,head = train_DL(training_dataset, epochs=config.epochs, batch_size=config.batch_size,\n",
    "    fold_splits=config.fold_splits, update_epx=config.update_every, use_mind=config.use_mind, \n",
    "    debug=config.debug, do_plot=config.do_plot)\n",
    "\n",
    "save_model(backbone, aspp, head,config.mdl_save_prefix)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_DL(\n",
    "    backbone, aspp, head, \n",
    "    inf_dataset, use_mind = True, do_plot=True, debug=False):\n",
    "\n",
    "    backbone.eval()\n",
    "    aspp.eval()\n",
    "    head.eval()\n",
    "    \n",
    "    all_dices = []\n",
    "\n",
    "    for sample_idx, (img, seg) in enumerate(inf_dataset):\n",
    "\n",
    "        img, seg = (\n",
    "            img.unsqueeze(0).unsqueeze(0).float(), \n",
    "            seg.unsqueeze(0).unsqueeze(0).float()\n",
    "        )\n",
    "        \n",
    "        if use_mind:\n",
    "            img = mindssc(img)\n",
    "\n",
    "        if do_plot:\n",
    "            img_slices = img[0:1].permute(0,1,4,2,3).squeeze().unsqueeze(1)\n",
    "            seg_slices = seg[0:1].permute(0,1,4,2,3).squeeze().to(dtype=torch.int64)\n",
    "            display_nonempty_seg_slices(img_slices, seg_slices)\n",
    "            \n",
    "        interpolated_seg = F.interpolate(seg, scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "\n",
    "        img.requires_grad = True\n",
    "        with amp.autocast(enabled=True):\n",
    "            with torch.no_grad():\n",
    "               \n",
    "                interpolated_seg = F.interpolate(seg, scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "                output_val = apply_model(backbone,aspp,head,img,checkpointing=False)\n",
    "\n",
    "                inf_dice = dice3d(\n",
    "                    torch.nn.functional.one_hot(output_val.argmax(1), 3),\n",
    "                    torch.nn.functional.one_hot(interpolated_seg, 3), one_hot_torch_style=True\n",
    "                )\n",
    "            if do_plot:\n",
    "                lbl_slices = seg.detach().squeeze(0).permute(3,0,1,2)\n",
    "                pred_seg_slices = output_val.argmax(1).squeeze(0).permute(2,0,1)\n",
    "                pred_seg_slices = F.upsample_nearest(pred_seg_slices.unsqueeze(0).unsqueeze(0).float(), scale_factor=2).squeeze(0).squeeze(0).long()\n",
    "                display_nonempty_seg_slices(lbl_slices, pred_seg_slices)\n",
    "\n",
    "            mean_no_bg = inf_dice.mean(dim=0)[1:].mean()\n",
    "            all_dices.append(mean_no_bg)\n",
    "            print(\n",
    "                'sample', sample_idx,\n",
    "                'dice_tensor',inf_dice, \n",
    "                'dice mean (nobg)', round(mean_no_bg.item(),4),\n",
    "            )\n",
    "            wandb.log({'scores/dice': mean_no_bg}, step=sample_idx)\n",
    "\n",
    "        if debug:\n",
    "            break\n",
    "    \n",
    "    mean_inf_dice = torch.stack(all_dices).mean()\n",
    "    print(f\"Mean dice over all classes and samples: {mean_inf_dice.item()*100:.2f}%\")\n",
    "    wandb.log({'scores/mean_dice_all_samples': mean_inf_dice}, step=sample_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lur248hn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 0... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>scores/dice</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>scores/dice</td><td>0.72707</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dutiful-elevator-5</strong>: <a href=\"https://wandb.ai/rap1ide/curriculum_deeplab/runs/lur248hn\" target=\"_blank\">https://wandb.ai/rap1ide/curriculum_deeplab/runs/lur248hn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211022_121300-lur248hn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lur248hn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rap1ide/curriculum_deeplab/runs/ziukaghn\" target=\"_blank\">wobbly-darkness-6</a></strong> to <a href=\"https://wandb.ai/rap1ide/curriculum_deeplab\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#CNN layer 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0 dice_tensor tensor([[0.9983, 0.8521, 0.1818]]) dice mean (nobg) 0.517\n",
      "sample 1 dice_tensor tensor([[0.9970, 0.7307, 0.7234]]) dice mean (nobg) 0.7271\n",
      "sample 2 dice_tensor tensor([[0.9980, 0.3852, 0.4186]]) dice mean (nobg) 0.4019\n",
      "sample 3 dice_tensor tensor([[0.9916, 0.5274, 0.3881]]) dice mean (nobg) 0.4578\n",
      "sample 4 dice_tensor tensor([[0.9986, 0.0000, 0.3529]]) dice mean (nobg) 0.1765\n",
      "sample 5 dice_tensor tensor([[0.9963, 0.8717, 0.4727]]) dice mean (nobg) 0.6722\n",
      "sample 6 dice_tensor tensor([[0.9992, 0.0000, 0.5455]]) dice mean (nobg) 0.2727\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"curriculum_deeplab\", group=\"testing\", config=config_dict,\n",
    "    settings=wandb.Settings(start_method=\"thread\") \n",
    "    # mode=\"disabled\"\n",
    ")\n",
    "config = wandb.config\n",
    "\n",
    "backbone, aspp, head = load_model(config.mdl_save_prefix, output_classes=3, use_mind=config.use_mind)\n",
    "inference_DL(backbone, aspp, head, validation_dataset, use_mind = config.use_mind, do_plot=config.do_plot, debug=config.debug)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0439928",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/share/data_sam1/ckruse/image_data/CrossMoDa/target_training/'\n",
    "label_path = '/share/data_supergrover1/weihsbach/tmp/crossmoda_full_set/'\n",
    "plot = False\n",
    "backbone.cuda() \n",
    "aspp.cuda() \n",
    "head.cuda() \n",
    "target_dices = torch.zeros(32)\n",
    "source_dices = torch.zeros(32)\n",
    "for i in range(32):\n",
    "    ind = i+150\n",
    "    nii_img = nib.load(path + 'crossmoda_'+ str(ind) + '_hrT2.nii.gz')\n",
    "    nii_label = nib.load(label_path + 'crossmoda_'+ str(ind) + '_hrT2_Label.nii.gz')\n",
    "    tmp = torch.from_numpy(nii_img.get_fdata())\n",
    "    label = torch.from_numpy(nii_label.get_fdata()).cuda()\n",
    "    org_size = tmp.size()  \n",
    "    size = (192,192,64)\n",
    "    tmp = crop_center_plane(tmp,size)\n",
    "    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "    org_img = torch.from_numpy(nii_img.get_fdata())\n",
    "    \n",
    "    img= tmp.float().cuda()\n",
    "    img = mindssc(img.unsqueeze(0).unsqueeze(0))\n",
    "    with torch.no_grad():\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=False)#\n",
    "    modeled_seg = F.interpolate(output_j,scale_factor=2).argmax(1)\n",
    "    modeled_seg = pad_center_plane(modeled_seg.squeeze(),org_size)\n",
    "    #print(modeled_seg.shape,torch.max(modeled_seg))\n",
    "    connectivity = 18 # only 4,8 (2D) and 26, 18, and 6 (3D) are allowed\n",
    "    np_label = modeled_seg.long().cpu().numpy().astype('int32')\n",
    "    tmp = pad_center_plane(tmp.squeeze(),org_size)\n",
    "    #plot = True\n",
    "    if plot:\n",
    "        slc = 30\n",
    "        i0 = overlaySegment(tmp[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        i1 = overlaySegment(org_img[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        fig,axs = plt.subplots(1, 2,figsize=(18, 9))\n",
    "        axs[0].imshow((i0+i1).cpu().numpy())\n",
    "        axs[1].imshow(i1.cpu().numpy())\n",
    "        plt.show()\n",
    "        fig.set_figwidth(40)\n",
    "        fig.set_figheight(10)\n",
    "    #print(modeled_seg.shape,torch.max(modeled_seg))\n",
    "    target_dices[i] = dice_coeff(modeled_seg,label)\n",
    "    print(f'image: crossmoda_{ind}_hrT2.nii.gz, dice: {target_dices[i]*100:0.2f}')\n",
    "    \n",
    "print(f'target dice mean: {target_dices.mean()*100:0.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ae4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/share/data_sam1/ckruse/image_data/CrossMoDa/target_validation/'\n",
    "plot = False\n",
    "save = True\n",
    "backbone.cuda() \n",
    "aspp.cuda() \n",
    "head.cuda() \n",
    "for i in range(32):\n",
    "    ind = i+211\n",
    "    nii_img = nib.load(path + 'crossmoda_'+ str(ind) + '_hrT2.nii.gz')\n",
    "    img_affine = nii_img.affine\n",
    "    tmp = torch.from_numpy(nii_img.get_fdata())\n",
    "    org_img = torch.from_numpy(nii_img.get_fdata())\n",
    "    org_size = tmp.size()  \n",
    "    size = (192,192,64)\n",
    "    tmp = crop_center_plane(tmp,size)\n",
    "    #print('red. shape',tmp.shape)\n",
    "    #tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size)\n",
    "    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "        \n",
    "    img= tmp.float().cuda()\n",
    "    #print(img.shape)\n",
    "    img = mindssc(img.unsqueeze(0).unsqueeze(0))\n",
    "    with torch.no_grad():\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=False)#\n",
    "    modeled_seg = F.interpolate(output_j,scale_factor=2).argmax(1)\n",
    "    modeled_seg = pad_center_plane(modeled_seg.squeeze(),org_size)\n",
    "    tmp = pad_center_plane(tmp.squeeze(),org_size)\n",
    "    #print('org shape',tmp.shape)\n",
    "    if plot:\n",
    "        slc = 30\n",
    "        i0 = overlaySegment(tmp[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        i1 = overlaySegment(org_img[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        fig,axs = plt.subplots(1, 2,figsize=(18, 9))\n",
    "        axs[0].imshow((i0+i1).cpu().numpy())\n",
    "        axs[1].imshow(i1.cpu().numpy())\n",
    "        plt.show()\n",
    "        fig.set_figwidth(40)\n",
    "        fig.set_figheight(10)\n",
    "    if save:\n",
    "        label_nii = nib.Nifti1Image(modeled_seg.float().squeeze().cpu().numpy(), img_affine)\n",
    "        nib.save(label_nii, 'Deeplab_validation/crossmoda_'+ str(ind) + '_Label.nii.gz')  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca07731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm Deeplab_validation.zip\n",
    "#!zip -r Deeplab_validation_half_res_adapted_model_target_train.zip Deeplab_validation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
