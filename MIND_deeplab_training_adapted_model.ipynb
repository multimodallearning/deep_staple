{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fc395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -3 -4\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import get_overlay_grid\n",
    "from curriculum_deeplab.mindssc import mindssc, ml_data_parameters_utils\n",
    "import wandb\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca60d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            # nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        # x = F.adaptive_avg_pool3d(x, (1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')  # , align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = [nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())]\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    " \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "# Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "def create_model(output_classes: int = 14,input_channels: int = 1):\n",
    "    # in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "    in_channels = torch.Tensor([input_channels, 24, 24, 32, 48, 48, 48, 64]).long()\n",
    "    mid_channels = torch.Tensor([64, 128, 192, 192, 256, 256, 256, 384]).long()\n",
    "    out_channels = torch.Tensor([24, 24, 32, 48, 48, 48, 64, 64]).long()\n",
    "    mid_stride = torch.Tensor([1, 1, 1, 2, 1, 1, 1, 1])\n",
    "    net = [nn.Identity()]\n",
    "    for i in range(8):\n",
    "        inc = int(in_channels[i])\n",
    "        midc = int(mid_channels[i])\n",
    "        outc = int(out_channels[i])\n",
    "        strd = int(mid_stride[i])\n",
    "        layer = nn.Sequential(nn.Conv3d(inc, midc, 1, bias=False), nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, midc, 3, stride=strd, padding=1, bias=False, groups=midc),\n",
    "                              nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, outc, 1, bias=False), nn.BatchNorm3d(outc))\n",
    "        if i == 0:\n",
    "            layer[0] = nn.Conv3d(inc, midc, 3, padding=1, stride=2, bias=False)\n",
    "        if (inc == outc) & (strd == 1):\n",
    "            net.append(ResBlock(layer))\n",
    "        else:\n",
    "            net.append(layer)\n",
    "\n",
    "    backbone = nn.Sequential(*net)\n",
    "\n",
    "    count = 0\n",
    "    # weight initialization\n",
    "    for m in backbone.modules():\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            count += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    print('#CNN layer', count)\n",
    "    # complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "    # newer model (one more stride, no groups in head)\n",
    "    aspp = ASPP(64, (2, 4, 8, 16, 32), 128)\n",
    "    head = nn.Sequential(nn.Conv3d(128 + 24, 64, 1, padding=0, groups=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, 64, 3, groups=1, padding=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, output_classes, 1))\n",
    "    return backbone, aspp, head\n",
    "\n",
    "\n",
    "def apply_model(backbone, aspp, head, img, checkpointing=True, return_intermediate=False):\n",
    "    if checkpointing:\n",
    "        x1 = checkpoint(backbone[:3], img)\n",
    "        x2 = checkpoint(backbone[3:], x1)\n",
    "        y = checkpoint(aspp, x2)\n",
    "        y1 = torch.cat((x1[0:1], F.interpolate(y[0:1], scale_factor=2)), 1)\n",
    "        output_j = checkpoint(head, y1)\n",
    "    else:\n",
    "        x1 = backbone[:3](img)\n",
    "        x2 = backbone[3:](x1)\n",
    "        y = aspp(x2)\n",
    "        y1 = torch.cat((x1[0:1], F.interpolate(y[0:1], scale_factor=2)), 1)\n",
    "        output_j = head(y1)\n",
    "    if return_intermediate:\n",
    "        return y1,output_j\n",
    "    else:\n",
    "        return output_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class CrossMoDa_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        ensure_labeled_pairs=True, use_additional_data=False, resample = True,\n",
    "        size:tuple = (96,96,60), normalize:bool = True):\n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                ensure_labeled_pairs (bool): Only images with corresponding labels will be loaded (default: True)\n",
    "                \n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "\n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "        path = base_dir + state_dir\n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "        if domain.lower() ==\"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "        if ensure_labeled_pairs:\n",
    "            def get_bare_basename(_path):\n",
    "                return str(Path(_path.replace('.nii.gz', '')).stem)\n",
    "\n",
    "            labeled_files = [\n",
    "                _path for _path in files \\\n",
    "                    if '_Label' in get_bare_basename(_path) \\\n",
    "                    or get_bare_basename(_path)+'_Label' in [get_bare_basename(_path) for _path in files]\n",
    "            ]\n",
    "            files = labeled_files\n",
    "            \n",
    "        #initialize variables\n",
    "        self.imgs = torch.zeros(0,size[0],size[1],size[2])\n",
    "        self.labels = torch.zeros(0,size[0],size[1],size[2])\n",
    "        self.img_nums = []\n",
    "        self.label_nums = []\n",
    "        #load data\n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "\n",
    "        for i,f in enumerate(tqdm(files)):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in f:\n",
    "                self.label_nums.append(int(re.findall(r'\\d+', os.path.basename(f))[0]))\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                self.labels = torch.cat((self.labels,tmp.unsqueeze(0)),dim=0)\n",
    "            elif domain in f:\n",
    "                self.img_nums.append(int(re.findall(r'\\d+', os.path.basename(f))[0]))\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                self.imgs = torch.cat((self.imgs,tmp.unsqueeze(0)),dim=0)\n",
    "        self.labels = self.labels.long()\n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(self.img_nums==self.label_nums))\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(self.imgs.shape,self.imgs.mean(),self.imgs.std()))\n",
    "        print(\"Label shape: {}, max.: {}\".format(self.labels.shape,torch.max(self.labels)))\n",
    "        print(\"Data import finished. Elapsed time: {:.1f} s\".format(time.time()-t0 ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.imgs.size(0))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label.long()\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.imgs,self.labels\n",
    "\n",
    "    def get_image_numbers(self):\n",
    "        return self.img_nums\n",
    "\n",
    "    def get_label_numbers(self):\n",
    "        return self.label_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training routine\n",
    "def display_nonempty_seg_slices(img_slices, seg_slices, alpha=.5):\n",
    "    color_map = {\n",
    "        0: None, \n",
    "        1: (255,0,0), #ONEHOT id and RGB color\n",
    "        2: (0,255,0)\n",
    "    }\n",
    "\n",
    "    idx_dept_with_segs, *_ = torch.nonzero(seg_slices > 0, as_tuple=True)\n",
    "    \n",
    "    if idx_dept_with_segs.nelement() > 0:\n",
    "        idx_dept_with_segs = idx_dept_with_segs.unique()\n",
    "\n",
    "        img_slices = img_slices[idx_dept_with_segs]\n",
    "        seg_slices = seg_slices[idx_dept_with_segs]\n",
    "        \n",
    "        pil_ov, _ = get_overlay_grid(\n",
    "            img_slices, \n",
    "            torch.nn.functional.one_hot(seg_slices, 3), \n",
    "            color_map, n_per_row=10, alpha=alpha\n",
    "        )\n",
    "        display(pil_ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"source\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "validation_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "target_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = torch.utils.data.Subset(training_dataset,range(2))\n",
    "for sample_idx, (img, seg) in enumerate(train_subset):\n",
    "    print(f\"Sample {sample_idx}:\")\n",
    "    img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "    seg_slices = seg.permute(2,0,1)\n",
    "    print(\"With ground-truth overlay\")\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "\n",
    "    print(\"W/o ground-truth overlay\")\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, seg in validation_dataset:\n",
    "    img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "    seg_slices = seg.permute(2,0,1)\n",
    "    display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "    # display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, seg = target_dataset[30]\n",
    "img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "seg_slices = seg.permute(2,0,1)\n",
    "display_nonempty_seg_slices(img_slices, seg_slices, alpha=.3)\n",
    "display_nonempty_seg_slices(img_slices, seg_slices, alpha=.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentAffine(img_in, seg_in, strength=0.05):\n",
    "    \"\"\"\n",
    "    3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: img_in batch (torch.cuda.FloatTensor), seg_in batch (torch.cuda.LongTensor)\n",
    "    :return: augmented BxCxTxHxW image batch (torch.cuda.FloatTensor), augmented BxTxHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    B,C,D,H,W = img_in.size()\n",
    "    affine_matrix = (torch.eye(3,4).unsqueeze(0) + torch.randn(B, 3, 4) * strength).to(img_in.device)\n",
    "\n",
    "    meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)), align_corners=False)\n",
    "\n",
    "    img_out = F.grid_sample(img_in, meshgrid,padding_mode='border')\n",
    "    seg_out = F.grid_sample(seg_in.float(), meshgrid, mode='nearest')\n",
    "\n",
    "    return img_out, seg_out\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(img_in,strength=0.05):\n",
    "    return img_in + strength*torch.randn_like(img_in)\n",
    "\n",
    "\n",
    "    \n",
    "def pad_center_plane(img,size):\n",
    "    s0,s1,s2 = img.size()\n",
    "    i0 = (size[0]-s0)//2\n",
    "    i1 = (size[1]-s1)//2\n",
    "    i2 = 0\n",
    "    pd = (i2,size[2]-s2-i2,i1,size[1]-s1-i1,i0,size[0]-s0-i0)\n",
    "    #print('pad',pd)\n",
    "    img = F.pad(img, pd, \"constant\", 0)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def crop_center_plane(img, size):\n",
    "    s0,s1,s2 = img.size()\n",
    "    i0 = (s0-size[0])//2\n",
    "    i1 = (s1-size[1])//2\n",
    "    i2 = 0\n",
    "    img = img[i0:i0+size[0],i1:i1+size[1],i2:i2+size[2]]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c902fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL(training_dataset, epochs=500, batch_size=1, fold_splits=3,\n",
    "    lr=0.001, \n",
    "    init_class_param=1.0, learn_class_parameters=False, lr_class_param=0.1,\n",
    "    init_inst_param=1.0, learn_inst_parameters=False, lr_inst_param=0.1,\n",
    "    wd_inst_param=0., wd_class_param=0.,\n",
    "    skip_clamp_data_param=False, clamp_inst_sigma_config={}, clamp_cls_sigma_config={},\n",
    "    log_every = 50, use_mind = True, do_plot=False, debug=False):\n",
    "    \n",
    "    if use_mind:\n",
    "        C =12\n",
    "    else:\n",
    "        C = 1\n",
    "    _, all_segs = training_dataset.get_data()\n",
    "\n",
    "    num_class = int(torch.max(all_segs).item()+1)\n",
    "    class_weight = torch.sqrt(1.0/(torch.bincount(all_segs.long().view(-1)).float()))\n",
    "    class_weight = class_weight/class_weight.mean()\n",
    "    class_weight[0] = 0.15\n",
    "    class_weight = class_weight.cuda()\n",
    "    print('inv sqrt class_weight', class_weight)\n",
    "\n",
    "    kf = KFold(n_splits=fold_splits)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "    train_idx, val_idx = next(kf.split(training_dataset)) # Only uses one fold\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=False, sampler=train_subsampler)\n",
    "    val_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=False, sampler=val_subsampler)\n",
    "\n",
    "    backbone, aspp, head = create_model(output_classes=num_class,input_channels=C)\n",
    "    optimizer = torch.optim.Adam(list(backbone.parameters())+list(aspp.parameters())+list(head.parameters()),lr=lr)\n",
    "\n",
    "    # Initialize class and instance based temperature\n",
    "    (class_parameters, inst_parameters, optimizer_class_param, optimizer_inst_param) = \\\n",
    "        ml_data_parameters_utils.get_class_inst_data_params_n_optimizer(\n",
    "            init_class_param, learn_class_parameters, lr_class_param,\n",
    "            init_inst_param, learn_inst_parameters, lr_inst_param,\n",
    "            nr_classes=3,\n",
    "            nr_instances=len(train_dataloader.dataset),\n",
    "            device='cuda'\n",
    "        )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(class_weight)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    top1 = ml_data_parameters_utils.AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = ml_data_parameters_utils.AverageMeter('Acc@5', ':6.2f')\n",
    "\n",
    "    backbone.cuda() \n",
    "    backbone.train()\n",
    "    aspp.cuda() \n",
    "    aspp.train()\n",
    "    head.cuda() \n",
    "    head.train()\n",
    "    t0 = time.time()\n",
    "\n",
    "    # for epx, (inputs, target, index_dataset) in enumerate(train_loader):\n",
    "    for epx in range(epochs):\n",
    "        backbone.train()\n",
    "        aspp.train()\n",
    "        head.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if learn_class_parameters:\n",
    "            optimizer_class_param.zero_grad()\n",
    "        if learn_inst_parameters:\n",
    "            optimizer_inst_param.zero_grad()\n",
    "\n",
    "        # Load datta\n",
    "        b_img, b_seg, index_dataset = next(iter(train_dataloader))\n",
    "        \n",
    "        b_img = b_img.unsqueeze(1)\n",
    "        b_seg = b_seg.unsqueeze(1)\n",
    "\n",
    "        b_img, b_seg = b_img.float().cuda(), b_seg.float().cuda()\n",
    "\n",
    "        b_img, b_seg = augmentAffine(b_img, b_seg, strength=0.1)\n",
    "        b_img = augmentNoise(b_img, strength=0.02)\n",
    "        \n",
    "        if use_mind:\n",
    "            b_img = mindssc(b_img)\n",
    "\n",
    "        b_interpolated_seg = F.interpolate(b_seg, scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "\n",
    "        b_img.requires_grad = True\n",
    "        #img_mr.requires_grad = True\n",
    "        with amp.autocast(enabled=True):\n",
    "            logits = apply_model(backbone, aspp, head, b_img, checkpointing=True)\n",
    "\n",
    "            if learn_class_parameters or learn_inst_parameters:\n",
    "                # Compute data parameters for instances in the minibatch\n",
    "                class_parameter_minibatch = class_parameters[b_interpolated_seg]\n",
    "                inst_parameter_minibatch = inst_parameters[index_dataset]\n",
    "                data_parameter_minibatch = ml_data_parameters_utils.get_data_param_for_minibatch(\n",
    "                                                learn_class_parameters=learn_class_parameters, \n",
    "                                                learn_inst_parameters=learn_inst_parameters,\n",
    "                                                class_param_minibatch=class_parameter_minibatch,\n",
    "                                                inst_param_minibatch=inst_parameter_minibatch)\n",
    "\n",
    "                # Compute logits scaled by data parameters\n",
    "                logits = logits / data_parameter_minibatch\n",
    "\n",
    "            loss = criterion(logits, b_interpolated_seg)\n",
    "            # Apply weight decay on data parameters\n",
    "            if learn_class_parameters or learn_inst_parameters:\n",
    "                loss = ml_data_parameters_utils.apply_weight_decay_data_parameters(\n",
    "                    learn_inst_parameters, wd_inst_param,\n",
    "                    learn_class_parameters, wd_class_param,\n",
    "                    loss,\n",
    "                    class_parameter_minibatch=class_parameter_minibatch,\n",
    "                    inst_parameter_minibatch=inst_parameter_minibatch)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Step for data parameters TODO: Use grad scaling\n",
    "        if learn_class_parameters:\n",
    "            optimizer_class_param.step()\n",
    "        if learn_inst_parameters:\n",
    "            optimizer_inst_param.step()\n",
    "\n",
    "        scaler.update()\n",
    "\n",
    "        # TODO: Check below\n",
    "        # Clamp class and instance level parameters within certain bounds\n",
    "        if learn_class_parameters or learn_inst_parameters:\n",
    "            ml_data_parameters_utils.clamp_data_parameters(\n",
    "                skip_clamp_data_param, learn_inst_parameters, learn_class_parameters,\n",
    "                class_parameters, inst_parameters,\n",
    "                clamp_inst_sigma_config, clamp_cls_sigma_config)\n",
    "\n",
    "        # # Measure accuracy and record loss\n",
    "        acc1, acc5 = ml_data_parameters_utils.compute_topk_accuracy(logits, b_interpolated_seg, topk=(1, 5))\n",
    "        top1.update(acc1[0], b_img.size(0))\n",
    "        top5.update(acc5[0], b_img.size(0))\n",
    "        \n",
    "        if epx % log_every == 0:\n",
    "            dice = dice3d(\n",
    "                torch.nn.functional.one_hot(output_j.argmax(1), 3),\n",
    "                torch.nn.functional.one_hot(b_interpolated_seg, 3), one_hot_torch_style=True\n",
    "            )\n",
    "            # Log data parameters\n",
    "            ml_data_parameters_utils.log_intermediate_iteration_stats(\n",
    "                epx,\n",
    "                learn_class_parameters, learn_inst_parameters,\n",
    "                class_parameters, inst_parameters, top1, top5)\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                backbone.eval()\n",
    "                aspp.eval()\n",
    "                head.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    b_val_img, b_val_seg = next(iter(val_dataloader))\n",
    "                    b_val_img, b_val_seg = (\n",
    "                        b_val_img.unsqueeze(1).float().cuda(), \n",
    "                        b_val_seg.unsqueeze(1).float().cuda()\n",
    "                    )\n",
    "                    if do_plot:\n",
    "                        print(\"Show val img/lbl\")\n",
    "                        val_img_slices = b_val_img.detach().squeeze(0).permute(3,0,1,2)\n",
    "                        val_seg_slices = b_val_seg.detach().squeeze(0).squeeze(0).permute(2,0,1).to(dtype=torch.int64)\n",
    "                        display_nonempty_seg_slices(val_img_slices, val_seg_slices)\n",
    "\n",
    "                    if use_mind:\n",
    "                        b_val_img = mindssc(b_val_img)\n",
    "\n",
    "                    b_interpolated_val_seg = F.interpolate(b_val_seg, scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "                    output_val = apply_model(backbone,aspp,head,b_val_img,checkpointing=False)\n",
    "                \n",
    "                    val_dice = dice3d(\n",
    "                        torch.nn.functional.one_hot(output_val.argmax(1), 3),\n",
    "                        torch.nn.functional.one_hot(b_interpolated_val_seg, 3), one_hot_torch_style=True\n",
    "                    )\n",
    "\n",
    "                    if do_plot:\n",
    "                        print(\"Show val lbl/prediction\")\n",
    "                        pred_seg_slices = output_val.argmax(1).squeeze(0).permute(2,0,1)\n",
    "                        pred_seg_slices = F.upsample_nearest(pred_seg_slices.unsqueeze(0).unsqueeze(0).float(), scale_factor=2).squeeze(0).squeeze(0).long()\n",
    "                        display_nonempty_seg_slices(val_seg_slices.unsqueeze(1), pred_seg_slices)\n",
    "\n",
    "            dice_mean_no_bg = round(dice.mean(dim=0)[1:].mean().item(),4)\n",
    "            val_dice_mean_no_bg = round(val_dice.mean(dim=0)[1:].mean().item(),4)\n",
    "            \n",
    "            print(\n",
    "                'epx', epx,round(time.time()-t0,2),'s',\n",
    "                'loss', round(loss.item(),6),\n",
    "                'dice_tensor', dice, \n",
    "                'dice mean (nobg)', dice_mean_no_bg,\n",
    "                'val_dice_mean (nobg)', val_dice_mean_no_bg\n",
    "            )\n",
    "\n",
    "            wandb.log({'losses/loss': loss}, step=epx)\n",
    "            wandb.log({'scores/dice_tensor': dice}, step=epx)\n",
    "            wandb.log({'scores/val_dice_tensor': val_dice}, step=epx)\n",
    "            wandb.log({'scores/dice_mean_wo_bg': dice_mean_no_bg}, step=epx)\n",
    "            wandb.log({'scores/val_dice_mean_wo_bg': round(val_dice.mean(dim=0)[1:].mean().item(),4)}, step=epx)\n",
    "        \n",
    "        if debug:\n",
    "            break\n",
    "#    stat_cuda('Visceral training')\n",
    "    backbone.cpu()\n",
    "    aspp.cpu() \n",
    "    head.cpu()\n",
    "\n",
    "    return backbone,aspp,head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(backbone,aspp,head,name):\n",
    "    torch.save(backbone.state_dict(), name + '_backbone.pth')\n",
    "    torch.save(aspp.state_dict(), name + '_aspp.pth')\n",
    "    torch.save(head.state_dict(), name + '_head.pth')\n",
    "    return None\n",
    "\n",
    "def load_model(name,output_classes,use_mind):\n",
    "    if use_mind:\n",
    "        input_channels = 12\n",
    "    else:\n",
    "        input_channels = 1\n",
    "\n",
    "    backbone, aspp, head = create_model(output_classes=output_classes,input_channels=input_channels)\n",
    "    backbone.load_state_dict(torch.load( name + '_backbone.pth'))\n",
    "    aspp.load_state_dict(torch.load(name + '_aspp.pth'))\n",
    "    head.load_state_dict(torch.load(name + '_head.pth'))\n",
    "    return backbone,aspp,head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'use_mind': True,\n",
    "    'epochs': 2000,\n",
    "    'batch_size': 1,\n",
    "    'fold_splits': 3,\n",
    "    'lr': 0.001,\n",
    "    # Data parameter config\n",
    "    'init_class_param': 1.0, \n",
    "    'learn_class_parameters': False, \n",
    "    'lr_class_param': 0.1,\n",
    "    'init_inst_param': 1.0, \n",
    "    'learn_inst_parameters': False, \n",
    "    'lr_inst_param': 0.1,\n",
    "    'wd_inst_param': 0.0,\n",
    "    'wd_class_param': 0.0,\n",
    "\n",
    "    'skip_clamp_data_param': False,\n",
    "    'clamp_inst_sigma_config': {\n",
    "        'min': np.log(1/20),\n",
    "        'max': np.log(20)\n",
    "    },\n",
    "    'clamp_cls_sigma_config': {\n",
    "        'min': np.log(1/20),\n",
    "        'max': np.log(20)\n",
    "    },\n",
    "\n",
    "    'log_every': 50,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "    \n",
    "    'do_plot': False,\n",
    "    'debug': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f476a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"curriculum_deeplab\", group=\"training\", config=config_dict,\n",
    "    settings=wandb.Settings(start_method=\"thread\") \n",
    "    # mode=\"disabled\"\n",
    ")\n",
    "config = wandb.config\n",
    "\n",
    "backbone, aspp, head = train_DL(training_dataset, epochs=config.epochs, batch_size=config.batch_size,\n",
    "    lr=config.lr,\n",
    "    init_class_param=config.init_class_param, learn_class_parameters=config.learn_class_parameters, lr_class_param=config.lr_class_param,\n",
    "    init_inst_param=config.init_inst_param, learn_inst_parameters=config.learn_inst_parameters, lr_inst_param=config.lr_inst_param,\n",
    "    wd_inst_param=config.wd_inst_param, wd_class_param=config.wd_class_param,\n",
    "    skip_clamp_data_param=config.skip_clamp_data_param,\n",
    "    clamp_inst_sigma_config=config.clamp_inst_sigma_config, clamp_cls_sigma_config=config.clamp_cls_sigma_config, \n",
    "    fold_splits=config.fold_splits, log_every=config.log_every, use_mind=config.use_mind, \n",
    "    debug=config.debug, do_plot=config.do_plot)\n",
    "\n",
    "save_model(backbone, aspp, head,config.mdl_save_prefix)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_DL(\n",
    "    backbone, aspp, head, \n",
    "    inf_dataset, use_mind = True, do_plot=True, debug=False):\n",
    "\n",
    "    backbone.eval()\n",
    "    aspp.eval()\n",
    "    head.eval()\n",
    "    \n",
    "    all_dices = []\n",
    "\n",
    "    for sample_idx, (img, seg) in enumerate(inf_dataset):\n",
    "\n",
    "        img, seg = (\n",
    "            img.unsqueeze(0).unsqueeze(0).float(), \n",
    "            seg.unsqueeze(0).unsqueeze(0).float()\n",
    "        )\n",
    "        \n",
    "        if use_mind:\n",
    "            img = mindssc(img)\n",
    "\n",
    "        if do_plot:\n",
    "            img_slices = img[0:1].permute(0,1,4,2,3).squeeze().unsqueeze(1)\n",
    "            seg_slices = seg[0:1].permute(0,1,4,2,3).squeeze().to(dtype=torch.int64)\n",
    "            display_nonempty_seg_slices(img_slices, seg_slices)\n",
    "            \n",
    "        interpolated_seg = F.interpolate(seg, scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "\n",
    "        img.requires_grad = True\n",
    "        with amp.autocast(enabled=True):\n",
    "            with torch.no_grad():\n",
    "               \n",
    "                interpolated_seg = F.interpolate(seg, scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "                output_val = apply_model(backbone,aspp,head,img,checkpointing=False)\n",
    "\n",
    "                inf_dice = dice3d(\n",
    "                    torch.nn.functional.one_hot(output_val.argmax(1), 3),\n",
    "                    torch.nn.functional.one_hot(interpolated_seg, 3), one_hot_torch_style=True\n",
    "                )\n",
    "            if do_plot:\n",
    "                lbl_slices = seg.detach().squeeze(0).permute(3,0,1,2)\n",
    "                pred_seg_slices = output_val.argmax(1).squeeze(0).permute(2,0,1)\n",
    "                pred_seg_slices = F.upsample_nearest(pred_seg_slices.unsqueeze(0).unsqueeze(0).float(), scale_factor=2).squeeze(0).squeeze(0).long()\n",
    "                display_nonempty_seg_slices(lbl_slices, pred_seg_slices)\n",
    "\n",
    "            mean_no_bg = inf_dice.mean(dim=0)[1:].mean()\n",
    "            all_dices.append(mean_no_bg)\n",
    "            print(\n",
    "                'sample', sample_idx,\n",
    "                'dice_tensor',inf_dice, \n",
    "                'dice mean (nobg)', round(mean_no_bg.item(),4),\n",
    "            )\n",
    "            wandb.log({'scores/dice': mean_no_bg}, step=sample_idx)\n",
    "\n",
    "        if debug:\n",
    "            break\n",
    "    \n",
    "    mean_inf_dice = torch.stack(all_dices).mean()\n",
    "    print(f\"Mean dice over all classes and samples: {mean_inf_dice.item()*100:.2f}%\")\n",
    "    wandb.log({'scores/mean_dice_all_samples': mean_inf_dice}, step=sample_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"curriculum_deeplab\", group=\"testing\", config=config_dict,\n",
    "    settings=wandb.Settings(start_method=\"thread\") \n",
    "    # mode=\"disabled\"\n",
    ")\n",
    "config = wandb.config\n",
    "\n",
    "backbone, aspp, head = load_model(config.mdl_save_prefix, output_classes=3, use_mind=config.use_mind)\n",
    "inference_DL(backbone, aspp, head, validation_dataset, use_mind = config.use_mind, do_plot=config.do_plot, debug=config.debug)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
