{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fc395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name                     Util    Mem free  Cuda             User(s)\n",
      "----  --------------------------  ------  ----------  ---------------  -------------\n",
      "   1  NVIDIA GeForce RTX 2080 Ti     0 %    6637 MiB  11.5(495.29.05)  popp\n",
      "   0  NVIDIA GeForce RTX 2080 Ti     0 %    4760 MiB  11.5(495.29.05)  falta, germer\n",
      "   3  NVIDIA GeForce RTX 2080 Ti     0 %    2083 MiB  11.5(495.29.05)  falta\n",
      "   2  NVIDIA GeForce RTX 2080 Ti    27 %    5683 MiB  11.5(495.29.05)  germer\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   1  NVIDIA GeForce RTX 2080 Ti  ->  cuda:0\n",
      "1.9.1+cu102\n",
      "7605\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -3 -4\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "\n",
    "import torchio as tio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import display_seg\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "\n",
    "import curriculum_deeplab.ml_data_parameters_utils as ml_data_parameters_utils\n",
    "import wandb\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca60d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            # nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        # x = F.adaptive_avg_pool3d(x, (1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')  # , align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = [nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())]\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    " \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "# Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "def create_model(output_classes: int = 14,input_channels: int = 1):\n",
    "    # in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "    in_channels = torch.Tensor([input_channels, 24, 24, 32, 48, 48, 48, 64]).long()\n",
    "    mid_channels = torch.Tensor([64, 128, 192, 192, 256, 256, 256, 384]).long()\n",
    "    out_channels = torch.Tensor([24, 24, 32, 48, 48, 48, 64, 64]).long()\n",
    "    mid_stride = torch.Tensor([1, 1, 1, 2, 1, 1, 1, 1])\n",
    "    net = [nn.Identity()]\n",
    "    for i in range(8):\n",
    "        inc = int(in_channels[i])\n",
    "        midc = int(mid_channels[i])\n",
    "        outc = int(out_channels[i])\n",
    "        strd = int(mid_stride[i])\n",
    "        layer = nn.Sequential(nn.Conv3d(inc, midc, 1, bias=False), nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, midc, 3, stride=strd, padding=1, bias=False, groups=midc),\n",
    "                              nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, outc, 1, bias=False), nn.BatchNorm3d(outc))\n",
    "        if i == 0:\n",
    "            layer[0] = nn.Conv3d(inc, midc, 3, padding=1, stride=2, bias=False)\n",
    "        if (inc == outc) & (strd == 1):\n",
    "            net.append(ResBlock(layer))\n",
    "        else:\n",
    "            net.append(layer)\n",
    "\n",
    "    backbone = nn.Sequential(*net)\n",
    "\n",
    "    count = 0\n",
    "    # weight initialization\n",
    "    for m in backbone.modules():\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            count += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    print('#CNN layer', count)\n",
    "    # complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "    # newer model (one more stride, no groups in head)\n",
    "    aspp = ASPP(64, (2, 4, 8, 16, 32), 128)\n",
    "    head = nn.Sequential(nn.Conv3d(128 + 24, 64, 1, padding=0, groups=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, 64, 3, groups=1, padding=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, output_classes, 1))\n",
    "    return backbone, aspp, head\n",
    "\n",
    "\n",
    "def apply_model(backbone, aspp, head, img, checkpointing=True, return_intermediate=False):\n",
    "    if checkpointing:\n",
    "        x1 = checkpoint(backbone[:3], img)\n",
    "        x2 = checkpoint(backbone[3:], x1)\n",
    "        y = checkpoint(aspp, x2)\n",
    "        y1 = torch.cat((x1, F.interpolate(y, scale_factor=2)), 1)\n",
    "        output_j = checkpoint(head, y1)\n",
    "    else:\n",
    "        x1 = backbone[:3](img)\n",
    "        x2 = backbone[3:](x1)\n",
    "        y = aspp(x2)\n",
    "        y1 = torch.cat((x1, F.interpolate(y, scale_factor=2)), 1)\n",
    "        output_j = head(y1)\n",
    "    if return_intermediate:\n",
    "        return y1,output_j\n",
    "    else:\n",
    "        return output_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd55c32e-bce6-4e35-be76-3bdc306543f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sample(b_image, b_label, scale_factor, yield_2d):\n",
    "    \n",
    "    if yield_2d:\n",
    "        scale = [scale_factor]*2\n",
    "        im_mode = 'bilinear'\n",
    "    else:\n",
    "        scale = [scale_factor]*3\n",
    "        im_mode = 'trilinear'\n",
    "    \n",
    "    b_image = F.interpolate(\n",
    "        b_image.unsqueeze(1), scale_factor=scale, mode=im_mode, align_corners=True\n",
    "    )\n",
    "\n",
    "    b_label = F.interpolate(\n",
    "        b_label.unsqueeze(1).float(), scale_factor=scale, mode='nearest').long()\n",
    "    \n",
    "    return b_image.squeeze(1), b_label.squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "def dilate_label_class(b_label, class_max_idx, class_dilate_idx, \n",
    "                       yield_2d, kernel_sz=3):\n",
    "    \n",
    "    b_dilated_label = b_label\n",
    "    \n",
    "    b_onehot = torch.nn.functional.one_hot(b_label.long(), class_max_idx+1)\n",
    "    class_slice = b_onehot[...,class_dilate_idx]\n",
    "    \n",
    "    pad_start, pad_end = int(kernel_sz//2)-((kernel_sz+1)%2), int(kernel_sz//2)\n",
    "\n",
    "    if yield_2d:\n",
    "        B, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz]).long()\n",
    "        kernel = kernel.view(1,1,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv2d(\n",
    "            class_slice.view(B,1,H,W), kernel, \n",
    "            padding=(pad_start, pad_end))\n",
    "        \n",
    "    else:\n",
    "        B, D, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz,kernel_sz])\n",
    "        kernel = kernel.long().view(1,1,kernel_sz,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv3d(\n",
    "            class_slice.view(B,1,D,H,W), kernel, padding=(pad_start, pad_end))\n",
    "        \n",
    "    dilated_class_slice = torch.clamp(class_slice.squeeze(0), 0, 1)\n",
    "    b_dilated_label[dilated_class_slice.bool()] = class_dilate_idx\n",
    "    \n",
    "    return b_dilated_label\n",
    "\n",
    "\n",
    "def get_batch_dice_wo_bg(b_dice) -> float:\n",
    "    if torch.all(torch.isnan(b_dice[:,1:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,1:]).item()\n",
    "    \n",
    "    \n",
    "\n",
    "def get_batch_dice_tumour(b_dice) -> float: \n",
    "    if torch.all(torch.isnan(b_dice[:,1])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,1]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_batch_dice_cochlea(b_dice) -> float:\n",
    "    if torch.all(torch.isnan(b_dice[:,2])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,2]).item()\n",
    "    \n",
    "    \n",
    "    \n",
    "def map_continuous_from_dataset_idxs(subset_to_map, dataset_idxs):\n",
    "    cont_idxs = torch.tensor([torch.where(dataset_idxs==d_idx) for d_idx in subset_to_map]).reshape(subset_to_map.shape)\n",
    "    return cont_idxs\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "        \n",
    "        \n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "    \n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "        \n",
    "    \n",
    "    \n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8594787c-cddd-4817-9a96-6afd52337a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentBspline(b_image, b_label, num_ctl_points=7, strength=0.05, yield_2d=False):\n",
    "    \"\"\"\n",
    "    2D/3D b-spline augmentation on image and segmentation mini-batch on GPU.\n",
    "    :input: b_image batch (torch.cuda.FloatTensor), b_label batch (torch.cuda.LongTensor)\n",
    "    :return: augmented Bx(D)xHxW image batch (torch.cuda.FloatTensor), \n",
    "    augmented Bx(D)xHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "\n",
    "    KERNEL_SIZE = 3\n",
    "\n",
    "    if yield_2d:\n",
    "        assert b_image.dim() == b_label.dim() == 3, \\\n",
    "            f\"Augmenting 2D. Input batch of image and \" \\\n",
    "            f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,H,W = b_image.shape\n",
    "        \n",
    "        bspline = torch.nn.Sequential(\n",
    "            nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2))\n",
    "        )\n",
    "        # Add an extra *.5 factor to dim strength to make strength fit 3D case\n",
    "        dim_strength = (torch.tensor([H,W]).float()*strength*.5).to(b_image.device)\n",
    "        rand_control_points = dim_strength.view(1,2,1,1) * torch.randn(\n",
    "            1, 2, num_ctl_points, num_ctl_points\n",
    "        ).to(b_image.device)\n",
    "        \n",
    "        bspline_disp = bspline(rand_control_points)\n",
    "        bspline_disp = torch.nn.functional.interpolate(\n",
    "            bspline_disp, size=(H,W), mode='bilinear', align_corners=True\n",
    "        ).permute(0,2,3,1)\n",
    "    \n",
    "        identity = torch.eye(2,3).expand(B,2,3).to(b_image.device)\n",
    "        \n",
    "        id_grid = F.affine_grid(identity, torch.Size((B,2,H,W)), \n",
    "            align_corners=False)\n",
    "\n",
    "    else:\n",
    "        assert b_image.dim() == b_label.dim() == 4, \\\n",
    "            f\"Augmenting 3D. Input batch of image and \" \\\n",
    "            f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,D,H,W = b_image.shape\n",
    "        \n",
    "        bspline = torch.nn.Sequential(\n",
    "            nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2))\n",
    "        )\n",
    "        dim_strength = (torch.tensor([D,H,W]).float()*strength).to(b_image.device)\n",
    "        rand_control_points = dim_strength.view(1,3,1,1,1) * torch.randn(\n",
    "            1, 3, num_ctl_points, num_ctl_points, num_ctl_points\n",
    "        ).to(b_image.device)\n",
    "\n",
    "        bspline_disp = bspline(rand_control_points)\n",
    "\n",
    "        bspline_disp = torch.nn.functional.interpolate(\n",
    "            bspline_disp, size=(D,H,W), mode='trilinear', align_corners=True\n",
    "        ).permute(0,2,3,4,1)\n",
    "    \n",
    "        identity = torch.eye(3,4).expand(B,3,4).to(b_image.device)\n",
    "        \n",
    "        id_grid = F.affine_grid(identity, torch.Size((B,3,D,H,W)), \n",
    "            align_corners=False)\n",
    "\n",
    "    b_image_out = F.grid_sample(\n",
    "        b_image.unsqueeze(1).float(), id_grid + bspline_disp, \n",
    "        padding_mode='border', align_corners=False)\n",
    "    b_label_out = F.grid_sample(\n",
    "        b_label.unsqueeze(1).float(), id_grid + bspline_disp, \n",
    "        mode='nearest', align_corners=False)\n",
    "\n",
    "    return b_image_out.squeeze(1), b_label_out.squeeze(1).long()\n",
    "\n",
    "\n",
    "\n",
    "def augmentAffine(b_image, b_label, strength=0.05, yield_2d=False):\n",
    "    \"\"\"\n",
    "    2D/3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: b_image batch (torch.cuda.FloatTensor), b_label batch (torch.cuda.LongTensor)\n",
    "    :return: augmented Bx(D)xHxW image batch (torch.cuda.FloatTensor), \n",
    "    augmented Bx(D)xHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    if yield_2d:\n",
    "        assert b_image.dim() == b_label.dim() == 3, \\\n",
    "            f\"Augmenting 2D. Input batch of image and \" \\\n",
    "            f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,H,W = b_image.shape\n",
    "        \n",
    "        affine_matrix = (torch.eye(2,3).unsqueeze(0) + torch.randn(B,2,3) \\\n",
    "                         * strength).to(b_image.device)\n",
    "\n",
    "        meshgrid = F.affine_grid(affine_matrix, torch.Size((B,1,H,W)), \n",
    "                                 align_corners=False)\n",
    "    else:\n",
    "        assert b_image.dim() == b_label.dim() == 4, \\\n",
    "            f\"Augmenting 3D. Input batch of image and \" \\\n",
    "            f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,D,H,W = b_image.shape\n",
    "        \n",
    "        affine_matrix = (torch.eye(3,4).unsqueeze(0) + torch.randn(B,3,4) \\\n",
    "                         * strength).to(b_image.device)\n",
    "\n",
    "        meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)), \n",
    "                                 align_corners=False)\n",
    "        \n",
    "    b_image_out = F.grid_sample(\n",
    "        b_image.unsqueeze(1).float(), meshgrid, padding_mode='border', align_corners=False)\n",
    "    b_label_out = F.grid_sample(\n",
    "        b_label.unsqueeze(1).float(), meshgrid, mode='nearest', align_corners=False)\n",
    "\n",
    "    return b_image_out.squeeze(1), b_label_out.squeeze(1).long()\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(b_image, strength=0.05):\n",
    "    return b_image + strength*torch.randn_like(b_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a979a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class CrossMoDa_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        ensure_labeled_pairs=True, use_additional_data=False, resample=True,\n",
    "        size:tuple=(96,96,60), normalize:bool=True, \n",
    "        max_load_num=None, crop_w_dim_range=None,\n",
    "        disturbed_idxs=None, yield_2d_normal_to=None, flip_r_samples=True,\n",
    "        dilate_kernel_sz=3):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                ensure_labeled_pairs (bool): Only images with corresponding labels will be loaded (default: True)\n",
    "                \n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "                max_load_num (int): maximum number of pairs to load (uses first max_load_num samples for either images and labels found)\n",
    "                crop_w_dim_range (tuple): Tuple of ints defining the range to which dimension W of (D,H,W) is cropped\n",
    "                yield_2d_normal_to (bool):\n",
    "                \n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.disturbed_idxs = disturbed_idxs\n",
    "        self.yield_2d_normal_to = yield_2d_normal_to\n",
    "        self.do_train = False\n",
    "        self.augment_at_collate = False\n",
    "        self.dilate_kernel_sz = dilate_kernel_sz\n",
    "        \n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "        \n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "        \n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "        \n",
    "        path = base_dir + state_dir\n",
    "        \n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "            \n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "            \n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "        \n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "        \n",
    "        if domain.lower() == \"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "        \n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "        # First read filepaths\n",
    "        self.img_paths = {}\n",
    "        self.label_paths = {}\n",
    "\n",
    "        for _path in files:\n",
    "     \n",
    "            numeric_id = int(re.findall(r'\\d+', os.path.basename(_path))[0])\n",
    "            if \"_l.nii.gz\" in _path or \"_l_Label.nii.gz\" in _path:\n",
    "                lr_id = 'l'\n",
    "            elif \"_r.nii.gz\" in _path or \"_r_Label.nii.gz\" in _path:\n",
    "                lr_id = 'r'\n",
    "            else:\n",
    "                lr_id = \"\"\n",
    "            \n",
    "            # Generate crossmoda id like 004r\n",
    "            crossmoda_id = f\"{numeric_id:03d}{lr_id}\"\n",
    "            \n",
    "            if \"Label\" in _path:\n",
    "                self.label_paths[crossmoda_id] = _path\n",
    "                    \n",
    "            elif domain in _path:\n",
    "                self.img_paths[crossmoda_id] = _path\n",
    "        \n",
    "        if ensure_labeled_pairs:\n",
    "            pair_idxs = set(self.img_paths).intersection(set(self.label_paths))\n",
    "            self.label_paths = {_id: _path for _id, _path in self.label_paths.items() if _id in pair_idxs}\n",
    "            self.img_paths = {_id: _path for _id, _path in self.img_paths.items() if _id in pair_idxs}\n",
    "        \n",
    "            \n",
    "        # Populate data\n",
    "        self.img_data = {}\n",
    "        self.label_data = {}\n",
    "        self.img_data_2d = {}\n",
    "        self.label_data_2d = {}\n",
    "        \n",
    "        #load data\n",
    "        \n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "        id_paths_to_load = list(self.label_paths.items()) + list(self.img_paths.items())\n",
    "        \n",
    "        description = f\"{len(self.img_paths)} images, {len(self.label_paths)} labels\"\n",
    "\n",
    "        for crossmoda_id, f in tqdm(id_paths_to_load, desc=description):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in f:\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "                \n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                    \n",
    "                if crop_w_dim_range:\n",
    "                    tmp = tmp[..., crop_w_dim_range[0]:crop_w_dim_range[1]]    \n",
    "                \n",
    "                self.label_data[crossmoda_id] = tmp.long()\n",
    "                    \n",
    "            elif domain in f:\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "                \n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                \n",
    "                if crop_w_dim_range:\n",
    "                    tmp = tmp[..., crop_w_dim_range[0]:crop_w_dim_range[1]]\n",
    "                    \n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "                    \n",
    "                self.img_data[crossmoda_id] = tmp\n",
    "        \n",
    "        # Postprocessing\n",
    "        for crossmoda_id in list(self.label_data.keys()):\n",
    "            if self.label_data[crossmoda_id].unique().numel() != 3:\n",
    "                del self.img_data[crossmoda_id]\n",
    "                del self.label_data[crossmoda_id]\n",
    "            elif \"r\" in crossmoda_id:\n",
    "                self.img_data[crossmoda_id] = self.img_data[crossmoda_id].flip(dims=(1,))\n",
    "                self.label_data[crossmoda_id] = self.label_data[crossmoda_id].flip(dims=(1,))\n",
    "        \n",
    "        if max_load_num and ensure_labeled_pairs:\n",
    "            for crossmoda_id in list(self.label_data.keys())[max_load_num:]:\n",
    "                del self.img_data[crossmoda_id]\n",
    "                del self.label_data[crossmoda_id]\n",
    "                \n",
    "        elif max_load_num:\n",
    "            for del_key in list(self.image_data.keys())[max_load_num:]:\n",
    "                del self.img_data[crossmoda_id]\n",
    "            for del_key in list(self.label_data.keys())[max_load_num:]:\n",
    "                del self.label_data[crossmoda_id]\n",
    "            \n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(set(self.img_data)==set(self.label_data)))\n",
    "        \n",
    "        img_stack = torch.stack(list(self.img_data.values()), dim=0)\n",
    "        img_mean, img_std = img_stack.mean(), img_stack.std()\n",
    "        \n",
    "        label_stack = torch.stack(list(self.label_data.values()), dim=0)\n",
    "\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(img_stack.shape, img_mean, img_std))\n",
    "        print(\"Label shape: {}, max.: {}\".format(label_stack.shape,torch.max(label_stack)))\n",
    "        \n",
    "        if yield_2d_normal_to:\n",
    "            if yield_2d_normal_to == \"D\":\n",
    "                slice_dim = -3\n",
    "            if yield_2d_normal_to == \"H\":\n",
    "                slice_dim = -2\n",
    "            if yield_2d_normal_to == \"W\":\n",
    "                slice_dim = -1\n",
    "  \n",
    "            for crossmoda_id, image in self.img_data.items():  \n",
    "                for idx, img_slc in [(slice_idx, image.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(image.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.img_data_2d[f\"{crossmoda_id}{yield_2d_normal_to}{idx:03d}\"] = img_slc\n",
    "                    \n",
    "            for crossmoda_id, label in self.label_data.items():   \n",
    "                for idx, lbl_slc in [(slice_idx, label.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(label.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.label_data_2d[f\"{crossmoda_id}{yield_2d_normal_to}{idx:03d}\"] = lbl_slc\n",
    "                    \n",
    "        print(\"Data import finished. Elapsed time: {:.1f} s\".format(time.time()-t0 ))\n",
    "        print(f\"CrossMoDa loader will yield {'2D' if self.yield_2d_normal_to else '3D'} samples\")\n",
    "        \n",
    "    def get_crossmoda_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data.keys())\n",
    "            .union(set(self.label_data.keys()))\n",
    "        ))\n",
    "    \n",
    "    def get_2d_crossmoda_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data_2d.keys())\n",
    "            .union(set(self.label_data_2d.keys()))\n",
    "        ))\n",
    "    \n",
    "    def get_crossmoda_id_dicts(self):\n",
    "        \n",
    "        all_crossmoda_ids = self.get_crossmoda_ids()\n",
    "        id_dicts = []\n",
    "        \n",
    "        for twod_dataset_idx, twod_crossmoda_id in enumerate(self.get_2d_crossmoda_ids()):\n",
    "            crossmoda_id = twod_crossmoda_id[:-4]\n",
    "            id_dicts.append(\n",
    "                {\n",
    "                    '2d_crossmoda_id': twod_crossmoda_id,\n",
    "                    '2d_dataset_idx': twod_dataset_idx,\n",
    "                    'crossmoda_id': crossmoda_id,\n",
    "                    'dataset_idx': all_crossmoda_ids.index(crossmoda_id),                    \n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return id_dicts\n",
    "        \n",
    "    def __len__(self, yield_2d_override=None):\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data length\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "            \n",
    "        if yield_2d:\n",
    "            return len(self.img_data_2d)\n",
    "        \n",
    "        return len(self.img_data)\n",
    "\n",
    "    def __getitem__(self, dataset_idx, yield_2d_override=None):\n",
    "        \n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data can be overridden\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "        \n",
    "        modified_label = []\n",
    "        \n",
    "        if yield_2d:\n",
    "            all_crossmoda_ids = self.get_2d_crossmoda_ids()\n",
    "            c_id = all_crossmoda_ids[dataset_idx]\n",
    "            image = self.img_data_2d.get(c_id, torch.tensor([]))\n",
    "            label = self.label_data_2d.get(c_id, torch.tensor([]))\n",
    "            \n",
    "            # For 2D crossmoda id cut last 4 \"003rW100\"\n",
    "            image_path = self.img_paths[c_id[:-4]]\n",
    "            label_path = self.label_paths[c_id[:-4]]\n",
    "            \n",
    "        else:\n",
    "            all_crossmoda_ids = self.get_crossmoda_ids()\n",
    "            c_id = all_crossmoda_ids[dataset_idx]\n",
    "            image = self.img_data.get(c_id, torch.tensor([]))\n",
    "            label = self.label_data.get(c_id, torch.tensor([]))\n",
    "\n",
    "            image_path = self.img_paths[c_id]\n",
    "            label_path = self.label_paths[c_id]\n",
    "        \n",
    "        if self.do_train:\n",
    "            # In case of training add augmentation, modification and\n",
    "            # disturbance\n",
    "            \n",
    "            if not self.augment_at_collate:\n",
    "                b_image = image.unsqueeze(0)\n",
    "                b_label = label.unsqueeze(0)\n",
    "                b_image, b_label = self.augment(b_image, b_label, yield_2d)\n",
    "                image = b_image.squeeze(0)\n",
    "                label = b_label.squeeze(0)\n",
    "            \n",
    "            # Dilate small cochlea segmentation\n",
    "            COCHLEA_CLASS_IDX = 2\n",
    "            pre_mod = b_label.squeeze(0)\n",
    "            modified_label = dilate_label_class(\n",
    "                b_label.detach().clone(), COCHLEA_CLASS_IDX, COCHLEA_CLASS_IDX, \n",
    "                yield_2d=yield_2d, kernel_sz=self.dilate_kernel_sz\n",
    "            ).squeeze(0)\n",
    "\n",
    "            if self.disturbed_idxs != None and dataset_idx in self.disturbed_idxs:\n",
    "                if yield_2d:\n",
    "                    modified_label = \\\n",
    "                        torch.flip(modified_label, dims=(-2,-1))\n",
    "                else:\n",
    "                    modified_label = \\\n",
    "                        torch.flip(modified_label, dims=(-3,-2,-1))\n",
    "        \n",
    "        if yield_2d:\n",
    "            assert image.dim() == label.dim() == 2\n",
    "        else:\n",
    "            assert image.dim() == label.dim() == 3\n",
    "            \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'modified_label': modified_label,\n",
    "            'dataset_idx': dataset_idx, \n",
    "            'crossmoda_id': c_id, \n",
    "            'image_path': image_path, \n",
    "            'label_path': label_path\n",
    "        }\n",
    "    \n",
    "    def get_crossmoda_3d_item(self, dataset_idx):\n",
    "        return self.__getitem__(dataset_idx, yield_2d_override=False)\n",
    "\n",
    "    def get_data(self):\n",
    "        img_stack = torch.stack(list(self.img_data.values()), dim=0)\n",
    "        label_stack = torch.stack(list(self.label_data.values()), dim=0)\n",
    "        \n",
    "        return img_stack, label_stack\n",
    "    \n",
    "    def set_disturbed_idxs(self, idxs):\n",
    "        self.disturbed_idxs = idxs\n",
    "        \n",
    "    def train(self):\n",
    "        self.do_train = True\n",
    "        \n",
    "    def eval(self):\n",
    "        self.do_train = False\n",
    "        \n",
    "    def set_augment_at_collate(self):\n",
    "        self.augment_at_collate = True\n",
    "    \n",
    "    def unset_augment_at_collate(self):\n",
    "        self.augment_at_collate = False\n",
    "    \n",
    "    def set_dilate_kernel_size(self, sz):\n",
    "        \n",
    "        self.dilate_kernel_sz = max(0,sz)\n",
    "        \n",
    "    def get_dilate_kernel_size(self):\n",
    "        return self.dilate_kernel_sz\n",
    "        \n",
    "    def get_efficient_augmentation_collate_fn(self):\n",
    "        yield_2d = True if self.yield_2d_normal_to else False\n",
    "        \n",
    "        def collate_closure(batch):\n",
    "            batch = torch.utils.data._utils.collate.default_collate(batch)\n",
    "            if self.augment_at_collate:\n",
    "                # Augment the whole batch not just one sample\n",
    "                b_image = batch['image'].cuda()\n",
    "                b_label = batch['label'].cuda()\n",
    "                b_image, b_label = self.augment(b_image, b_label, yield_2d)\n",
    "                batch['image'], batch['label'] = b_image.cpu(), b_label.cpu()\n",
    "\n",
    "            return batch\n",
    "        \n",
    "        return collate_closure\n",
    "    \n",
    "    def augment(self, b_image, b_label, yield_2d):\n",
    "        \n",
    "        # Wrap image tensor and label with ScalarImage and LabelMap:\n",
    "        # \n",
    "\n",
    "        # Prepare dims for torchio: All transformed \n",
    "        # images / labels need to be 4-dim;\n",
    "        # 2D images need to have dims=1xHxWx1 to make transformation work\n",
    "        \n",
    "        if yield_2d:\n",
    "            assert b_image.dim() == b_label.dim() == 3, \\\n",
    "                f\"Augmenting 2D. Input batch of image and \" \\\n",
    "                f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        else:\n",
    "            assert b_image.dim() == b_label.dim() == 4, \\\n",
    "                f\"Augmenting 3D. Input batch of image and \" \\\n",
    "                f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "\n",
    "        spatial_aug_selector = np.random.rand()\n",
    "        \n",
    "        b_image, b_label = interpolate_sample(b_image, b_label, 2., yield_2d)\n",
    "        if spatial_aug_selector < .4:\n",
    "            b_image, b_label = augmentAffine(\n",
    "                b_image, b_label, strength=0.05, yield_2d=yield_2d)\n",
    "\n",
    "        elif spatial_aug_selector <.8:\n",
    "            b_image, b_label = augmentBspline(\n",
    "                b_image, b_label, num_ctl_points=7, strength=0.005, yield_2d=yield_2d)\n",
    "        else:\n",
    "            pass\n",
    "        b_image, b_label = interpolate_sample(b_image, b_label, .5, yield_2d)\n",
    "        \n",
    "        b_image = augmentNoise(b_image, strength=0.05)\n",
    "        b_label = b_label.long()\n",
    "        \n",
    "        return b_image, b_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb18f481-6804-48ad-8263-7f44b80cd5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    \n",
    "    'num_classes': 3,\n",
    "    'use_mind': True,\n",
    "    'epochs': 120,\n",
    "    \n",
    "    'batch_size': 64,\n",
    "    'val_batch_size': 1,\n",
    "    \n",
    "    'train_set_max_len': 100,\n",
    "    'crop_w_dim_range': (24, 110),\n",
    "    'yield_2d_normal_to': \"W\",\n",
    "    \n",
    "    'lr': 0.0005,\n",
    "    \n",
    "    # Data parameter config\n",
    "    'data_param_optim_momentum': .5,\n",
    "    'init_class_param': 1.0, \n",
    "    'learn_class_parameters': False, \n",
    "    'lr_class_param': 0.1,\n",
    "    'init_inst_param': 1.0, \n",
    "    'learn_inst_parameters': False, \n",
    "    'lr_inst_param': 0.1,\n",
    "    'wd_inst_param': 0.0,\n",
    "    'wd_class_param': 0.0,\n",
    "    \n",
    "    'skip_clamp_data_param': False,\n",
    "    'clamp_inst_sigma_config': {\n",
    "        'min': np.log(1/20),\n",
    "        'max': np.log(20)\n",
    "    },\n",
    "    'clamp_cls_sigma_config': {\n",
    "        'min': np.log(1/20),\n",
    "        'max': np.log(20)\n",
    "    },\n",
    "\n",
    "    'log_every': 1,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "    \n",
    "    'do_plot': False,\n",
    "    'debug': False,\n",
    "    'wandb_mode': \"online\",\n",
    "\n",
    "    'disturbed_flipped_num': 0,\n",
    "    'start_disturbing_after_ep': 1000000,\n",
    "    \n",
    "    'start_dilate_kernel_sz': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "    domain=\"source\", state=\"l4\", size=(128, 128, 128),\n",
    "    ensure_labeled_pairs=True, \n",
    "    max_load_num=config_dict['train_set_max_len'], \n",
    "    crop_w_dim_range=config_dict['crop_w_dim_range'],\n",
    "    yield_2d_normal_to=config_dict['yield_2d_normal_to'],\n",
    "    dilate_kernel_sz=config_dict['start_dilate_kernel_sz']\n",
    ")\n",
    "# validation_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "#     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "# target_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "#     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f25703b-de2d-4422-82cf-249ee741cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128, 128, 86])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, all_labels = training_dataset.get_data()\n",
    "print(all_labels.shape)\n",
    "# D_stack = make_2d_stack_from_3d(all_labels.unsqueeze(1), \"D\")\n",
    "# print(D_stack.shape)\n",
    "sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "plt.xlabel(\"W\")\n",
    "plt.ylabel(\"ground truth>0\")\n",
    "plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b1edbb-45de-4a35-8524-a4848e50bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    # Print bare 2D data\n",
    "    # print(\"Displaying 2D bare sample\")\n",
    "    # for img, label in zip(training_dataset.img_data_2d.values(), \n",
    "    #                       training_dataset.label_data_2d.values()):\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=img.unsqueeze(0), \n",
    "    #                 ground_truth=label,\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "        \n",
    "    # Print transformed 2D data\n",
    "    # training_dataset.train()\n",
    "    # print(\"Displaying 2D training sample\")\n",
    "    # for sample in training_dataset:\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=sample['image'].unsqueeze(0), \n",
    "    #                 ground_truth=sample['label'],\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "    # Print transformed 3D data\n",
    "    training_dataset.train()\n",
    "    print(\"Displaying 3D training sample\")\n",
    "    leng = 1# training_dataset.__len__(yield_2d_override=False)\n",
    "    for sample in (training_dataset.get_crossmoda_3d_item(idx) for idx in range(leng)):\n",
    "        # training_dataset.set_dilate_kernel_size(1)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample['image'].unsqueeze(0), \n",
    "                    ground_truth=sample['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "        \n",
    "        # training_dataset.set_dilate_kernel_size(7)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample['image'].unsqueeze(0), \n",
    "                    ground_truth=sample['modified_label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ad65dae-5a01-4b13-af12-8422b730f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    for sidx in [1,]:\n",
    "        print(f\"Sample {sidx}:\")\n",
    "        \n",
    "        training_dataset.eval()\n",
    "        sample_eval = training_dataset.get_crossmoda_3d_item(sidx)\n",
    "\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=sample_eval['image'].unsqueeze(0), \n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "        \n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=sample_eval['image'].unsqueeze(0), \n",
    "                    ground_truth=sample_eval['label'], \n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "\n",
    "        training_dataset.train()\n",
    "        print(\"Train sample with ground-truth overlay\")\n",
    "        sample_train = training_dataset.get_crossmoda_3d_item(sidx)\n",
    "        print(sample_train['label'].unique())\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=sample_train['image'].unsqueeze(0), \n",
    "                    ground_truth=sample_train['label'], \n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt=.3)\n",
    "\n",
    "        print(\"Eval/train diff with diff overlay\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=(sample_eval['image'] - sample_train['image']).unsqueeze(0), \n",
    "                    ground_truth=(sample_eval['label'] - sample_train['label']).clamp(min=0), \n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f7ed00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_subset = torch.utils.data.Subset(training_dataset,range(2))\n",
    "if config_dict['do_plot']:\n",
    "    train_plotset = (training_dataset.get_crossmoda_3d_item(idx) for idx in (55, 81, 63))\n",
    "    for sample in train_plotset:\n",
    "        print(f\"Sample {sample['dataset_idx']}:\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "            img=sample_eval['image'].unsqueeze(0), \n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .6)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "            img=sample_eval['image'].unsqueeze(0), \n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb958f2-0fee-4ee3-b108-2a203bb65f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "\n",
    "import functools\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use get_named_layers_leaves(module) to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d4986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(lraspp, inst_parameters, class_parameters, \n",
    "    optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "    scaler, _path):\n",
    "    \n",
    "    torch.save(lraspp.state_dict(), _path + '_lraspp.pth')\n",
    "    \n",
    "    torch.save(inst_parameters, _path + '_inst_parameters.pth')\n",
    "    torch.save(class_parameters, _path + '_class_parameters.pth')\n",
    "    \n",
    "    torch.save(optimizer.state_dict(), _path + '_optimizer.pth')\n",
    "    torch.save(optimizer_inst_param.state_dict(), _path + '_optimizer_inst_param.pth')\n",
    "    torch.save(optimizer_class_param.state_dict(), _path + '_optimizer_class_param.pth')\n",
    "    \n",
    "    torch.save(scaler.state_dict(), _path + '_grad_scaler.pth')\n",
    "\n",
    "    \n",
    "    \n",
    "def get_model(config, dataset_len, _path=None):\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=False, progress=True, num_classes=config.num_classes)\n",
    "    set_module(lraspp, 'backbone.0.0', torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False))\n",
    "        \n",
    "    optimizer = torch.optim.Adam(lraspp.parameters(), lr=config.lr)\n",
    "    \n",
    "    (class_parameters, inst_parameters, optimizer_class_param, optimizer_inst_param) = \\\n",
    "        ml_data_parameters_utils.get_class_inst_data_params_n_optimizer(config,\n",
    "            config.init_class_param, config.learn_class_parameters, config.lr_class_param,\n",
    "            config.init_inst_param, config.learn_inst_parameters, config.lr_inst_param,\n",
    "            nr_classes=config.num_classes,\n",
    "            nr_instances=dataset_len,\n",
    "            device='cuda'\n",
    "        )\n",
    "    \n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    if _path and os.path.isfile(_path + '_lraspp.pth'):\n",
    "        print(f\"Loading lr-aspp model, data parameters, optimizers and grad scalers from {_path}\")\n",
    "        lraspp.load_state_dict(torch.load(_path + '_lraspp.pth', map_location='cuda'))\n",
    "\n",
    "        inst_parameters = torch.load(_path + '_inst_parameters.pth', map_location='cuda')\n",
    "        class_parameters = torch.load(_path + '_class_parameters.pth', map_location='cuda')\n",
    "\n",
    "        optimizer.load_state_dict(torch.load(_path + '_optimizer.pth', map_location='cuda'))\n",
    "        optimizer_inst_param.load_state_dict(torch.load(_path + '_optimizer_inst_param.pth', map_location='cuda'))\n",
    "        optimizer_class_param.load_state_dict(torch.load(_path + '_optimizer_class_param.pth', map_location='cuda'))\n",
    "\n",
    "        scaler.load_state_dict(torch.load(_path + '_grad_scaler.pth', map_location='cuda'))\n",
    "    else:\n",
    "        print(\"Loading fresh lr-aspp model, data parameters, optimizers and grad scalers.\")\n",
    "        \n",
    "    return (lraspp, inst_parameters, class_parameters, \n",
    "        optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "        scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cabd4c44-6225-4ed6-bcdd-6b9fc6b30034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "        \n",
    "    \n",
    "            \n",
    "def log_n_largest_data_parameters(parameter_idxs, parameters, log_path, n=10):\n",
    "    data = [[inst_idx, val] for (inst_idx, val) in \\\n",
    "        zip(parameter_idxs, torch.exp(parameters).tolist())]\n",
    "\n",
    "    data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    data = data[:n]\n",
    "    table = wandb.Table(data=data, columns = [\"parameter_idx\", \"value\"])\n",
    "    wandb.log({log_path : wandb.plot.bar(table, \"parameter_idx\", \"value\", title=log_path)})\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_largest_data_parameters_in_target_ratio(parameter_idxs, parameters, target_idxs):\n",
    "    # print(\"param_idxs\", parameter_idxs)\n",
    "    # print(\"parameters\", parameters)\n",
    "    # print(\"target_idxs\", target_idxs)\n",
    "    data = [[inst_idx, val] for (inst_idx, val) in \\\n",
    "        zip(parameter_idxs, torch.exp(parameters).tolist())]\n",
    "    \n",
    "    topk_cont_idxs = torch.argsort(parameters, descending=True)[:len(target_idxs)]\n",
    "    # print(\"topk_cont_idxs\", topk_cont_idxs)\n",
    "    topk_dataset_idxs = parameter_idxs[topk_cont_idxs]\n",
    "    # print(\"topk_dataset_idxs\", topk_dataset_idxs)\n",
    "    ratio = np.sum(np.in1d(topk_dataset_idxs, target_idxs))/len(target_idxs)\n",
    "    \n",
    "    return ratio\n",
    "    \n",
    "    \n",
    "    \n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e9d47e9-fe55-4241-86f1-796e6286e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "    \n",
    "    fold_iter = enumerate(kf.split(training_dataset))\n",
    "\n",
    "    if config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "    \n",
    "    if config.wandb_mode != 'disabled':\n",
    "        # Log dataset info\n",
    "        training_dataset.eval()\n",
    "        dataset_info = [[smp['dataset_idx'], smp['crossmoda_id'], smp['image_path'], smp['label_path']] \\\n",
    "                        for smp in training_dataset]\n",
    "        wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', 'crossmoda_id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    " \n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        \n",
    "        # Training happens in 2D, validation happens in 3D:\n",
    "        # Read 2D dataset idxs which are used for training, \n",
    "        # get their 3D super-ids by 3d dataset length\n",
    "        # and substract these from all 3D ids to get val_3d_idxs\n",
    "        trained_3d_dataset_idxs = {dct['dataset_idx'] \\\n",
    "             for dct in training_dataset.get_crossmoda_id_dicts() if dct['2d_dataset_idx'] in train_idxs.tolist()}\n",
    "        val_3d_idxs = set(range(training_dataset.__len__(yield_2d_override=False))) - trained_3d_dataset_idxs\n",
    "        print(\"Will run validation with these 3D samples:\", val_3d_idxs)\n",
    "        \n",
    "        ### Disturb dataset ###\n",
    "        disturbed_idxs = np.random.choice(train_idxs, size=config.disturbed_flipped_num, replace=False)\n",
    "        disturbed_idxs = torch.tensor(disturbed_idxs)\n",
    "        \n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(disturbed_idxs.tolist()))\n",
    "        \n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in disturbed_idxs])}, \n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "        \n",
    "        ### Visualization ###\n",
    "        if config.do_plot:\n",
    "            print(\"Disturbed samples:\")\n",
    "            for d_idx in disturbed_idxs:\n",
    "                display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=training_dataset[d_idx][0], \n",
    "                    ground_truth=disturb_seg(training_dataset[d_idx][1]),\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "        \n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels =12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "            \n",
    "        _, all_segs = training_dataset.get_data()\n",
    "\n",
    "        # TODO add class weights again\n",
    "        # class_weight = torch.sqrt(1.0/(torch.bincount(all_segs.long().view(-1)).float()))\n",
    "        # class_weight = class_weight/class_weight.mean()\n",
    "        # class_weight[0] = 0.15\n",
    "        # class_weight = class_weight.cuda()\n",
    "        # print('inv sqrt class_weight', class_weight)\n",
    "        \n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size, \n",
    "            sampler=train_subsampler, pin_memory=True, drop_last=False,\n",
    "            collate_fn=training_dataset.get_efficient_augmentation_collate_fn())\n",
    "        training_dataset.unset_augment_at_collate()\n",
    "#         val_dataloader = DataLoader(training_dataset, batch_size=config.val_batch_size, \n",
    "#                                     sampler=val_subsampler, pin_memory=True, drop_last=False)\n",
    "      \n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        (lraspp, inst_parameters, class_parameters, optimizer, optimizer_inst_param, \n",
    "         optimizer_class_param, scaler) = get_model(config, len(train_dataloader), _path=None)#f\"{config.mdl_save_prefix}_fold{fold_idx}\") # TODO start fresh set _path None\n",
    "        \n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=200, T_mult=2, eta_min=config.lr*.1, last_epoch=- 1, verbose=False)\n",
    "        \n",
    "        lraspp.cuda()\n",
    "        inst_parameters = inst_parameters.cuda()\n",
    "        class_parameters = class_parameters.cuda()\n",
    "        \n",
    "        # criterion = nn.CrossEntropyLoss(class_weight)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        top1 = ml_data_parameters_utils.AverageMeter('Acc@1', ':6.2f')\n",
    "        top5 = ml_data_parameters_utils.AverageMeter('Acc@5', ':6.2f')\n",
    "        \n",
    "        t0 = time.time()\n",
    "\n",
    "        for epx in range(config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "        \n",
    "            lraspp.train()\n",
    "            training_dataset.train()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if config.learn_class_parameters:\n",
    "                optimizer_class_param.zero_grad()\n",
    "            if config.learn_inst_parameters:\n",
    "                optimizer_inst_param.zero_grad()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            do_disturb = epx > config.start_disturbing_after_ep\n",
    "            wandb.log({\"do_disturb\": float(do_disturb)}, step=global_idx)\n",
    "\n",
    "            if do_disturb:\n",
    "                training_dataset.set_disturbed_idxs(disturbed_idxs)\n",
    "            else:\n",
    "                training_dataset.set_disturbed_idxs([])\n",
    "                \n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            dices_tumour = []\n",
    "            dices_cochlea = []\n",
    "            \n",
    "            # Load data\n",
    "            for batch in train_dataloader:\n",
    "                \n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "\n",
    "                b_img = b_img.float().cuda()\n",
    "                b_seg_modified = b_seg_modified.cuda()\n",
    "        \n",
    "                b_seg = b_seg.cuda()\n",
    "    \n",
    "                if config.use_mind:\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=True):\n",
    "                    assert b_img.dim() == 4, \\\n",
    "                        f\"Input image for model must be 4D: BxCxHxW but is {b_img.shape}\"\n",
    "                    \n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    if config.learn_class_parameters or config.learn_inst_parameters:\n",
    "                        # TODO change class parameters\n",
    "                        # Compute data parameters for instances in the minibatch\n",
    "                        class_parameter_minibatch = torch.tensor([0])\n",
    "                        # class_parameter_minibatch = class_parameters[b_seg_modified] TODO: Readd that again\n",
    "                        cont_idxs = map_continuous_from_dataset_idxs(b_idxs_dataset, train_idxs)\n",
    "                        inst_parameter_minibatch = inst_parameters[cont_idxs]\n",
    "\n",
    "                        data_parameter_minibatch = ml_data_parameters_utils.get_data_param_for_minibatch(\n",
    "                                                        learn_class_parameters=config.learn_class_parameters, \n",
    "                                                        learn_inst_parameters=config.learn_inst_parameters,\n",
    "                                                        class_param_minibatch=class_parameter_minibatch,\n",
    "                                                        inst_param_minibatch=inst_parameter_minibatch)\n",
    "\n",
    "                        # Compute logits scaled by data parameters\n",
    "                        logits = logits / data_parameter_minibatch.view([-1] + [1]*(logits.dim()-1))\n",
    "\n",
    "\n",
    "                    ### Calculate loss ###\n",
    "                    assert logits.dim() == 4, \\\n",
    "                        f\"Input shape for loss must be BxNUM_CLASSESxHxW but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == 3, \\\n",
    "                        f\"Target shape for loss must be BxHxW but is {b_seg_modified.shape}\"\n",
    "    \n",
    "                    loss = criterion(logits, b_seg_modified)\n",
    "\n",
    "                    ### Apply weight decay on data parameters##\n",
    "                    if config.learn_class_parameters or config.learn_inst_parameters:\n",
    "                        loss = ml_data_parameters_utils.apply_weight_decay_data_parameters(\n",
    "                            config.learn_inst_parameters, config.wd_inst_param,\n",
    "                            config.learn_class_parameters, config.wd_class_param,\n",
    "                            loss,\n",
    "                            class_parameter_minibatch=class_parameter_minibatch,\n",
    "                            inst_parameter_minibatch=inst_parameter_minibatch)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "\n",
    "                if config.learn_class_parameters:\n",
    "                    scaler.step(optimizer_class_param)\n",
    "                if config.learn_inst_parameters:\n",
    "                    scaler.step(optimizer_inst_param)\n",
    "\n",
    "                scaler.update()\n",
    "\n",
    "                # Clamp class and instance level parameters within certain bounds\n",
    "                if config.learn_class_parameters or config.learn_inst_parameters:\n",
    "                    ml_data_parameters_utils.clamp_data_parameters(\n",
    "                        config.skip_clamp_data_param, config.learn_inst_parameters, config.learn_class_parameters,\n",
    "                        class_parameters, inst_parameters,\n",
    "                        config.clamp_inst_sigma_config, config.clamp_cls_sigma_config)\n",
    "                    \n",
    "                epx_losses.append(loss)\n",
    "                \n",
    "                # Prepare logits for scoring\n",
    "                logits_for_score = logits.argmax(1)\n",
    "\n",
    "                # Calculate dice score\n",
    "                dice = dice2d(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, 3),\n",
    "                    torch.nn.functional.one_hot(b_seg, 3), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_wo_bg(dice))\n",
    "                dices_tumour.append(get_batch_dice_tumour(dice))\n",
    "                dices_cochlea.append(get_batch_dice_cochlea(dice))\n",
    "                \n",
    "                if config.do_plot:\n",
    "                    print(\"Training 2D stack image label/ground-truth\")\n",
    "                    print(dice)\n",
    "                    \n",
    "                    display_seg(in_type=\"batch_2D\", \n",
    "                        img=batch['image'].unsqueeze(1).cpu(), \n",
    "                        seg=logits_for_score.cpu(),\n",
    "                        ground_truth=b_seg.cpu(),\n",
    "                        crop_to_non_zero_seg=True,\n",
    "                        crop_to_non_zero_gt=True,\n",
    "                        alpha_seg=.1,\n",
    "                        alpha_gt =.2\n",
    "                    )\n",
    "                    \n",
    "                if config.debug:\n",
    "                    break\n",
    "                    \n",
    "                ###  Scheduler management ###\n",
    "                scheduler.step()\n",
    "\n",
    "                if scheduler.T_cur == 0:\n",
    "                    sz = training_dataset.get_dilate_kernel_size()\n",
    "                    print(\"sz is\", sz)\n",
    "                    training_dataset.set_dilate_kernel_size(sz-1)\n",
    "                    print(f\"Current dilate kernel size is {training_dataset.get_dilate_kernel_size()}.\")\n",
    "\n",
    "            ### Logging ###\n",
    "            if epx % config.log_every == 0 or (epx+1 == config.epochs):\n",
    "                           \n",
    "                ### Log wandb data ###\n",
    "                # Log the epoch idx per fold - so we can recover the diagram by setting \n",
    "                # ref_epoch_idx as x-axis in wandb interface\n",
    "                print(f'epx_fold{fold_idx}{epx} {time.time()-t0:.2f}s')\n",
    "                wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "                \n",
    "                mean_loss = torch.tensor(epx_losses).mean()\n",
    "                mean_dice = np.nanmean(dices)\n",
    "                mean_dice_tumour = np.nanmean(dices_tumour)\n",
    "                mean_dice_cochlea = np.nanmean(dices_cochlea)\n",
    "                \n",
    "                print()\n",
    "                print(f'loss_fold{fold_idx} {mean_loss:.6f}')\n",
    "                print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "                print(f'dice_mean_tumour_fold{fold_idx}', f\"{mean_dice_tumour*100:.2f}%\")\n",
    "                print(f'dice_mean_cochlea_fold{fold_idx}', f\"{mean_dice_cochlea*100:.2f}%\")\n",
    "                wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "                wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "                wandb.log({f'scores/dice_mean_tumour_fold{fold_idx}': mean_dice_tumour}, step=global_idx)\n",
    "                wandb.log({f'scores/dice_mean_cochlea_fold{fold_idx}': mean_dice_cochlea}, step=global_idx)\n",
    "            \n",
    "                # Log data parameters of disturbed samples\n",
    "                if disturbed_idxs.numel() > 0:\n",
    "                    cont_idxs_disturbed = map_continuous_from_dataset_idxs(disturbed_idxs, train_idxs)\n",
    "                    ml_data_parameters_utils.log_intermediate_iteration_stats(\n",
    "                        \"data_parameters/disturbed/\", f\"_fold{fold_idx}\",\n",
    "                        global_idx,\n",
    "                        config.learn_class_parameters, config.learn_inst_parameters,\n",
    "                        class_parameters, inst_parameters[cont_idxs_disturbed], top1, top5)\n",
    "\n",
    "                    # Log data parameters of clean samples\n",
    "                    cont_idxs_clean = map_continuous_from_dataset_idxs(clean_idxs, train_idxs)\n",
    "                    ml_data_parameters_utils.log_intermediate_iteration_stats(\n",
    "                        \"data_parameters/clean/\", f\"_fold{fold_idx}\",\n",
    "                        global_idx,\n",
    "                        config.learn_class_parameters, config.learn_inst_parameters,\n",
    "                        class_parameters, inst_parameters[cont_idxs_clean], top1, top5)\n",
    "                \n",
    "                    # Calculate ratio of data parameters in topN disturbed (is 1.0 if every disturbed\n",
    "                    # sample gets the highest data parameter)\n",
    "                    inst_param_ratio = get_largest_data_parameters_in_target_ratio(\n",
    "                        train_idxs, inst_parameters, disturbed_idxs\n",
    "                    )\n",
    "            \n",
    "                    print(f'data_param_ratio_fold{fold_idx}', inst_param_ratio)\n",
    "                    wandb.log(\n",
    "                        {f'data_parameters/inst_param_largest_in_target_ratio_fold{fold_idx}': inst_param_ratio}, \n",
    "                        step=global_idx\n",
    "                    )\n",
    "                \n",
    "                ## Validation ###\n",
    "                                                    \n",
    "                lraspp.eval()\n",
    "                training_dataset.eval()   \n",
    "                # TODO remove saving \n",
    "                # save_model(lraspp, inst_parameters, class_parameters, \n",
    "                #     optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "                #     scaler, f\"{config.mdl_save_prefix}_fold{fold_idx}\")\n",
    "                \n",
    "                with amp.autocast(enabled=True):\n",
    "                    with torch.no_grad():\n",
    "                        val_dices = []\n",
    "                        val_dices_tumour = []\n",
    "                        val_dices_cochlea = []\n",
    "                        \n",
    "                        for val_idx in val_3d_idxs:\n",
    "                            val_sample = training_dataset.get_crossmoda_3d_item(val_idx)\n",
    "                            stack_dim = training_dataset.yield_2d_normal_to\n",
    "                            # Create batch out of single val sample\n",
    "                            b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                            b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "                            \n",
    "                            B = b_val_img.shape[0]\n",
    "\n",
    "                            b_val_img = b_val_img.unsqueeze(1).float().cuda()\n",
    "                            b_val_seg = b_val_seg.cuda()\n",
    "                            b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=stack_dim)\n",
    "                            \n",
    "                            if config.use_mind:\n",
    "                                b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "                                \n",
    "                            output_val = lraspp(b_val_img_2d)['out']\n",
    "\n",
    "                            # Prepare logits for scoring\n",
    "                            # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                            val_logits_for_score = make_3d_from_2d_stack(output_val, stack_dim, B)\n",
    "                            val_logits_for_score = val_logits_for_score.argmax(1)\n",
    "\n",
    "                            val_dice = dice3d(\n",
    "                                torch.nn.functional.one_hot(val_logits_for_score, 3),\n",
    "                                torch.nn.functional.one_hot(b_val_seg, 3), \n",
    "                                one_hot_torch_style=True\n",
    "                            )\n",
    "                            val_dices.append(get_batch_dice_wo_bg(val_dice))\n",
    "                            val_dices_tumour.append(get_batch_dice_tumour(val_dice))\n",
    "                            val_dices_cochlea.append(get_batch_dice_cochlea(val_dice))\n",
    "                            \n",
    "                            if config.do_plot:\n",
    "                                print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                                print(val_dice)\n",
    "                                # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                                display_seg(in_type=\"single_3D\", \n",
    "                                    reduce_dim=\"W\",\n",
    "                                    img=val_sample['image'].unsqueeze(0).cpu(), \n",
    "                                    seg=val_logits_for_score.squeeze(0).cpu(),\n",
    "                                    ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                    crop_to_non_zero_seg=True,\n",
    "                                    crop_to_non_zero_gt=True,\n",
    "                                    alpha_seg=.4,\n",
    "                                    alpha_gt=.2\n",
    "                                )\n",
    "                        mean_val_dice = np.nanmean(val_dices)\n",
    "                        mean_val_dice_tumour = np.nanmean(val_dices_tumour)\n",
    "                        mean_val_dice_cochlea = np.nanmean(val_dices_cochlea)\n",
    "                        \n",
    "                        print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                        print(f'val_dice_mean_tumour_fold{fold_idx}', f\"{mean_val_dice_tumour*100:.2f}%\")\n",
    "                        print(f'val_dice_mean_cochlea_fold{fold_idx}', f\"{mean_val_dice_cochlea*100:.2f}%\")\n",
    "                        wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                        wandb.log({f'scores/val_dice_mean_tumour_fold{fold_idx}': mean_val_dice_tumour}, step=global_idx)\n",
    "                        wandb.log({f'scores/val_dice_mean_cochlea_fold{fold_idx}': mean_val_dice_cochlea}, step=global_idx)\n",
    "                        print(val_dices, len(val_dices))\n",
    "            if config.debug:\n",
    "                break\n",
    "                \n",
    "        # End of fold loop        \n",
    "        log_n_largest_data_parameters(train_idxs, inst_parameters, \n",
    "            f\"data_parameters/largest_instance_parameters_fold{fold_idx}\", n=30\n",
    "        )\n",
    "        \n",
    "        log_n_largest_data_parameters(train_idxs, inst_parameters, \n",
    "            f\"data_parameters/all_instance_parameters_fold{fold_idx}\", n=len(training_dataset)\n",
    "        )\n",
    "\n",
    "        lraspp.cpu()\n",
    "        \n",
    "        save_model(lraspp, inst_parameters, class_parameters, \n",
    "            optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "            scaler, f\"{config.mdl_save_prefix}_fold{fold_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6f476a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run validation with these 3D samples: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32}\n",
      "Disturbed indexes: []\n",
      "Warning: skip_update_zero_grad set to True. We will zero out update to state and momentum buffer for parameters with zero gradient. \n",
      "Warning: skip_update_zero_grad set to True. We will zero out update to state and momentum buffer for parameters with zero gradient. \n",
      "Loading fresh lr-aspp model, data parameters, optimizers and grad scalers.\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1190531/3429291533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_DL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1190531/4193401751.py\u001b[0m in \u001b[0;36mtrain_DL\u001b[0;34m(run_name, config, training_dataset)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_mind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     \u001b[0mb_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmindssc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;31m### Forward pass ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/curriculum_deeplab/mindssc.py\u001b[0m in \u001b[0;36mmindssc\u001b[0;34m(img, delta, sigma)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmind_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmind_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmind_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmind_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmind_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mmind\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mmind_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mmind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config_dict['debug'] = False\n",
    "config_dict['wandb_mode'] = 'online'\n",
    "\n",
    "run = wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "    config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "    mode=config_dict['wandb_mode']\n",
    ")\n",
    "\n",
    "run_name = run.name\n",
    "config = wandb.config\n",
    "\n",
    "train_DL(run_name, config, training_dataset)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fef1bd-ec6a-4bbd-a5d0-a2b006e8813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_DL(run_name, config, inf_dataset):\n",
    "\n",
    "    score_dicts = []\n",
    "    \n",
    "    fold_iter = range(config.num_folds)\n",
    "    if config_dict['only_first_fold']:\n",
    "        fold_iter = fold_iter[0:1]\n",
    "        \n",
    "    for fold_idx in fold_iter:\n",
    "        lraspp, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "        lraspp.eval()\n",
    "        inf_dataset.eval()   \n",
    "        stack_dim = config.yield_2d_normal_to\n",
    "        \n",
    "        inf_dices = []\n",
    "        inf_dices_tumour = []\n",
    "        inf_dices_cochlea = []\n",
    "        \n",
    "        for inf_sample in inf_dataset:\n",
    "            global_idx = get_global_idx(fold_idx, sample_idx, config.epochs)\n",
    "            crossmoda_id = sample['crossmoda_id']\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Create batch out of single val sample\n",
    "                    b_inf_img = inf_sample['image'].unsqueeze(0)\n",
    "                    b_inf_seg = inf_sample['label'].unsqueeze(0)\n",
    "\n",
    "                    B = b_inf_img.shape[0]\n",
    "\n",
    "                    b_inf_img = b_inf_img.unsqueeze(1).float().cuda()\n",
    "                    b_inf_seg = b_inf_seg.cuda()\n",
    "                    b_inf_img_2d = make_2d_stack_from_3d(b_inf_img, stack_dim=stack_dim)\n",
    "\n",
    "                    if config.use_mind:\n",
    "                        b_inf_img_2d = mindssc(b_inf_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                    output_inf = lraspp(b_inf_img_2d)['out']\n",
    "\n",
    "                    # Prepare logits for scoring\n",
    "                    # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                    inf_logits_for_score = make_3d_from_2d_stack(output_inf, stack_dim, B)\n",
    "                    inf_logits_for_score = inf_logits_for_score.argmax(1)\n",
    "\n",
    "                    inf_dice = dice3d(\n",
    "                        torch.nn.functional.one_hot(inf_logits_for_score, 3),\n",
    "                        torch.nn.functional.one_hot(b_inf_seg, 3), \n",
    "                        one_hot_torch_style=True\n",
    "                    )\n",
    "                    inf_dices.append(get_batch_dice_wo_bg(inf_dice))\n",
    "                    inf_dices_tumour.append(get_batch_dice_tumour(inf_dice))\n",
    "                    inf_dices_cochlea.append(get_batch_dice_cochlea(inf_dice))\n",
    "\n",
    "                    if config.do_plot:\n",
    "                        print(\"Inference 3D image label/ground-truth\")\n",
    "                        print(inf_dice)\n",
    "                        # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                        display_seg(in_type=\"single_3D\", \n",
    "                            reduce_dim=\"W\",\n",
    "                            img=inf_sample['image'].unsqueeze(0).cpu(), \n",
    "                            seg=inf_logits_for_score.squeeze(0).cpu(),\n",
    "                            ground_truth=b_inf_seg.squeeze(0).cpu(),\n",
    "                            crop_to_non_zero_seg=True,\n",
    "                            crop_to_non_zero_gt=True,\n",
    "                            alpha_seg=.4,\n",
    "                            alpha_gt=.2\n",
    "                        )\n",
    "                        \n",
    "            if config.debug:\n",
    "                break\n",
    "                \n",
    "        mean_inf_dice = np.nanmean(inf_dices)\n",
    "        mean_inf_dice_tumour = np.nanmean(inf_dices_tumour)\n",
    "        mean_inf_dice_cochlea = np.nanmean(inf_dices_cochlea)\n",
    "\n",
    "        print(f'inf_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_inf_dice*100:.2f}%\")\n",
    "        print(f'inf_dice_mean_tumour_fold{fold_idx}', f\"{mean_inf_dice_tumour*100:.2f}%\")\n",
    "        print(f'inf_dice_mean_cochlea_fold{fold_idx}', f\"{mean_inf_dice_cochlea*100:.2f}%\")\n",
    "        wandb.log({f'scores/inf_dice_mean_wo_bg_fold{fold_idx}': mean_inf_dice}, step=global_idx)\n",
    "        wandb.log({f'scores/inf_dice_mean_tumour_fold{fold_idx}': mean_inf_dice_tumour}, step=global_idx)\n",
    "        wandb.log({f'scores/inf_dice_mean_cochlea_fold{fold_idx}': mean_inf_dice_cochlea}, step=global_idx)\n",
    "\n",
    "        # Store data for inter-fold scoring\n",
    "        class_dice_list = inf_dices.tolist()[0]\n",
    "        for class_idx, class_dice in enumerate(class_dice_list):\n",
    "            score_dicts.append(\n",
    "                {\n",
    "                    'fold_idx': fold_idx,\n",
    "                    'crossmoda_id': crossmoda_id,\n",
    "                    'class_idx': class_idx,\n",
    "                    'class_dice': class_dice,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    mean_oa_inf_dice = np.nanmean(torch.tensor([score['class_dice'] for score in score_dicts]))\n",
    "    print(f\"Mean dice over all folds, classes and samples: {mean_oa_inf_dice*100:.2f}%\")\n",
    "    wandb.log({'scores/mean_dice_all_folds_samples_classes': mean_oa_inf_dice}, step=global_idx)\n",
    "\n",
    "    return score_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_scores = []\n",
    "run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"testing\", job_type=\"test\",\n",
    "        config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "        mode=config_dict['wandb_mode']\n",
    ")\n",
    "config = wandb.config\n",
    "score_dicts = inference_DL(run_name, config, validation_dataset)\n",
    "folds_scores.append(score_dicts)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
