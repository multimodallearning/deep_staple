{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fc395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name                     Util    Mem free  Cuda             User(s)\n",
      "----  --------------------------  ------  ----------  ---------------  ---------\n",
      "   0  NVIDIA GeForce RTX 2080 Ti     0 %    5963 MiB  11.5(495.29.05)  falta\n",
      "   3  NVIDIA GeForce RTX 2080 Ti     0 %    2083 MiB  11.5(495.29.05)  falta\n",
      "   1  NVIDIA GeForce RTX 2080 Ti    30 %    4872 MiB  11.5(495.29.05)  popp\n",
      "   2  NVIDIA GeForce RTX 2080 Ti  ! 97 %    4898 MiB  11.5(495.29.05)  popp\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   0  NVIDIA GeForce RTX 2080 Ti  ->  cuda:0\n",
      "1.9.1+cu102\n",
      "7605\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -3 -4\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "\n",
    "import torchio as tio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import display_seg\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "\n",
    "import wandb\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from curriculum_deeplab.data_parameters import DataParamMode, DataParamOptim\n",
    "from curriculum_deeplab.data_parameters import DataParameterManager\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "# print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dd55c32e-bce6-4e35-be76-3bdc306543f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sample(b_image, b_label, scale_factor, yield_2d):\n",
    "    \n",
    "    if yield_2d:\n",
    "        scale = [scale_factor]*2\n",
    "        im_mode = 'bilinear'\n",
    "    else:\n",
    "        scale = [scale_factor]*3\n",
    "        im_mode = 'trilinear'\n",
    "    \n",
    "    b_image = F.interpolate(\n",
    "        b_image.unsqueeze(1), scale_factor=scale, mode=im_mode, align_corners=True\n",
    "    )\n",
    "\n",
    "    b_label = F.interpolate(\n",
    "        b_label.unsqueeze(1).float(), scale_factor=scale, mode='nearest').long()\n",
    "    \n",
    "    return b_image.squeeze(1), b_label.squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "def dilate_label_class(b_label, class_max_idx, class_dilate_idx, \n",
    "                       yield_2d, kernel_sz=3):\n",
    "    \n",
    "    if kernel_sz < 2:\n",
    "        return b_label\n",
    "    \n",
    "    b_dilated_label = b_label\n",
    "    \n",
    "    b_onehot = torch.nn.functional.one_hot(b_label.long(), class_max_idx+1)\n",
    "    class_slice = b_onehot[...,class_dilate_idx]\n",
    "    \n",
    "    if yield_2d:\n",
    "        B, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz]).long()\n",
    "        kernel = kernel.view(1,1,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv2d(\n",
    "            class_slice.view(B,1,H,W), kernel, padding='same')\n",
    "        \n",
    "    else:\n",
    "        B, D, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz,kernel_sz])\n",
    "        kernel = kernel.long().view(1,1,kernel_sz,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv3d(\n",
    "            class_slice.view(B,1,D,H,W), kernel, padding='same')\n",
    "        \n",
    "    dilated_class_slice = torch.clamp(class_slice.squeeze(0), 0, 1)\n",
    "    b_dilated_label[dilated_class_slice.bool()] = class_dilate_idx\n",
    "    \n",
    "    return b_dilated_label\n",
    "\n",
    "\n",
    "def get_batch_dice_per_class(b_dice, class_tags, exclude_bg=True) -> dict:\n",
    "    score_dict = {}\n",
    "    for cls_idx, cls_tag in enumerate(class_tags):\n",
    "        if exclude_bg and cls_idx == 0:\n",
    "            continue\n",
    "            \n",
    "        if torch.all(torch.isnan(b_dice[:,cls_idx])):\n",
    "            score = float('nan')\n",
    "        else:\n",
    "            score = np.nanmean(b_dice[:,cls_idx:]).item()\n",
    "\n",
    "        score_dict[cls_tag] = score\n",
    "        \n",
    "    return score_dict\n",
    "\n",
    "def get_batch_dice_over_all(b_dice, exclude_bg=True) -> float: \n",
    "    start_idx = 1 if exclude_bg else 0\n",
    "    if torch.all(torch.isnan(b_dice[:,start_idx:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,start_idx:]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "        \n",
    "        \n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "    \n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "        \n",
    "    \n",
    "    \n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8594787c-cddd-4817-9a96-6afd52337a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentBspline(b_image, b_label, num_ctl_points=7, strength=0.05, yield_2d=False):\n",
    "    \"\"\"\n",
    "    2D/3D b-spline augmentation on image and segmentation mini-batch on GPU.\n",
    "    :input: b_image batch (torch.cuda.FloatTensor), b_label batch (torch.cuda.LongTensor)\n",
    "    :return: augmented Bx(D)xHxW image batch (torch.cuda.FloatTensor), \n",
    "    augmented Bx(D)xHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "\n",
    "    KERNEL_SIZE = 3\n",
    "\n",
    "    if yield_2d:\n",
    "        assert b_image.dim() == b_label.dim() == 3, \\\n",
    "            f\"Augmenting 2D. Input batch of image and \" \\\n",
    "            f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,H,W = b_image.shape\n",
    "        \n",
    "        bspline = torch.nn.Sequential(\n",
    "            nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2))\n",
    "        )\n",
    "        # Add an extra *.5 factor to dim strength to make strength fit 3D case\n",
    "        dim_strength = (torch.tensor([H,W]).float()*strength*.5).to(b_image.device)\n",
    "        rand_control_points = dim_strength.view(1,2,1,1) * torch.randn(\n",
    "            1, 2, num_ctl_points, num_ctl_points\n",
    "        ).to(b_image.device)\n",
    "        \n",
    "        bspline_disp = bspline(rand_control_points)\n",
    "        bspline_disp = torch.nn.functional.interpolate(\n",
    "            bspline_disp, size=(H,W), mode='bilinear', align_corners=True\n",
    "        ).permute(0,2,3,1)\n",
    "    \n",
    "        identity = torch.eye(2,3).expand(B,2,3).to(b_image.device)\n",
    "        \n",
    "        id_grid = F.affine_grid(identity, torch.Size((B,2,H,W)), \n",
    "            align_corners=False)\n",
    "\n",
    "    else:\n",
    "        assert b_image.dim() == b_label.dim() == 4, \\\n",
    "            f\"Augmenting 3D. Input batch of image and \" \\\n",
    "            f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,D,H,W = b_image.shape\n",
    "        \n",
    "        bspline = torch.nn.Sequential(\n",
    "            nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "            nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2))\n",
    "        )\n",
    "        dim_strength = (torch.tensor([D,H,W]).float()*strength).to(b_image.device)\n",
    "        rand_control_points = dim_strength.view(1,3,1,1,1) * torch.randn(\n",
    "            1, 3, num_ctl_points, num_ctl_points, num_ctl_points\n",
    "        ).to(b_image.device)\n",
    "\n",
    "        bspline_disp = bspline(rand_control_points)\n",
    "\n",
    "        bspline_disp = torch.nn.functional.interpolate(\n",
    "            bspline_disp, size=(D,H,W), mode='trilinear', align_corners=True\n",
    "        ).permute(0,2,3,4,1)\n",
    "    \n",
    "        identity = torch.eye(3,4).expand(B,3,4).to(b_image.device)\n",
    "        \n",
    "        id_grid = F.affine_grid(identity, torch.Size((B,3,D,H,W)), \n",
    "            align_corners=False)\n",
    "\n",
    "    b_image_out = F.grid_sample(\n",
    "        b_image.unsqueeze(1).float(), id_grid + bspline_disp, \n",
    "        padding_mode='border', align_corners=False)\n",
    "    b_label_out = F.grid_sample(\n",
    "        b_label.unsqueeze(1).float(), id_grid + bspline_disp, \n",
    "        mode='nearest', align_corners=False)\n",
    "\n",
    "    return b_image_out.squeeze(1), b_label_out.squeeze(1).long()\n",
    "\n",
    "\n",
    "\n",
    "def augmentAffine(b_image, b_label, strength=0.05, yield_2d=False):\n",
    "    \"\"\"\n",
    "    2D/3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: b_image batch (torch.cuda.FloatTensor), b_label batch (torch.cuda.LongTensor)\n",
    "    :return: augmented Bx(D)xHxW image batch (torch.cuda.FloatTensor), \n",
    "    augmented Bx(D)xHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    if yield_2d:\n",
    "        assert b_image.dim() == b_label.dim() == 3, \\\n",
    "            f\"Augmenting 2D. Input batch of image and \" \\\n",
    "            f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,H,W = b_image.shape\n",
    "        \n",
    "        affine_matrix = (torch.eye(2,3).unsqueeze(0) + torch.randn(B,2,3) \\\n",
    "                         * strength).to(b_image.device)\n",
    "\n",
    "        meshgrid = F.affine_grid(affine_matrix, torch.Size((B,1,H,W)), \n",
    "                                 align_corners=False)\n",
    "    else:\n",
    "        assert b_image.dim() == b_label.dim() == 4, \\\n",
    "            f\"Augmenting 3D. Input batch of image and \" \\\n",
    "            f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        B,D,H,W = b_image.shape\n",
    "        \n",
    "        affine_matrix = (torch.eye(3,4).unsqueeze(0) + torch.randn(B,3,4) \\\n",
    "                         * strength).to(b_image.device)\n",
    "\n",
    "        meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)), \n",
    "                                 align_corners=False)\n",
    "        \n",
    "    b_image_out = F.grid_sample(\n",
    "        b_image.unsqueeze(1).float(), meshgrid, padding_mode='border', align_corners=False)\n",
    "    b_label_out = F.grid_sample(\n",
    "        b_label.unsqueeze(1).float(), meshgrid, mode='nearest', align_corners=False)\n",
    "\n",
    "    return b_image_out.squeeze(1), b_label_out.squeeze(1).long()\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(b_image, strength=0.05):\n",
    "    return b_image + strength*torch.randn_like(b_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "29481e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class CrossMoDa_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        ensure_labeled_pairs=True, use_additional_data=False, resample=True,\n",
    "        size:tuple=(96,96,60), normalize:bool=True, \n",
    "        max_load_num=None, crop_w_dim_range=None,\n",
    "        disturbed_idxs=None, yield_2d_normal_to=None, flip_r_samples=True,\n",
    "        dilate_kernel_sz=3,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                ensure_labeled_pairs (bool): Only images with corresponding labels will be loaded (default: True)\n",
    "                \n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "                max_load_num (int): maximum number of pairs to load (uses first max_load_num samples for either images and labels found)\n",
    "                crop_w_dim_range (tuple): Tuple of ints defining the range to which dimension W of (D,H,W) is cropped\n",
    "                yield_2d_normal_to (bool):\n",
    "                \n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.disturbed_idxs = disturbed_idxs\n",
    "        self.yield_2d_normal_to = yield_2d_normal_to\n",
    "        self.do_train = False\n",
    "        self.augment_at_collate = False\n",
    "        self.dilate_kernel_sz = dilate_kernel_sz\n",
    "        \n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "        \n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "        \n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "        \n",
    "        path = base_dir + state_dir\n",
    "        \n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "            \n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "            \n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "        \n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "        \n",
    "        if domain.lower() == \"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "        \n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "        # First read filepaths\n",
    "        self.img_paths = {}\n",
    "        self.label_paths = {}\n",
    "\n",
    "        for _path in files:\n",
    "     \n",
    "            numeric_id = int(re.findall(r'\\d+', os.path.basename(_path))[0])\n",
    "            if \"_l.nii.gz\" in _path or \"_l_Label.nii.gz\" in _path:\n",
    "                lr_id = 'l'\n",
    "            elif \"_r.nii.gz\" in _path or \"_r_Label.nii.gz\" in _path:\n",
    "                lr_id = 'r'\n",
    "            else:\n",
    "                lr_id = \"\"\n",
    "            \n",
    "            # Generate crossmoda id like 004r\n",
    "            crossmoda_id = f\"{numeric_id:03d}{lr_id}\"\n",
    "            \n",
    "            if \"Label\" in _path:\n",
    "                self.label_paths[crossmoda_id] = _path\n",
    "                    \n",
    "            elif domain in _path:\n",
    "                self.img_paths[crossmoda_id] = _path\n",
    "        \n",
    "        if ensure_labeled_pairs:\n",
    "            pair_idxs = set(self.img_paths).intersection(set(self.label_paths))\n",
    "            self.label_paths = {_id: _path for _id, _path in self.label_paths.items() if _id in pair_idxs}\n",
    "            self.img_paths = {_id: _path for _id, _path in self.img_paths.items() if _id in pair_idxs}\n",
    "        \n",
    "            \n",
    "        # Populate data\n",
    "        self.img_data = {}\n",
    "        self.label_data = {}\n",
    "        self.img_data_2d = {}\n",
    "        self.label_data_2d = {}\n",
    "        \n",
    "        #load data\n",
    "        \n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "        id_paths_to_load = list(self.label_paths.items()) + list(self.img_paths.items())\n",
    "        \n",
    "        description = f\"{len(self.img_paths)} images, {len(self.label_paths)} labels\"\n",
    "\n",
    "        for crossmoda_id, f in tqdm(id_paths_to_load, desc=description):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in f:\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "                \n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                    \n",
    "                if crop_w_dim_range:\n",
    "                    tmp = tmp[..., crop_w_dim_range[0]:crop_w_dim_range[1]]    \n",
    "                \n",
    "                self.label_data[crossmoda_id] = tmp.long()\n",
    "                    \n",
    "            elif domain in f:\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "                \n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                \n",
    "                if crop_w_dim_range:\n",
    "                    tmp = tmp[..., crop_w_dim_range[0]:crop_w_dim_range[1]]\n",
    "                    \n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "                    \n",
    "                self.img_data[crossmoda_id] = tmp\n",
    "        \n",
    "        # Postprocessing\n",
    "        for crossmoda_id in list(self.label_data.keys()):\n",
    "            if self.label_data[crossmoda_id].unique().numel() != 3:\n",
    "                del self.img_data[crossmoda_id]\n",
    "                del self.label_data[crossmoda_id]\n",
    "            elif \"r\" in crossmoda_id:\n",
    "                self.img_data[crossmoda_id] = self.img_data[crossmoda_id].flip(dims=(1,))\n",
    "                self.label_data[crossmoda_id] = self.label_data[crossmoda_id].flip(dims=(1,))\n",
    "        \n",
    "        if max_load_num and ensure_labeled_pairs:\n",
    "            for crossmoda_id in list(self.label_data.keys())[max_load_num:]:\n",
    "                del self.img_data[crossmoda_id]\n",
    "                del self.label_data[crossmoda_id]\n",
    "                \n",
    "        elif max_load_num:\n",
    "            for del_key in list(self.image_data.keys())[max_load_num:]:\n",
    "                del self.img_data[crossmoda_id]\n",
    "            for del_key in list(self.label_data.keys())[max_load_num:]:\n",
    "                del self.label_data[crossmoda_id]\n",
    "            \n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(set(self.img_data)==set(self.label_data)))\n",
    "        \n",
    "        img_stack = torch.stack(list(self.img_data.values()), dim=0)\n",
    "        img_mean, img_std = img_stack.mean(), img_stack.std()\n",
    "        \n",
    "        label_stack = torch.stack(list(self.label_data.values()), dim=0)\n",
    "\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(img_stack.shape, img_mean, img_std))\n",
    "        print(\"Label shape: {}, max.: {}\".format(label_stack.shape,torch.max(label_stack)))\n",
    "        \n",
    "        if yield_2d_normal_to:\n",
    "            if yield_2d_normal_to == \"D\":\n",
    "                slice_dim = -3\n",
    "            if yield_2d_normal_to == \"H\":\n",
    "                slice_dim = -2\n",
    "            if yield_2d_normal_to == \"W\":\n",
    "                slice_dim = -1\n",
    "  \n",
    "            for crossmoda_id, image in self.img_data.items():  \n",
    "                for idx, img_slc in [(slice_idx, image.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(image.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.img_data_2d[f\"{crossmoda_id}{yield_2d_normal_to}{idx:03d}\"] = img_slc\n",
    "                    \n",
    "            for crossmoda_id, label in self.label_data.items():   \n",
    "                for idx, lbl_slc in [(slice_idx, label.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(label.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.label_data_2d[f\"{crossmoda_id}{yield_2d_normal_to}{idx:03d}\"] = lbl_slc\n",
    "                    \n",
    "        print(\"Data import finished. Elapsed time: {:.1f} s\".format(time.time()-t0 ))\n",
    "        print(f\"CrossMoDa loader will yield {'2D' if self.yield_2d_normal_to else '3D'} samples\")\n",
    "        \n",
    "    def get_crossmoda_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data.keys())\n",
    "            .union(set(self.label_data.keys()))\n",
    "        ))\n",
    "    \n",
    "    def get_2d_crossmoda_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data_2d.keys())\n",
    "            .union(set(self.label_data_2d.keys()))\n",
    "        ))\n",
    "    \n",
    "    def get_crossmoda_id_dicts(self):\n",
    "        \n",
    "        all_crossmoda_ids = self.get_crossmoda_ids()\n",
    "        id_dicts = []\n",
    "        \n",
    "        for twod_dataset_idx, twod_crossmoda_id in enumerate(self.get_2d_crossmoda_ids()):\n",
    "            crossmoda_id = twod_crossmoda_id[:-4]\n",
    "            id_dicts.append(\n",
    "                {\n",
    "                    '2d_crossmoda_id': twod_crossmoda_id,\n",
    "                    '2d_dataset_idx': twod_dataset_idx,\n",
    "                    'crossmoda_id': crossmoda_id,\n",
    "                    'dataset_idx': all_crossmoda_ids.index(crossmoda_id),                    \n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return id_dicts\n",
    "        \n",
    "    def __len__(self, yield_2d_override=None):\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data length\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "            \n",
    "        if yield_2d:\n",
    "            return len(self.img_data_2d)\n",
    "        \n",
    "        return len(self.img_data)\n",
    "\n",
    "    def __getitem__(self, dataset_idx, yield_2d_override=None):\n",
    "        \n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data can be overridden\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "        \n",
    "        modified_label = []\n",
    "        \n",
    "        if yield_2d:\n",
    "            all_crossmoda_ids = self.get_2d_crossmoda_ids()\n",
    "            c_id = all_crossmoda_ids[dataset_idx]\n",
    "            image = self.img_data_2d.get(c_id, torch.tensor([]))\n",
    "            label = self.label_data_2d.get(c_id, torch.tensor([]))\n",
    "            \n",
    "            # For 2D crossmoda id cut last 4 \"003rW100\"\n",
    "            image_path = self.img_paths[c_id[:-4]]\n",
    "            label_path = self.label_paths[c_id[:-4]]\n",
    "            \n",
    "        else:\n",
    "            all_crossmoda_ids = self.get_crossmoda_ids()\n",
    "            c_id = all_crossmoda_ids[dataset_idx]\n",
    "            image = self.img_data.get(c_id, torch.tensor([]))\n",
    "            label = self.label_data.get(c_id, torch.tensor([]))\n",
    "\n",
    "            image_path = self.img_paths[c_id]\n",
    "            label_path = self.label_paths[c_id]\n",
    "        \n",
    "        if self.do_train:\n",
    "            # In case of training add augmentation, modification and\n",
    "            # disturbance\n",
    "            \n",
    "            if not self.augment_at_collate:\n",
    "                b_image = image.unsqueeze(0)\n",
    "                b_label = label.unsqueeze(0)\n",
    "                # b_image, b_label = self.augment_tio(b_image, b_label, yield_2d) # TODO\n",
    "                image = b_image.squeeze(0)\n",
    "                label = b_label.squeeze(0)\n",
    "            \n",
    "            # Dilate small cochlea segmentation\n",
    "            COCHLEA_CLASS_IDX = 2\n",
    "            pre_mod = b_label.squeeze(0)\n",
    "            modified_label = dilate_label_class(\n",
    "                b_label.detach().clone(), COCHLEA_CLASS_IDX, COCHLEA_CLASS_IDX, \n",
    "                yield_2d=yield_2d, kernel_sz=self.dilate_kernel_sz\n",
    "            ).squeeze(0)\n",
    "\n",
    "            if self.disturbed_idxs != None and dataset_idx in self.disturbed_idxs:\n",
    "                if yield_2d:\n",
    "                    modified_label = \\\n",
    "                        torch.flip(modified_label, dims=(-2,-1))\n",
    "                else:\n",
    "                    modified_label = \\\n",
    "                        torch.flip(modified_label, dims=(-3,-2,-1))\n",
    "        \n",
    "        if yield_2d:\n",
    "            assert image.dim() == label.dim() == 2\n",
    "        else:\n",
    "            assert image.dim() == label.dim() == 3\n",
    "            \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'modified_label': modified_label,\n",
    "            'dataset_idx': dataset_idx, \n",
    "            'crossmoda_id': c_id, \n",
    "            'image_path': image_path, \n",
    "            'label_path': label_path\n",
    "        }\n",
    "    \n",
    "    def get_crossmoda_3d_item(self, dataset_idx):\n",
    "        return self.__getitem__(dataset_idx, yield_2d_override=False)\n",
    "\n",
    "    def get_data(self):\n",
    "        img_stack = torch.stack(list(self.img_data.values()), dim=0)\n",
    "        label_stack = torch.stack(list(self.label_data.values()), dim=0)\n",
    "        \n",
    "        return img_stack, label_stack\n",
    "    \n",
    "    def set_disturbed_idxs(self, idxs):\n",
    "        self.disturbed_idxs = idxs\n",
    "        \n",
    "    def train(self):\n",
    "        self.do_train = True\n",
    "        \n",
    "    def eval(self):\n",
    "        self.do_train = False\n",
    "        \n",
    "    def set_augment_at_collate(self):\n",
    "        self.augment_at_collate = True\n",
    "    \n",
    "    def unset_augment_at_collate(self):\n",
    "        self.augment_at_collate = False\n",
    "    \n",
    "    def set_dilate_kernel_size(self, sz):\n",
    "        \n",
    "        self.dilate_kernel_sz = max(1,sz)\n",
    "        \n",
    "    def get_dilate_kernel_size(self):\n",
    "        return self.dilate_kernel_sz\n",
    "        \n",
    "    def get_efficient_augmentation_collate_fn(self):\n",
    "        yield_2d = True if self.yield_2d_normal_to else False\n",
    "        \n",
    "        def collate_closure(batch):\n",
    "            batch = torch.utils.data._utils.collate.default_collate(batch)\n",
    "            if self.augment_at_collate:\n",
    "                # Augment the whole batch not just one sample\n",
    "                b_image = batch['image'].cuda()\n",
    "                b_label = batch['label'].cuda()\n",
    "                # b_image, b_label = self.augment(b_image, b_label, yield_2d) # TODO\n",
    "                batch['image'], batch['label'] = b_image.cpu(), b_label.cpu()\n",
    "\n",
    "            return batch\n",
    "        \n",
    "        return collate_closure\n",
    "    \n",
    "    def augment(self, b_image, b_label, yield_2d):\n",
    "        if yield_2d:\n",
    "            assert b_image.dim() == b_label.dim() == 3, \\\n",
    "                f\"Augmenting 2D. Input batch of image and \" \\\n",
    "                f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        else:\n",
    "            assert b_image.dim() == b_label.dim() == 4, \\\n",
    "                f\"Augmenting 3D. Input batch of image and \" \\\n",
    "                f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "\n",
    "        spatial_aug_selector = np.random.rand()\n",
    "        \n",
    "        b_image, b_label = interpolate_sample(b_image, b_label, 2., yield_2d)\n",
    "        if spatial_aug_selector < .4:\n",
    "            b_image, b_label = augmentAffine(\n",
    "                b_image, b_label, strength=0.05, yield_2d=yield_2d)\n",
    "\n",
    "        elif spatial_aug_selector <.8:\n",
    "            b_image, b_label = augmentBspline(\n",
    "                b_image, b_label, num_ctl_points=7, strength=0.005, yield_2d=yield_2d)\n",
    "        else:\n",
    "            pass\n",
    "        b_image, b_label = interpolate_sample(b_image, b_label, .5, yield_2d)\n",
    "        \n",
    "        b_image = augmentNoise(b_image, strength=0.05)\n",
    "        b_label = b_label.long()\n",
    "        \n",
    "        b_label = b_label.long()\n",
    "        \n",
    "        return b_image, b_label\n",
    "        \n",
    "    def augment_tio(self, image, label, yield_2d):\n",
    "        # Prepare dims for torchio: All transformed \n",
    "        # images / labels need to be 4-dim;\n",
    "        # 2D images need to have dims=1xHxWx1 to make transformation work\n",
    "        \n",
    "        if yield_2d:\n",
    "            assert image.dim() == label.dim() == 3, \\\n",
    "                f\"Augmenting 2D. Input batch of image and \" \\\n",
    "                f\"label should be 1xHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        else:\n",
    "            assert image.dim() == label.dim() == 4, \\\n",
    "                f\"Augmenting 3D. Input batch of image and \" \\\n",
    "                f\"label should be 1xDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "\n",
    "            \n",
    "        if self.yield_2d_normal_to:           \n",
    "            self.spatial_transform = tio.OneOf({\n",
    "                tio.transforms.RandomAffine(.05): 0.8,\n",
    "                tio.transforms.RandomElasticDeformation(num_control_points=7, max_displacement=(7.5,7.5,1e-5)): 0.2,\n",
    "                },\n",
    "                p=0.75,\n",
    "            )\n",
    "        else:\n",
    "            self.spatial_transform = tio.OneOf({\n",
    "                tio.transforms.RandomAffine(.05): 0.8,\n",
    "                tio.transforms.RandomElasticDeformation(num_control_points=7, max_displacement=7.5): 0.2,\n",
    "                },\n",
    "                p=0.75,\n",
    "            )\n",
    "\n",
    "        # Transforms can be composed as in torchvision.transforms\n",
    "\n",
    "        self.intensity_transform = tio.OneOf({\n",
    "            tio.transforms.RandomNoise(std=0.05): 0.6,\n",
    "        })\n",
    "\n",
    "        if yield_2d:\n",
    "            image = image.unsqueeze(-1)\n",
    "            label = label.unsqueeze(-1)\n",
    "\n",
    "            \n",
    "        # Run torchio transformation - LabelMap will be secured for intensity\n",
    "        # transformations\n",
    "        subject = tio.Subject(\n",
    "            image=tio.ScalarImage(tensor=image),  \n",
    "            label=tio.LabelMap(tensor=label)\n",
    "        )\n",
    "        subject = self.spatial_transform(subject)\n",
    "        # Transform image intensities apart from subject - spatial transform\n",
    "        # was not applied to label correctly if transformations are stacked.\n",
    "        image = self.intensity_transform(subject.image).data\n",
    "        image = subject.image.data\n",
    "        label = subject.label.data\n",
    "\n",
    "        if yield_2d:\n",
    "            image = image.squeeze(-1)\n",
    "            label = label.squeeze(-1)\n",
    "                \n",
    "        label = label.long()\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "13a979a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist.dataset import OrganMNIST3D, MedMNIST\n",
    "\n",
    "class WrapperOrganMNIST3D():\n",
    "    \n",
    "    def __init__(self,\n",
    "        split='train', root='./data', download=True, normalize=True, \n",
    "        max_load_num=None, crop_w_dim_range=None,\n",
    "        disturbed_idxs=None, yield_2d_normal_to=None\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        MedMNIST dataset\n",
    "\n",
    "        Parameters:\n",
    "                data_flag (str): one of ... TODO (default: 'organsmnist')\n",
    "                \n",
    "        Returns:\n",
    "                torch.utils.data.Dataset\n",
    "\n",
    "        \"\"\"\n",
    "        self.mnist_set = OrganMNIST3D(split=split, transform=None, target_transform=None,\n",
    "            download=download, as_rgb=False, root=root)\n",
    "\n",
    "        self.disturbed_idxs = []\n",
    "        # self.disturbed_permuted = {}\n",
    "        self.set_disturbed_idxs(disturbed_idxs)\n",
    "        \n",
    "        self.yield_2d_normal_to = yield_2d_normal_to\n",
    "        self.do_train = False\n",
    "        self.augment_at_collate = False\n",
    "\n",
    "        self.img_data_3d = {}\n",
    "        self.label_data_3d = {}\n",
    "\n",
    "        self.img_data_2d = {}\n",
    "        self.label_data_2d = {}\n",
    "\n",
    "        if max_load_num:\n",
    "            effective_len = min(len(self.mnist_set), max_load_num)\n",
    "        else:\n",
    "            effective_len = len(self.mnist_set)\n",
    "        \n",
    "        used_mnist_idxs = np.random.permutation(effective_len).tolist()\n",
    "        \n",
    "        for medmnist_id in sorted(used_mnist_idxs): \n",
    "            # Reference data from super class\n",
    "            img = torch.tensor(self.mnist_set.imgs[medmnist_id], dtype=torch.float)\n",
    "            self.label_data_3d[medmnist_id] = torch.tensor(self.mnist_set.labels[medmnist_id]).expand_as(img).long()\n",
    "            \n",
    "            if normalize: #normalize image to zero mean and unit std\n",
    "                img = (img - img.mean()) / img.std()\n",
    "                self.img_data_3d[medmnist_id] = img\n",
    "\n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(set(self.img_data_3d)==set(self.label_data_3d)))\n",
    "        \n",
    "        img_stack = torch.stack(list(self.img_data_3d.values()), dim=0)\n",
    "        img_mean, img_std = img_stack.mean(), img_stack.std()\n",
    "        \n",
    "        label_stack = torch.stack(list(self.label_data_3d.values()), dim=0)\n",
    "\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(img_stack.shape, img_mean, img_std))\n",
    "        print(\"Label shape: {}, max.: {}\".format(label_stack.shape,torch.max(label_stack)))\n",
    "        \n",
    "        if yield_2d_normal_to:\n",
    "            if yield_2d_normal_to == \"D\":\n",
    "                slice_dim = -3\n",
    "            if yield_2d_normal_to == \"H\":\n",
    "                slice_dim = -2\n",
    "            if yield_2d_normal_to == \"W\":\n",
    "                slice_dim = -1\n",
    "  \n",
    "            for _3d_id, image in self.img_data_3d.items():  \n",
    "                for idx, img_slc in [(slice_idx, image.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(image.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.img_data_2d[f\"{_3d_id}{yield_2d_normal_to}{idx:03d}\"] = img_slc\n",
    "                    \n",
    "            for _3d_id, label in self.label_data_3d.items():   \n",
    "                for idx, lbl_slc in [(slice_idx, label.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(label.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.label_data_2d[f\"{_3d_id}{yield_2d_normal_to}{idx:03d}\"] = lbl_slc\n",
    "                    \n",
    "        print(\"Data import finished.\")\n",
    "        print(f\"Medmnist loader will yield {'2D' if self.yield_2d_normal_to else '3D'} samples\")\n",
    "        \n",
    "    def get_3d_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data_3d.keys())\n",
    "            .union(set(self.label_data_3d.keys()))\n",
    "        ))\n",
    "    \n",
    "    def get_2d_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data_2d.keys())\n",
    "            .union(set(self.label_data_2d.keys()))\n",
    "        ))\n",
    "    \n",
    "    def get_id_dicts(self):\n",
    "        \n",
    "        all_3d_ids = self.get_3d_ids()\n",
    "        id_dicts = []\n",
    "        \n",
    "        for _2d_dataset_idx, _2d_id in enumerate(self.get_2d_ids()):\n",
    "            _3d_id = int(_2d_id[:-4])\n",
    "            id_dicts.append(\n",
    "                {\n",
    "                    '2d_id': _2d_id,\n",
    "                    '2d_dataset_idx': _2d_dataset_idx,\n",
    "                    '3d_id': _3d_id,\n",
    "                    '3d_dataset_idx': all_3d_ids.index(_3d_id),                    \n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return id_dicts\n",
    "        \n",
    "    def __len__(self, yield_2d_override=None):\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data length\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "            \n",
    "        if yield_2d:\n",
    "            return len(self.img_data_2d)\n",
    "        \n",
    "        return len(self.img_data_3d)\n",
    "\n",
    "    def __getitem__(self, dataset_idx, yield_2d_override=None):\n",
    "        \n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data can be overridden\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "        \n",
    "        modified_label = []\n",
    "        \n",
    "        if yield_2d:\n",
    "            all_ids = self.get_2d_ids()\n",
    "            _id = all_ids[dataset_idx]\n",
    "            image = self.img_data_2d.get(_id, torch.tensor([]))\n",
    "            label = self.label_data_2d.get(_id, torch.tensor([]))\n",
    "            \n",
    "            # No paths, keep for consistency\n",
    "            image_path = \"\"\n",
    "            label_path = \"\"\n",
    "            \n",
    "        else:\n",
    "            all_ids = self.get_3d_ids()\n",
    "            _id = all_ids[dataset_idx]\n",
    "            image = self.img_data_3d.get(_id, torch.tensor([]))\n",
    "            label = self.label_data_3d.get(_id, torch.tensor([]))\n",
    "\n",
    "            # No paths, keep for consistency\n",
    "            image_path = \"\"\n",
    "            label_path = \"\"\n",
    "        \n",
    "        if self.do_train:\n",
    "            # In case of training add augmentation, modification and\n",
    "            # disturbance\n",
    "            \n",
    "            if not self.augment_at_collate:\n",
    "                b_image = image.unsqueeze(0)\n",
    "                b_label = label.unsqueeze(0)\n",
    "                # Do not augment label here (single label key for whole image -> classification)\n",
    "                b_image, _ = self.augment(b_image, b_label, yield_2d)\n",
    "                image = b_image.squeeze(0)\n",
    "                label = b_label.squeeze(0)\n",
    "            \n",
    "            modified_label = label.detach().clone()\n",
    "\n",
    "            if self.disturbed_idxs != None and dataset_idx in self.disturbed_idxs:\n",
    "                label_keys = [int(key) for key in self.mnist_set.info['label'].keys()]\n",
    "                modified_label = modified_label + 1\n",
    "                modified_label[modified_label >= max(label_keys)] = min(label_keys)\n",
    "                \n",
    "                # if yield_2d:\n",
    "                #     modified_label\n",
    "                # else:\n",
    "                #     modified_label = self.label_data_3d[_permuted_id]\n",
    "                # Permute labels of disturbed set\n",
    "                # permuted_idx = self.disturbed_permuted[dataset_idx]\n",
    "                # _permuted_id = all_ids[permuted_idx]\n",
    "                # \n",
    "                # if yield_2d:\n",
    "                #     modified_label = self.label_data_2d[_id]\n",
    "                # else:\n",
    "                #     modified_label = self.label_data_3d[_permuted_id]\n",
    "        \n",
    "        if yield_2d:\n",
    "            assert image.dim() == label.dim() == 2\n",
    "        else:\n",
    "            assert image.dim() == label.dim() == 3\n",
    "            \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'modified_label': modified_label,\n",
    "            'dataset_idx': dataset_idx, \n",
    "            'id': _id, \n",
    "            'image_path': image_path, \n",
    "            'label_path': label_path\n",
    "        }\n",
    "    \n",
    "    def get_3d_item(self, _3d_dataset_idx):\n",
    "        return self.__getitem__(_3d_dataset_idx, yield_2d_override=False)\n",
    "\n",
    "    def get_data(self):\n",
    "        img_stack = torch.stack(list(self.img_data_3d.values()), dim=0)\n",
    "        label_stack = torch.stack(list(self.label_data_3d.values()), dim=0)\n",
    "        \n",
    "        return img_stack, label_stack\n",
    "    \n",
    "    def set_disturbed_idxs(self, idxs):\n",
    "        if idxs is not None:\n",
    "            if isinstance(idxs, (np.ndarray, torch.Tensor)):\n",
    "                idxs = idxs.tolist()\n",
    "            \n",
    "            self.disturbed_idxs = idxs\n",
    "            \n",
    "            # idxs_lookup = zip(\n",
    "            #     self.disturbed_idxs, \n",
    "            #     np.random.permutation(self.disturbed_idxs).tolist()\n",
    "            # )\n",
    "            \n",
    "            # self.disturbed_permuted = {\n",
    "            #     orig_idx: permuted_idx for orig_idx, permuted_idx in idxs_lookup\n",
    "            # }\n",
    "        else:\n",
    "            self.disturbed_idxs = []\n",
    "            # self.disturbed_permuted = {}\n",
    "            \n",
    "    def train(self):\n",
    "        self.do_train = True\n",
    "        \n",
    "    def eval(self):\n",
    "        self.do_train = False\n",
    "        \n",
    "    def set_augment_at_collate(self):\n",
    "        self.augment_at_collate = True\n",
    "    \n",
    "    def unset_augment_at_collate(self):\n",
    "        self.augment_at_collate = False\n",
    "        \n",
    "    def get_efficient_augmentation_collate_fn(self):\n",
    "        yield_2d = True if self.yield_2d_normal_to else False\n",
    "        \n",
    "        def collate_closure(batch):\n",
    "            batch = torch.utils.data._utils.collate.default_collate(batch)\n",
    "            if self.augment_at_collate:\n",
    "                # Augment the whole batch not just one sample\n",
    "                b_image = batch['image'].cuda()\n",
    "                b_label = batch['label'].cuda()\n",
    "                b_image, b_label = self.augment(b_image, b_label, yield_2d)\n",
    "                batch['image'], batch['label'] = b_image.cpu(), b_label.cpu()\n",
    "\n",
    "            return batch\n",
    "        \n",
    "        return collate_closure\n",
    "    \n",
    "    def augment(self, b_image, b_label, yield_2d):\n",
    "        if yield_2d:\n",
    "            assert b_image.dim() == b_label.dim() == 3, \\\n",
    "                f\"Augmenting 2D. Input batch of image and \" \\\n",
    "                f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        else:\n",
    "            assert b_image.dim() == b_label.dim() == 4, \\\n",
    "                f\"Augmenting 3D. Input batch of image and \" \\\n",
    "                f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "\n",
    "        spatial_aug_selector = np.random.rand()\n",
    "        \n",
    "        b_image, b_label = interpolate_sample(b_image, b_label, 2., yield_2d)\n",
    "        if spatial_aug_selector < .4:\n",
    "            b_image, b_label = augmentAffine(\n",
    "                b_image, b_label, strength=0.05, yield_2d=yield_2d)\n",
    "\n",
    "        elif spatial_aug_selector <.8:\n",
    "            b_image, b_label = augmentBspline(\n",
    "                b_image, b_label, num_ctl_points=7, strength=0.005, yield_2d=yield_2d)\n",
    "        else:\n",
    "            pass\n",
    "        b_image, b_label = interpolate_sample(b_image, b_label, .5, yield_2d)\n",
    "        \n",
    "        b_image = augmentNoise(b_image, strength=0.05)\n",
    "        b_label = b_label.long()\n",
    "        \n",
    "        return b_image, b_label\n",
    "        \n",
    "    def augment_tio(self, image, label, yield_2d):\n",
    "        # Prepare dims for torchio: All transformed \n",
    "        # images / labels need to be 4-dim;\n",
    "        # 2D images need to have dims=1xHxWx1 to make transformation work\n",
    "        \n",
    "        if yield_2d:\n",
    "            assert image.dim() == label.dim() == 3, \\\n",
    "                f\"Augmenting 2D. Input batch of image and \" \\\n",
    "                f\"label should be 1xHxW but are {image.shape} and {label.shape}\"\n",
    "        else:\n",
    "            assert image.dim() == label.dim() == 4, \\\n",
    "                f\"Augmenting 3D. Input batch of image and \" \\\n",
    "                f\"label should be 1xDxHxW but are {image.shape} and {label.shape}\"\n",
    "\n",
    "        if self.yield_2d_normal_to:           \n",
    "            self.spatial_transform = tio.OneOf({\n",
    "                tio.transforms.RandomAffine(.05): 0.8,\n",
    "                tio.transforms.RandomElasticDeformation(num_control_points=7, max_displacement=(7.5,7.5,1e-5)): 0.2,\n",
    "                },\n",
    "                p=0.75,\n",
    "            )\n",
    "        else:\n",
    "            self.spatial_transform = tio.OneOf({\n",
    "                tio.transforms.RandomAffine(.05): 0.8,\n",
    "                tio.transforms.RandomElasticDeformation(num_control_points=7, max_displacement=7.5): 0.2,\n",
    "                },\n",
    "                p=0.75,\n",
    "            )\n",
    "\n",
    "        # Transforms can be composed as in torchvision.transforms\n",
    "        self.intensity_transform = tio.OneOf({\n",
    "            tio.transforms.RandomNoise(std=0.05): 0.6,\n",
    "        })\n",
    "\n",
    "        if yield_2d:\n",
    "            image = image.unsqueeze(-1)\n",
    "            label = label.unsqueeze(-1)\n",
    "\n",
    "            \n",
    "        # Run torchio transformation - LabelMap will be secured for intensity\n",
    "        # transformations\n",
    "        subject = tio.Subject(\n",
    "            image=tio.ScalarImage(tensor=image),  \n",
    "            label=tio.LabelMap(tensor=label)\n",
    "        )\n",
    "        subject = self.spatial_transform(subject)\n",
    "        # Transform image intensities apart from subject - spatial transform\n",
    "        # was not applied to label correctly if transformations are stacked.\n",
    "        image = self.intensity_transform(subject.image).data\n",
    "        image = subject.image.data\n",
    "        label = subject.label.data\n",
    "\n",
    "        if yield_2d:\n",
    "            image = image.squeeze(-1)\n",
    "            label = label.squeeze(-1)\n",
    "                \n",
    "        label = label.long()\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bb18f481-6804-48ad-8263-7f44b80cd5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    \n",
    "    'label_tags': [],\n",
    "    'use_mind': True,\n",
    "    'epochs': 120,\n",
    "    \n",
    "    'batch_size': 64,\n",
    "    'val_batch_size': 1,\n",
    "    \n",
    "    'train_set_max_len': 100,\n",
    "    'crop_w_dim_range': (24, 110),\n",
    "    'yield_2d_normal_to': \"W\",\n",
    "    \n",
    "    'lr': 0.0005,\n",
    "    'use_cosine_annealing': False,\n",
    "    \n",
    "    # Data parameter config\n",
    "    'data_parameter_config': DotDict(\n",
    "        data_param_mode=int(DataParamMode.DISABLED),\n",
    "        init_class_param=0.01,\n",
    "        lr_class_param=0.1,\n",
    "        init_inst_param=1.0,\n",
    "        lr_inst_param=0.1,\n",
    "        wd_inst_param=0.0,\n",
    "        wd_class_param=0.0,\n",
    "        skip_clamp_data_param=False,\n",
    "        clamp_sigma_min=np.log(1/20),\n",
    "        clamp_sigma_max=np.log(20),\n",
    "        optim_algorithm=int(DataParamOptim.ADAM),\n",
    "        optim_options=dict(\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    'log_every': 1,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "    \n",
    "    'do_plot': False,\n",
    "    'debug': False,\n",
    "    'wandb_mode': \"online\",\n",
    "\n",
    "    'disturbed_percentage': .0,\n",
    "    'start_disturbing_after_ep': 20e10,\n",
    "    \n",
    "    'start_dilate_kernel_sz': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9bfdd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "#     domain=\"source\", state=\"l4\", size=(128, 128, 128),\n",
    "#     ensure_labeled_pairs=True, \n",
    "#     max_load_num=config_dict['train_set_max_len'], \n",
    "#     crop_w_dim_range=config_dict['crop_w_dim_range'],\n",
    "#     yield_2d_normal_to=config_dict['yield_2d_normal_to'],\n",
    "#     dilate_kernel_sz=config_dict['start_dilate_kernel_sz'],\n",
    "# )\n",
    "# validation_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "#     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "# target_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "#     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "05bb3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/medmnist/organmnist3d.npz\n",
      "Equal image and label numbers: True\n",
      "Image shape: torch.Size([100, 28, 28, 28]), mean.: 0.00, std.: 1.00\n",
      "Label shape: torch.Size([100, 28, 28, 28]), max.: 10\n",
      "Data import finished.\n",
      "Medmnist loader will yield 2D samples\n",
      "{'python_class': 'OrganMNIST3D', 'description': 'The source of the OrganMNIST3D is the same as that of the Organ{A,C,S}MNIST. Instead of 2D images, we directly use the 3D bounding boxes and process the images into 28×28×28 to perform multi-class classification of 11 body organs. The same 115 and 16 CT scans as the Organ{A,C,S}MNIST from the source training set are used as training and validation set, respectively, and the same 70 CT scans as the Organ{A,C,S}MNIST from the source test set are treated as the test set.', 'url': 'https://zenodo.org/record/5208230/files/organmnist3d.npz?download=1', 'MD5': '21f0a239e7f502e6eca33c3fc453c0b6', 'task': 'multi-class', 'label': {'0': 'liver', '1': 'kidney-right', '2': 'kidney-left', '3': 'femur-right', '4': 'femur-left', '5': 'bladder', '6': 'heart', '7': 'lung-right', '8': 'lung-left', '9': 'spleen', '10': 'pancreas'}, 'n_channels': 1, 'n_samples': {'train': 972, 'val': 161, 'test': 610}, 'license': 'CC BY 4.0'}\n",
      "Classes:  ['liver', 'kidney-right', 'kidney-left', 'femur-right', 'femur-left', 'bladder', 'heart', 'lung-right', 'lung-left', 'spleen', 'pancreas']\n",
      "Samples:  2800\n"
     ]
    }
   ],
   "source": [
    "training_dataset = WrapperOrganMNIST3D(\n",
    "    split='train', root='./data/medmnist', download=True, normalize=True, \n",
    "    max_load_num=100, crop_w_dim_range=None,\n",
    "    disturbed_idxs=None, yield_2d_normal_to='W'\n",
    ")\n",
    "print(training_dataset.mnist_set.info)\n",
    "label_tags = list(training_dataset.mnist_set.info['label'].values())\n",
    "config_dict['label_tags'] = label_tags\n",
    "print(\"Classes: \", label_tags)\n",
    "print(\"Samples: \", len(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6f25703b-de2d-4422-82cf-249ee741cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 28, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, all_labels = training_dataset.get_data()\n",
    "print(all_labels.shape)\n",
    "# D_stack = make_2d_stack_from_3d(all_labels.unsqueeze(1), \"D\")\n",
    "# print(D_stack.shape)\n",
    "sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "plt.xlabel(\"W\")\n",
    "plt.ylabel(\"ground truth>0\")\n",
    "plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "21b1edbb-45de-4a35-8524-a4848e50bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    # Print bare 2D data\n",
    "    # print(\"Displaying 2D bare sample\")\n",
    "    # for img, label in zip(training_dataset.img_data_2d.values(), \n",
    "    #                       training_dataset.label_data_2d.values()):\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=img.unsqueeze(0), \n",
    "    #                 ground_truth=label,\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "        \n",
    "    # Print transformed 2D data\n",
    "    # train_subset = torch.utils.data.Subset(training_dataset,range(2))\n",
    "    # training_dataset.train()\n",
    "    # print(\"Displaying 2D training sample\")\n",
    "    # for sample in train_subset:\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=sample['image'].unsqueeze(0), \n",
    "    #                 ground_truth=sample['label'],\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .0)\n",
    "    \n",
    "    # Print transformed 3D data\n",
    "    training_dataset.train()\n",
    "    print(\"Displaying 3D training sample\")\n",
    "    leng = 10# training_dataset.__len__(yield_2d_override=False)\n",
    "    for idx in range(leng):\n",
    "        # training_dataset.set_dilate_kernel_size(1)\n",
    "        sample = training_dataset.get_3d_item(idx)\n",
    "        print(f\"Sample {idx} ###\")\n",
    "        print(\"Unmodified sample\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample['image'].unsqueeze(0), \n",
    "                    ground_truth=sample['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "        print(sample['label'].unique().item())\n",
    "        \n",
    "        print(\"Modified sample\")\n",
    "        # training_dataset.set_dilate_kernel_size(7)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample['image'].unsqueeze(0), \n",
    "                    ground_truth=sample['modified_label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "        print(sample['modified_label'].unique().item())\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4ad65dae-5a01-4b13-af12-8422b730f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    for sidx in [1,]:\n",
    "        print(f\"Sample {sidx}:\")\n",
    "        \n",
    "        training_dataset.eval()\n",
    "        sample_eval = training_dataset.get_crossmoda_3d_item(sidx)\n",
    "\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=sample_eval['image'].unsqueeze(0), \n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "        \n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=sample_eval['image'].unsqueeze(0), \n",
    "                    ground_truth=sample_eval['label'], \n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "\n",
    "        training_dataset.train()\n",
    "        print(\"Train sample with ground-truth overlay\")\n",
    "        sample_train = training_dataset.get_crossmoda_3d_item(sidx)\n",
    "        print(sample_train['label'].unique())\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=sample_train['image'].unsqueeze(0), \n",
    "                    ground_truth=sample_train['label'], \n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt=.3)\n",
    "\n",
    "        print(\"Eval/train diff with diff overlay\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=(sample_eval['image'] - sample_train['image']).unsqueeze(0), \n",
    "                    ground_truth=(sample_eval['label'] - sample_train['label']).clamp(min=0), \n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0f7ed00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_subset = torch.utils.data.Subset(training_dataset,range(2))\n",
    "if config_dict['do_plot']:\n",
    "    train_plotset = (training_dataset.get_crossmoda_3d_item(idx) for idx in (55, 81, 63))\n",
    "    for sample in train_plotset:\n",
    "        print(f\"Sample {sample['dataset_idx']}:\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "            img=sample_eval['image'].unsqueeze(0), \n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .6)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "            img=sample_eval['image'].unsqueeze(0), \n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1eb958f2-0fee-4ee3-b108-2a203bb65f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "\n",
    "import functools\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use get_named_layers_leaves(module) to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "24d4986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(lraspp, optimizer, scaler, _path):\n",
    "    \n",
    "    torch.save(lraspp.state_dict(), _path + '_lraspp.pth')    \n",
    "    torch.save(optimizer.state_dict(), _path + '_optimizer.pth')\n",
    "    torch.save(scaler.state_dict(), _path + '_grad_scaler.pth')\n",
    "\n",
    "    # TODO add saving inst/class parameters again\n",
    "    # torch.save(inst_parameters, _path + '_inst_parameters.pth')\n",
    "    # torch.save(class_parameters, _path + '_class_parameters.pth')\n",
    "\n",
    "    \n",
    "    \n",
    "def get_model(config, dataset_len, _path=None):\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(\n",
    "        pretrained=False, progress=True, num_classes=len(config.label_tags)\n",
    "    )\n",
    "    set_module(lraspp, 'backbone.0.0', \n",
    "               torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2), \n",
    "                               padding=(1, 1), bias=False)\n",
    "    )\n",
    "        \n",
    "    optimizer = torch.optim.Adam(lraspp.parameters(), lr=config.lr)\n",
    "    \n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    if _path and os.path.isfile(_path + '_lraspp.pth'):\n",
    "        print(f\"Loading lr-aspp model, optimizers and grad scalers from {_path}\")\n",
    "        # TODO add loading of data parameters\n",
    "        lraspp.load_state_dict(torch.load(_path + '_lraspp.pth', map_location='cuda'))\n",
    "\n",
    "        optimizer.load_state_dict(torch.load(_path + '_optimizer.pth', map_location='cuda'))\n",
    "        scaler.load_state_dict(torch.load(_path + '_grad_scaler.pth', map_location='cuda'))\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "        \n",
    "    return (lraspp, optimizer, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cabd4c44-6225-4ed6-bcdd-6b9fc6b30034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "        \n",
    "    \n",
    "            \n",
    "def log_data_parameters(log_path, parameter_idxs, parameters):\n",
    "    data = [[idx, param] for (idx, param) in \\\n",
    "        zip(parameter_idxs, torch.exp(parameters).tolist())]\n",
    "\n",
    "    table = wandb.Table(data=data, columns = [\"parameter_idx\", \"value\"])\n",
    "    wandb.log({log_path:wandb.plot.bar(table, \"parameter_idx\", \"value\", title=log_path)})\n",
    "    \n",
    "\n",
    "    \n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "    \n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "    \n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "    \n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "    \n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio\n",
    "    \n",
    "    \n",
    "    \n",
    "# def get_largest_data_parameters_in_target_ratio(parameter_idxs, parameters, target_idxs):\n",
    "#     # print(\"param_idxs\", parameter_idxs)\n",
    "#     # print(\"parameters\", parameters)\n",
    "#     # print(\"target_idxs\", target_idxs)\n",
    "#     data = [[inst_idx, val] for (inst_idx, val) in \\\n",
    "#         zip(parameter_idxs, torch.exp(parameters).tolist())]\n",
    "    \n",
    "#     topk_cont_idxs = torch.argsort(parameters, descending=True)[:len(target_idxs)]\n",
    "#     # print(\"topk_cont_idxs\", topk_cont_idxs)\n",
    "#     topk_dataset_idxs = parameter_idxs[topk_cont_idxs]\n",
    "#     # print(\"topk_dataset_idxs\", topk_dataset_idxs)\n",
    "#     ratio = np.sum(np.in1d(topk_dataset_idxs, target_idxs))/len(target_idxs)\n",
    "    \n",
    "#     return ratio\n",
    "\n",
    "\n",
    "\n",
    "def log_data_parameter_stats(log_path, epx, data_parameters):\n",
    "    \"\"\"Log stats for data parameters on wandb.\"\"\"\n",
    "    data_parameters = data_parameters.exp()\n",
    "    wandb.log({f'{log_path}/highest': torch.max(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/lowest': torch.min(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/mean': torch.mean(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/std': torch.std(data_parameters).item()}, step=epx)\n",
    "\n",
    "    \n",
    "    \n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    \n",
    "    \n",
    "def log_class_dices(log_prefix, log_postfix, class_dices, log_idx):\n",
    "    for cls_name in class_dices[0].keys():\n",
    "        log_path = f\"{log_prefix}{cls_name}{log_postfix}\"\n",
    "\n",
    "        cls_dices = list(map(lambda dct: dct[cls_name], class_dices))\n",
    "        mean_per_class =np.nanmean(cls_dices)\n",
    "        print(log_path, f\"{mean_per_class*100:.2f}%\")\n",
    "        wandb.log({log_path: mean_per_class}, step=log_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6e9d47e9-fe55-4241-86f1-796e6286e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "    \n",
    "    fold_iter = enumerate(kf.split(training_dataset))\n",
    "\n",
    "    if config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "    \n",
    "    if config.wandb_mode != 'disabled':\n",
    "        # Log dataset info\n",
    "        training_dataset.eval()\n",
    "        dataset_info = [[smp['dataset_idx'], smp['_id'], smp['image_path'], smp['label_path']] \\\n",
    "                        for smp in training_dataset]\n",
    "        wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', '_id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    " \n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        \n",
    "        # Training happens in 2D, validation happens in 3D:\n",
    "        # Read 2D dataset idxs which are used for training, \n",
    "        # get their 3D super-ids by 3d dataset length\n",
    "        # and substract these from all 3D ids to get val_3d_idxs\n",
    "        trained_3d_dataset_idxs = {dct['3d_dataset_idx'] \\\n",
    "             for dct in training_dataset.get_id_dicts() if dct['2d_dataset_idx'] in train_idxs.tolist()}\n",
    "        val_3d_idxs = set(range(training_dataset.__len__(yield_2d_override=False))) - trained_3d_dataset_idxs\n",
    "        print(\"Will run validation with these 3D samples:\", val_3d_idxs)\n",
    "        \n",
    "        ### Disturb dataset ###\n",
    "        disturbed_idxs = np.random.choice(train_idxs, size=int(len(train_idxs)*config.disturbed_percentage), replace=False)\n",
    "        disturbed_idxs = torch.tensor(disturbed_idxs)\n",
    "        \n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(disturbed_idxs.tolist()))\n",
    "        \n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in disturbed_idxs])}, \n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "        \n",
    "        ### Visualization ###\n",
    "        if config.do_plot:\n",
    "            print(\"Disturbed samples:\")\n",
    "            for d_idx in disturbed_idxs:\n",
    "                display_seg(in_type=\"single_3D\", reduce_dim=\"W\", \n",
    "                    img=training_dataset[d_idx][0], \n",
    "                    ground_truth=disturb_seg(training_dataset[d_idx][1]),\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "        \n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels =12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "            \n",
    "        _, all_segs = training_dataset.get_data()\n",
    "\n",
    "        # TODO add class weights again\n",
    "        # class_weight = torch.sqrt(1.0/(torch.bincount(all_segs.long().view(-1)).float()))\n",
    "        # class_weight = class_weight/class_weight.mean()\n",
    "        # class_weight[0] = 0.15\n",
    "        # class_weight = class_weight.cuda()\n",
    "        # print('inv sqrt class_weight', class_weight)\n",
    "        \n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size, \n",
    "            sampler=train_subsampler, pin_memory=True, drop_last=False,\n",
    "            collate_fn=training_dataset.get_efficient_augmentation_collate_fn())\n",
    "        training_dataset.unset_augment_at_collate()\n",
    "#         val_dataloader = DataLoader(training_dataset, batch_size=config.val_batch_size, \n",
    "#                                     sampler=val_subsampler, pin_memory=True, drop_last=False)\n",
    "      \n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        (lraspp, optimizer, scaler) = get_model(config, len(train_dataloader), _path=None)#f\"{config.mdl_save_prefix}_fold{fold_idx}\") # TODO start fresh set _path None\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=200, T_mult=2, eta_min=config.lr*.1, last_epoch=- 1, verbose=False)\n",
    "        \n",
    "        lraspp.cuda()\n",
    "        dpm = DataParameterManager(train_idxs.tolist(), config.label_tags, \n",
    "            config=config.data_parameter_config, device='cuda')\n",
    "        \n",
    "        # criterion = nn.CrossEntropyLoss(class_weight)\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        for epx in range(config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "        \n",
    "            lraspp.train()\n",
    "            training_dataset.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            do_disturb = epx > config.start_disturbing_after_ep\n",
    "            wandb.log({\"do_disturb\": float(do_disturb)}, step=global_idx)\n",
    "\n",
    "            if do_disturb:\n",
    "                training_dataset.set_disturbed_idxs(disturbed_idxs)\n",
    "            else:\n",
    "                training_dataset.set_disturbed_idxs([])\n",
    "                \n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            class_dices = []\n",
    "            \n",
    "            # Load data\n",
    "            for batch in train_dataloader:\n",
    "                \n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "\n",
    "                b_img = b_img.float().cuda()\n",
    "                b_seg_modified = b_seg_modified.cuda()\n",
    "        \n",
    "                b_seg = b_seg.cuda()\n",
    "    \n",
    "                if config.use_mind:\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=True):\n",
    "                    assert b_img.dim() == 4, \\\n",
    "                        f\"Input image for model must be 4D: BxCxHxW but is {b_img.shape}\"\n",
    "                    \n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    ### Check dimensions ###\n",
    "                    assert logits.dim() == 4, \\\n",
    "                        f\"Logits shape must be BxNUM_CLASSESxHxW but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == 3, \\\n",
    "                        f\"Target shape for loss must be BxHxW but is {b_seg_modified.shape}\"\n",
    "\n",
    "                    dp_logits, loss_value = dpm.do_basic_train_step(\n",
    "                        criterion, logits.permute(0,2,3,1), \n",
    "                        torch.nn.functional.one_hot(b_seg_modified, len(config.label_tags)), \n",
    "                        optimizer, inst_keys=b_idxs_dataset.tolist(), scaler=scaler)\n",
    "    \n",
    "                epx_losses.append(loss_value)\n",
    "                \n",
    "                # Prepare logits for scoring\n",
    "                logits_for_score = logits.argmax(1) # TODO Check logits ok or dp_logits instead?\n",
    "\n",
    "                # Calculate dice score\n",
    "                b_dice = dice2d(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, len(config.label_tags)),\n",
    "                    torch.nn.functional.one_hot(b_seg, len(config.label_tags)), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_over_all(\n",
    "                    b_dice, exclude_bg=False))\n",
    "                class_dices.append(get_batch_dice_per_class(\n",
    "                    b_dice, config.label_tags, exclude_bg=False))\n",
    "                \n",
    "                if config.do_plot:\n",
    "                    print(\"Training 2D stack image label/ground-truth\")\n",
    "                    print(b_dice)\n",
    "                    \n",
    "                    display_seg(in_type=\"batch_2D\", \n",
    "                        img=batch['image'].unsqueeze(1).cpu(), \n",
    "                        seg=logits_for_score.cpu(),\n",
    "                        ground_truth=b_seg.cpu(),\n",
    "                        crop_to_non_zero_seg=True,\n",
    "                        crop_to_non_zero_gt=True,\n",
    "                        alpha_seg=.1,\n",
    "                        alpha_gt =.2\n",
    "                    )\n",
    "                    \n",
    "                if config.debug:\n",
    "                    break\n",
    "                    \n",
    "                ###  Scheduler management ###\n",
    "                if config.use_cosine_annealing:\n",
    "                    scheduler.step()\n",
    "\n",
    "                # if scheduler.T_cur == 0:\n",
    "                #     sz = training_dataset.get_dilate_kernel_size()\n",
    "                #     training_dataset.set_dilate_kernel_size(sz-1)\n",
    "                #     print(f\"Current dilate kernel size is {training_dataset.get_dilate_kernel_size()}.\")\n",
    "\n",
    "            ### Logging ###\n",
    "            if epx % config.log_every == 0 or (epx+1 == config.epochs):\n",
    "                \n",
    "                print(f\"### Log epoch {epx} @ {time.time()-t0:.2f}\")\n",
    "                print(\"### Training\")\n",
    "                ### Log wandb data ###\n",
    "                # Log the epoch idx per fold - so we can recover the diagram by setting \n",
    "                # ref_epoch_idx as x-axis in wandb interface\n",
    "                wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "                \n",
    "                mean_loss = torch.tensor(epx_losses).mean()\n",
    "                wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "                \n",
    "                mean_dice = np.nanmean(dices)\n",
    "                print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "                                   \n",
    "                log_class_dices(\"scores/dice_mean_\", f\"_fold{fold_idx}\", class_dices, global_idx)\n",
    "            \n",
    "                if (not dpm.disabled) and (dpm.data_param_mode == DataParamMode.ONLY_INSTANCE_PARAMS):\n",
    "                    # Log instance parameters of all samples\n",
    "                    log_data_parameter_stats(f\"data_parameters/instances/all_fold{fold_idx}\", global_idx, \n",
    "                            dpm.get_parameter_tensor(inst_keys='all').detach()\n",
    "                    )\n",
    "\n",
    "                    # Log instance parameters of disturbed samples\n",
    "                    if disturbed_idxs.numel() > 0:\n",
    "                        param_dict = dpm.get_data_parameters_dict()\n",
    "\n",
    "                        log_data_parameter_stats(f\"data_parameters/instances/disturbed_fold{fold_idx}\", \n",
    "                            global_idx, \n",
    "                            dpm.get_parameter_tensor(inst_keys=disturbed_idxs.tolist()).detach()\n",
    "                        )\n",
    "\n",
    "                        log_data_parameter_stats(f\"data_parameters/instances/clean_fold{fold_idx}\", \n",
    "                            global_idx, \n",
    "                            dpm.get_parameter_tensor(inst_keys=clean_idxs.tolist()).detach()\n",
    "                        )\n",
    "\n",
    "                        # Calculate ratio of data parameters in topN disturbed (is 1.0 if every disturbed\n",
    "                        # sample gets the highest data parameter)\n",
    "                        min_param_ratio = calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_idxs.tolist(), 'min')\n",
    "                        max_param_ratio = calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_idxs.tolist(), 'max')\n",
    "         \n",
    "                        print(f'inst_param_ratio_min_fold{fold_idx}', min_param_ratio)\n",
    "                        print(f'inst_param_ratio_max_fold{fold_idx}', max_param_ratio)\n",
    "                        wandb.log(\n",
    "                            {f'data_parameters/inst_param_ratio_min_fold{fold_idx}': min_param_ratio}, \n",
    "                            step=global_idx\n",
    "                        )\n",
    "                        wandb.log(\n",
    "                            {f'data_parameters/inst_param_ratio_max_fold{fold_idx}': max_param_ratio}, \n",
    "                            step=global_idx\n",
    "                        )\n",
    "                        \n",
    "                print()\n",
    "                print(\"### Validation\")                                    \n",
    "                lraspp.eval()\n",
    "                training_dataset.eval()   \n",
    "                # TODO remove saving \n",
    "                # save_model(lraspp, inst_parameters, class_parameters, \n",
    "                #     optimizer, optimizer_inst_param, optimizer_class_param, \n",
    "                #     scaler, f\"{config.mdl_save_prefix}_fold{fold_idx}\")\n",
    "                \n",
    "                with amp.autocast(enabled=True):\n",
    "                    with torch.no_grad():\n",
    "                        val_dices = []\n",
    "                        val_class_dices = []\n",
    "                        \n",
    "                        for val_idx in val_3d_idxs:\n",
    "                            val_sample = training_dataset.get_3d_item(val_idx)\n",
    "                            stack_dim = training_dataset.yield_2d_normal_to\n",
    "                            # Create batch out of single val sample\n",
    "                            b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                            b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "                            \n",
    "                            B = b_val_img.shape[0]\n",
    "\n",
    "                            b_val_img = b_val_img.unsqueeze(1).float().cuda()\n",
    "                            b_val_seg = b_val_seg.cuda()\n",
    "                            b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=stack_dim)\n",
    "                            \n",
    "                            if config.use_mind:\n",
    "                                b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "                                \n",
    "                            output_val = lraspp(b_val_img_2d)['out']\n",
    "\n",
    "                            # Prepare logits for scoring\n",
    "                            # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                            val_logits_for_score = make_3d_from_2d_stack(output_val, stack_dim, B)\n",
    "                            val_logits_for_score = val_logits_for_score.argmax(1)\n",
    "\n",
    "                            b_val_dice = dice3d(\n",
    "                                torch.nn.functional.one_hot(val_logits_for_score, len(config.label_tags)),\n",
    "                                torch.nn.functional.one_hot(b_val_seg, len(config.label_tags)), \n",
    "                                one_hot_torch_style=True\n",
    "                            )\n",
    "                            \n",
    "                            # Get mean score over batch\n",
    "                            val_dices.append(get_batch_dice_over_all(\n",
    "                                b_val_dice, exclude_bg=False))\n",
    "                            \n",
    "                            val_class_dices.append(get_batch_dice_per_class(\n",
    "                                b_val_dice, config.label_tags, exclude_bg=False))\n",
    "                \n",
    "                            if config.do_plot:\n",
    "                                print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                                print(b_val_dice)\n",
    "                                # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                                display_seg(in_type=\"single_3D\", \n",
    "                                    reduce_dim=\"W\",\n",
    "                                    img=val_sample['image'].unsqueeze(0).cpu(), \n",
    "                                    seg=val_logits_for_score.squeeze(0).cpu(),\n",
    "                                    ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                    crop_to_non_zero_seg=True,\n",
    "                                    crop_to_non_zero_gt=True,\n",
    "                                    alpha_seg=.4,\n",
    "                                    alpha_gt=.2\n",
    "                                )\n",
    "                        mean_val_dice = np.nanmean(val_dices)\n",
    "                        print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                        wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                    \n",
    "                        log_class_dices(\"scores/val_dice_mean_\", f\"_fold{fold_idx}\", val_class_dices, global_idx)\n",
    "                  \n",
    "                print()\n",
    "                # End of logging\n",
    "                \n",
    "            if config.debug:\n",
    "                break\n",
    "        \n",
    "        # End of fold loop\n",
    "        if dpm.data_param_mode == DataParamMode.ONLY_INSTANCE_PARAMS:\n",
    "            inst_parameters = dpm.get_parameter_tensor(inst_keys=train_idxs.tolist()).detach() # TODO simplify logging\n",
    "            log_data_parameters(f\"data_parameters/instance_parameters_fold{fold_idx}\", train_idxs, inst_parameters)\n",
    "\n",
    "        lraspp.cpu()\n",
    "        \n",
    "        save_model(lraspp,optimizer, scaler, f\"{config.mdl_save_prefix}_fold{fold_idx}\") # TODO save data parameter manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e6f476a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n",
      "Will run validation with these 3D samples: {0, 1, 2, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39}\n",
      "Disturbed indexes: []\n",
      "Generating fresh lr-aspp model, optimizer and grad scaler.\n",
      "### Log epoch 0 @ 12.18\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 9.96%\n",
      "scores/dice_mean_liver_fold0 9.96%\n",
      "scores/dice_mean_kidney-right_fold0 8.91%\n",
      "scores/dice_mean_kidney-left_fold0 8.94%\n",
      "scores/dice_mean_femur-right_fold0 9.54%\n",
      "scores/dice_mean_femur-left_fold0 10.56%\n",
      "scores/dice_mean_bladder_fold0 10.26%\n",
      "scores/dice_mean_heart_fold0 9.50%\n",
      "scores/dice_mean_lung-right_fold0 10.49%\n",
      "scores/dice_mean_lung-left_fold0 12.99%\n",
      "scores/dice_mean_spleen_fold0 15.11%\n",
      "scores/dice_mean_pancreas_fold0 22.49%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 5.18%\n",
      "scores/val_dice_mean_liver_fold0 5.18%\n",
      "scores/val_dice_mean_kidney-right_fold0 6.65%\n",
      "scores/val_dice_mean_kidney-left_fold0 0.20%\n",
      "scores/val_dice_mean_femur-right_fold0 0.00%\n",
      "scores/val_dice_mean_femur-left_fold0 0.00%\n",
      "scores/val_dice_mean_bladder_fold0 5.05%\n",
      "scores/val_dice_mean_heart_fold0 1.26%\n",
      "scores/val_dice_mean_lung-right_fold0 0.00%\n",
      "scores/val_dice_mean_lung-left_fold0 5.62%\n",
      "scores/val_dice_mean_spleen_fold0 7.76%\n",
      "scores/val_dice_mean_pancreas_fold0 19.25%\n",
      "\n",
      "### Log epoch 1 @ 24.29\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 35.97%\n",
      "scores/dice_mean_liver_fold0 35.97%\n",
      "scores/dice_mean_kidney-right_fold0 32.91%\n",
      "scores/dice_mean_kidney-left_fold0 34.28%\n",
      "scores/dice_mean_femur-right_fold0 36.38%\n",
      "scores/dice_mean_femur-left_fold0 37.69%\n",
      "scores/dice_mean_bladder_fold0 35.43%\n",
      "scores/dice_mean_heart_fold0 29.33%\n",
      "scores/dice_mean_lung-right_fold0 31.79%\n",
      "scores/dice_mean_lung-left_fold0 34.34%\n",
      "scores/dice_mean_spleen_fold0 40.38%\n",
      "scores/dice_mean_pancreas_fold0 56.56%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 3.79%\n",
      "scores/val_dice_mean_liver_fold0 1.99%\n",
      "scores/val_dice_mean_kidney-right_fold0 3.25%\n",
      "scores/val_dice_mean_kidney-left_fold0 0.94%\n",
      "scores/val_dice_mean_femur-right_fold0 2.04%\n",
      "scores/val_dice_mean_femur-left_fold0 4.74%\n",
      "scores/val_dice_mean_bladder_fold0 5.90%\n",
      "scores/val_dice_mean_heart_fold0 0.00%\n",
      "scores/val_dice_mean_lung-right_fold0 0.86%\n",
      "scores/val_dice_mean_lung-left_fold0 2.56%\n",
      "scores/val_dice_mean_spleen_fold0 2.99%\n",
      "scores/val_dice_mean_pancreas_fold0 10.53%\n",
      "\n",
      "### Log epoch 2 @ 36.84\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 50.84%\n",
      "scores/dice_mean_liver_fold0 50.92%\n",
      "scores/dice_mean_kidney-right_fold0 47.80%\n",
      "scores/dice_mean_kidney-left_fold0 48.27%\n",
      "scores/dice_mean_femur-right_fold0 50.63%\n",
      "scores/dice_mean_femur-left_fold0 52.18%\n",
      "scores/dice_mean_bladder_fold0 50.06%\n",
      "scores/dice_mean_heart_fold0 45.28%\n",
      "scores/dice_mean_lung-right_fold0 45.58%\n",
      "scores/dice_mean_lung-left_fold0 48.29%\n",
      "scores/dice_mean_spleen_fold0 49.78%\n",
      "scores/dice_mean_pancreas_fold0 59.81%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 10.12%\n",
      "scores/val_dice_mean_liver_fold0 18.85%\n",
      "scores/val_dice_mean_kidney-right_fold0 0.04%\n",
      "scores/val_dice_mean_kidney-left_fold0 4.57%\n",
      "scores/val_dice_mean_femur-right_fold0 3.00%\n",
      "scores/val_dice_mean_femur-left_fold0 9.57%\n",
      "scores/val_dice_mean_bladder_fold0 14.52%\n",
      "scores/val_dice_mean_heart_fold0 0.00%\n",
      "scores/val_dice_mean_lung-right_fold0 0.00%\n",
      "scores/val_dice_mean_lung-left_fold0 0.79%\n",
      "scores/val_dice_mean_spleen_fold0 6.63%\n",
      "scores/val_dice_mean_pancreas_fold0 25.99%\n",
      "\n",
      "### Log epoch 3 @ 48.87\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 64.15%\n",
      "scores/dice_mean_liver_fold0 64.27%\n",
      "scores/dice_mean_kidney-right_fold0 62.10%\n",
      "scores/dice_mean_kidney-left_fold0 62.01%\n",
      "scores/dice_mean_femur-right_fold0 64.16%\n",
      "scores/dice_mean_femur-left_fold0 64.45%\n",
      "scores/dice_mean_bladder_fold0 62.46%\n",
      "scores/dice_mean_heart_fold0 59.62%\n",
      "scores/dice_mean_lung-right_fold0 58.46%\n",
      "scores/dice_mean_lung-left_fold0 60.60%\n",
      "scores/dice_mean_spleen_fold0 63.32%\n",
      "scores/dice_mean_pancreas_fold0 73.67%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 11.06%\n",
      "scores/val_dice_mean_liver_fold0 10.33%\n",
      "scores/val_dice_mean_kidney-right_fold0 3.95%\n",
      "scores/val_dice_mean_kidney-left_fold0 5.20%\n",
      "scores/val_dice_mean_femur-right_fold0 1.42%\n",
      "scores/val_dice_mean_femur-left_fold0 11.88%\n",
      "scores/val_dice_mean_bladder_fold0 13.32%\n",
      "scores/val_dice_mean_heart_fold0 0.00%\n",
      "scores/val_dice_mean_lung-right_fold0 40.92%\n",
      "scores/val_dice_mean_lung-left_fold0 4.74%\n",
      "scores/val_dice_mean_spleen_fold0 11.13%\n",
      "scores/val_dice_mean_pancreas_fold0 24.32%\n",
      "\n",
      "### Log epoch 4 @ 61.20\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 69.47%\n",
      "scores/dice_mean_liver_fold0 69.33%\n",
      "scores/dice_mean_kidney-right_fold0 67.27%\n",
      "scores/dice_mean_kidney-left_fold0 66.97%\n",
      "scores/dice_mean_femur-right_fold0 68.15%\n",
      "scores/dice_mean_femur-left_fold0 69.84%\n",
      "scores/dice_mean_bladder_fold0 68.32%\n",
      "scores/dice_mean_heart_fold0 63.82%\n",
      "scores/dice_mean_lung-right_fold0 60.73%\n",
      "scores/dice_mean_lung-left_fold0 63.43%\n",
      "scores/dice_mean_spleen_fold0 63.62%\n",
      "scores/dice_mean_pancreas_fold0 69.75%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 10.15%\n",
      "scores/val_dice_mean_liver_fold0 9.80%\n",
      "scores/val_dice_mean_kidney-right_fold0 5.46%\n",
      "scores/val_dice_mean_kidney-left_fold0 6.90%\n",
      "scores/val_dice_mean_femur-right_fold0 9.02%\n",
      "scores/val_dice_mean_femur-left_fold0 12.96%\n",
      "scores/val_dice_mean_bladder_fold0 13.90%\n",
      "scores/val_dice_mean_heart_fold0 4.97%\n",
      "scores/val_dice_mean_lung-right_fold0 36.47%\n",
      "scores/val_dice_mean_lung-left_fold0 12.34%\n",
      "scores/val_dice_mean_spleen_fold0 11.77%\n",
      "scores/val_dice_mean_pancreas_fold0 27.48%\n",
      "\n",
      "### Log epoch 5 @ 73.40\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 76.73%\n",
      "scores/dice_mean_liver_fold0 76.73%\n",
      "scores/dice_mean_kidney-right_fold0 75.25%\n",
      "scores/dice_mean_kidney-left_fold0 76.24%\n",
      "scores/dice_mean_femur-right_fold0 77.41%\n",
      "scores/dice_mean_femur-left_fold0 77.48%\n",
      "scores/dice_mean_bladder_fold0 76.48%\n",
      "scores/dice_mean_heart_fold0 75.71%\n",
      "scores/dice_mean_lung-right_fold0 72.92%\n",
      "scores/dice_mean_lung-left_fold0 74.73%\n",
      "scores/dice_mean_spleen_fold0 74.87%\n",
      "scores/dice_mean_pancreas_fold0 80.84%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 10.71%\n",
      "scores/val_dice_mean_liver_fold0 6.24%\n",
      "scores/val_dice_mean_kidney-right_fold0 6.83%\n",
      "scores/val_dice_mean_kidney-left_fold0 8.01%\n",
      "scores/val_dice_mean_femur-right_fold0 6.94%\n",
      "scores/val_dice_mean_femur-left_fold0 11.85%\n",
      "scores/val_dice_mean_bladder_fold0 9.91%\n",
      "scores/val_dice_mean_heart_fold0 7.15%\n",
      "scores/val_dice_mean_lung-right_fold0 11.32%\n",
      "scores/val_dice_mean_lung-left_fold0 7.72%\n",
      "scores/val_dice_mean_spleen_fold0 10.01%\n",
      "scores/val_dice_mean_pancreas_fold0 27.44%\n",
      "\n",
      "### Log epoch 6 @ 85.58\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 82.54%\n",
      "scores/dice_mean_liver_fold0 82.39%\n",
      "scores/dice_mean_kidney-right_fold0 81.53%\n",
      "scores/dice_mean_kidney-left_fold0 79.99%\n",
      "scores/dice_mean_femur-right_fold0 81.19%\n",
      "scores/dice_mean_femur-left_fold0 82.67%\n",
      "scores/dice_mean_bladder_fold0 80.96%\n",
      "scores/dice_mean_heart_fold0 76.14%\n",
      "scores/dice_mean_lung-right_fold0 80.35%\n",
      "scores/dice_mean_lung-left_fold0 77.01%\n",
      "scores/dice_mean_spleen_fold0 77.50%\n",
      "scores/dice_mean_pancreas_fold0 77.44%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 7.49%\n",
      "scores/val_dice_mean_liver_fold0 7.32%\n",
      "scores/val_dice_mean_kidney-right_fold0 8.84%\n",
      "scores/val_dice_mean_kidney-left_fold0 3.45%\n",
      "scores/val_dice_mean_femur-right_fold0 6.46%\n",
      "scores/val_dice_mean_femur-left_fold0 5.57%\n",
      "scores/val_dice_mean_bladder_fold0 6.77%\n",
      "scores/val_dice_mean_heart_fold0 7.11%\n",
      "scores/val_dice_mean_lung-right_fold0 8.55%\n",
      "scores/val_dice_mean_lung-left_fold0 7.66%\n",
      "scores/val_dice_mean_spleen_fold0 6.73%\n",
      "scores/val_dice_mean_pancreas_fold0 15.25%\n",
      "\n",
      "### Log epoch 7 @ 97.48\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 82.51%\n",
      "scores/dice_mean_liver_fold0 82.21%\n",
      "scores/dice_mean_kidney-right_fold0 81.08%\n",
      "scores/dice_mean_kidney-left_fold0 80.48%\n",
      "scores/dice_mean_femur-right_fold0 80.43%\n",
      "scores/dice_mean_femur-left_fold0 81.85%\n",
      "scores/dice_mean_bladder_fold0 79.95%\n",
      "scores/dice_mean_heart_fold0 78.02%\n",
      "scores/dice_mean_lung-right_fold0 78.85%\n",
      "scores/dice_mean_lung-left_fold0 79.65%\n",
      "scores/dice_mean_spleen_fold0 79.10%\n",
      "scores/dice_mean_pancreas_fold0 84.10%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 10.35%\n",
      "scores/val_dice_mean_liver_fold0 8.49%\n",
      "scores/val_dice_mean_kidney-right_fold0 11.81%\n",
      "scores/val_dice_mean_kidney-left_fold0 8.80%\n",
      "scores/val_dice_mean_femur-right_fold0 7.49%\n",
      "scores/val_dice_mean_femur-left_fold0 10.27%\n",
      "scores/val_dice_mean_bladder_fold0 11.21%\n",
      "scores/val_dice_mean_heart_fold0 6.44%\n",
      "scores/val_dice_mean_lung-right_fold0 12.49%\n",
      "scores/val_dice_mean_lung-left_fold0 7.16%\n",
      "scores/val_dice_mean_spleen_fold0 12.54%\n",
      "scores/val_dice_mean_pancreas_fold0 28.26%\n",
      "\n",
      "### Log epoch 8 @ 109.51\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 86.63%\n",
      "scores/dice_mean_liver_fold0 86.45%\n",
      "scores/dice_mean_kidney-right_fold0 85.21%\n",
      "scores/dice_mean_kidney-left_fold0 84.32%\n",
      "scores/dice_mean_femur-right_fold0 87.05%\n",
      "scores/dice_mean_femur-left_fold0 87.00%\n",
      "scores/dice_mean_bladder_fold0 85.39%\n",
      "scores/dice_mean_heart_fold0 87.08%\n",
      "scores/dice_mean_lung-right_fold0 83.93%\n",
      "scores/dice_mean_lung-left_fold0 83.70%\n",
      "scores/dice_mean_spleen_fold0 84.63%\n",
      "scores/dice_mean_pancreas_fold0 90.79%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 15.54%\n",
      "scores/val_dice_mean_liver_fold0 11.30%\n",
      "scores/val_dice_mean_kidney-right_fold0 8.32%\n",
      "scores/val_dice_mean_kidney-left_fold0 6.24%\n",
      "scores/val_dice_mean_femur-right_fold0 5.59%\n",
      "scores/val_dice_mean_femur-left_fold0 14.93%\n",
      "scores/val_dice_mean_bladder_fold0 13.51%\n",
      "scores/val_dice_mean_heart_fold0 14.18%\n",
      "scores/val_dice_mean_lung-right_fold0 27.68%\n",
      "scores/val_dice_mean_lung-left_fold0 15.45%\n",
      "scores/val_dice_mean_spleen_fold0 11.46%\n",
      "scores/val_dice_mean_pancreas_fold0 33.00%\n",
      "\n",
      "### Log epoch 9 @ 121.38\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 85.30%\n",
      "scores/dice_mean_liver_fold0 85.20%\n",
      "scores/dice_mean_kidney-right_fold0 84.41%\n",
      "scores/dice_mean_kidney-left_fold0 82.94%\n",
      "scores/dice_mean_femur-right_fold0 84.43%\n",
      "scores/dice_mean_femur-left_fold0 84.33%\n",
      "scores/dice_mean_bladder_fold0 84.42%\n",
      "scores/dice_mean_heart_fold0 83.57%\n",
      "scores/dice_mean_lung-right_fold0 82.08%\n",
      "scores/dice_mean_lung-left_fold0 82.38%\n",
      "scores/dice_mean_spleen_fold0 80.47%\n",
      "scores/dice_mean_pancreas_fold0 82.57%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 9.95%\n",
      "scores/val_dice_mean_liver_fold0 6.11%\n",
      "scores/val_dice_mean_kidney-right_fold0 6.66%\n",
      "scores/val_dice_mean_kidney-left_fold0 6.94%\n",
      "scores/val_dice_mean_femur-right_fold0 3.95%\n",
      "scores/val_dice_mean_femur-left_fold0 8.24%\n",
      "scores/val_dice_mean_bladder_fold0 6.66%\n",
      "scores/val_dice_mean_heart_fold0 10.65%\n",
      "scores/val_dice_mean_lung-right_fold0 8.22%\n",
      "scores/val_dice_mean_lung-left_fold0 8.60%\n",
      "scores/val_dice_mean_spleen_fold0 9.64%\n",
      "scores/val_dice_mean_pancreas_fold0 25.51%\n",
      "\n",
      "### Log epoch 10 @ 133.37\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 87.35%\n",
      "scores/dice_mean_liver_fold0 87.22%\n",
      "scores/dice_mean_kidney-right_fold0 86.51%\n",
      "scores/dice_mean_kidney-left_fold0 85.30%\n",
      "scores/dice_mean_femur-right_fold0 86.50%\n",
      "scores/dice_mean_femur-left_fold0 86.69%\n",
      "scores/dice_mean_bladder_fold0 85.22%\n",
      "scores/dice_mean_heart_fold0 82.94%\n",
      "scores/dice_mean_lung-right_fold0 85.08%\n",
      "scores/dice_mean_lung-left_fold0 82.55%\n",
      "scores/dice_mean_spleen_fold0 84.61%\n",
      "scores/dice_mean_pancreas_fold0 84.26%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 11.01%\n",
      "scores/val_dice_mean_liver_fold0 6.50%\n",
      "scores/val_dice_mean_kidney-right_fold0 8.98%\n",
      "scores/val_dice_mean_kidney-left_fold0 8.14%\n",
      "scores/val_dice_mean_femur-right_fold0 6.14%\n",
      "scores/val_dice_mean_femur-left_fold0 12.20%\n",
      "scores/val_dice_mean_bladder_fold0 10.61%\n",
      "scores/val_dice_mean_heart_fold0 5.15%\n",
      "scores/val_dice_mean_lung-right_fold0 22.47%\n",
      "scores/val_dice_mean_lung-left_fold0 9.63%\n",
      "scores/val_dice_mean_spleen_fold0 11.04%\n",
      "scores/val_dice_mean_pancreas_fold0 24.28%\n",
      "\n",
      "### Log epoch 11 @ 145.32\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 88.88%\n",
      "scores/dice_mean_liver_fold0 88.91%\n",
      "scores/dice_mean_kidney-right_fold0 87.42%\n",
      "scores/dice_mean_kidney-left_fold0 87.85%\n",
      "scores/dice_mean_femur-right_fold0 88.95%\n",
      "scores/dice_mean_femur-left_fold0 89.08%\n",
      "scores/dice_mean_bladder_fold0 88.89%\n",
      "scores/dice_mean_heart_fold0 91.06%\n",
      "scores/dice_mean_lung-right_fold0 87.43%\n",
      "scores/dice_mean_lung-left_fold0 87.59%\n",
      "scores/dice_mean_spleen_fold0 87.50%\n",
      "scores/dice_mean_pancreas_fold0 89.40%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 17.31%\n",
      "scores/val_dice_mean_liver_fold0 10.66%\n",
      "scores/val_dice_mean_kidney-right_fold0 13.71%\n",
      "scores/val_dice_mean_kidney-left_fold0 10.08%\n",
      "scores/val_dice_mean_femur-right_fold0 3.23%\n",
      "scores/val_dice_mean_femur-left_fold0 13.13%\n",
      "scores/val_dice_mean_bladder_fold0 11.45%\n",
      "scores/val_dice_mean_heart_fold0 9.65%\n",
      "scores/val_dice_mean_lung-right_fold0 17.64%\n",
      "scores/val_dice_mean_lung-left_fold0 10.87%\n",
      "scores/val_dice_mean_spleen_fold0 8.53%\n",
      "scores/val_dice_mean_pancreas_fold0 31.92%\n",
      "\n",
      "### Log epoch 12 @ 157.30\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 91.34%\n",
      "scores/dice_mean_liver_fold0 91.17%\n",
      "scores/dice_mean_kidney-right_fold0 90.55%\n",
      "scores/dice_mean_kidney-left_fold0 89.82%\n",
      "scores/dice_mean_femur-right_fold0 92.48%\n",
      "scores/dice_mean_femur-left_fold0 91.49%\n",
      "scores/dice_mean_bladder_fold0 91.11%\n",
      "scores/dice_mean_heart_fold0 90.64%\n",
      "scores/dice_mean_lung-right_fold0 91.76%\n",
      "scores/dice_mean_lung-left_fold0 90.40%\n",
      "scores/dice_mean_spleen_fold0 88.98%\n",
      "scores/dice_mean_pancreas_fold0 89.77%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 11.12%\n",
      "scores/val_dice_mean_liver_fold0 7.42%\n",
      "scores/val_dice_mean_kidney-right_fold0 11.20%\n",
      "scores/val_dice_mean_kidney-left_fold0 4.16%\n",
      "scores/val_dice_mean_femur-right_fold0 10.05%\n",
      "scores/val_dice_mean_femur-left_fold0 14.54%\n",
      "scores/val_dice_mean_bladder_fold0 9.14%\n",
      "scores/val_dice_mean_heart_fold0 13.81%\n",
      "scores/val_dice_mean_lung-right_fold0 15.26%\n",
      "scores/val_dice_mean_lung-left_fold0 10.04%\n",
      "scores/val_dice_mean_spleen_fold0 9.08%\n",
      "scores/val_dice_mean_pancreas_fold0 24.46%\n",
      "\n",
      "### Log epoch 13 @ 169.73\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 88.16%\n",
      "scores/dice_mean_liver_fold0 88.24%\n",
      "scores/dice_mean_kidney-right_fold0 87.24%\n",
      "scores/dice_mean_kidney-left_fold0 87.35%\n",
      "scores/dice_mean_femur-right_fold0 88.14%\n",
      "scores/dice_mean_femur-left_fold0 88.28%\n",
      "scores/dice_mean_bladder_fold0 86.83%\n",
      "scores/dice_mean_heart_fold0 84.49%\n",
      "scores/dice_mean_lung-right_fold0 88.05%\n",
      "scores/dice_mean_lung-left_fold0 86.43%\n",
      "scores/dice_mean_spleen_fold0 85.21%\n",
      "scores/dice_mean_pancreas_fold0 88.36%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 10.50%\n",
      "scores/val_dice_mean_liver_fold0 7.50%\n",
      "scores/val_dice_mean_kidney-right_fold0 10.49%\n",
      "scores/val_dice_mean_kidney-left_fold0 4.19%\n",
      "scores/val_dice_mean_femur-right_fold0 11.13%\n",
      "scores/val_dice_mean_femur-left_fold0 11.26%\n",
      "scores/val_dice_mean_bladder_fold0 9.74%\n",
      "scores/val_dice_mean_heart_fold0 10.01%\n",
      "scores/val_dice_mean_lung-right_fold0 10.81%\n",
      "scores/val_dice_mean_lung-left_fold0 9.34%\n",
      "scores/val_dice_mean_spleen_fold0 6.98%\n",
      "scores/val_dice_mean_pancreas_fold0 33.55%\n",
      "\n",
      "### Log epoch 14 @ 181.84\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 89.99%\n",
      "scores/dice_mean_liver_fold0 89.87%\n",
      "scores/dice_mean_kidney-right_fold0 89.36%\n",
      "scores/dice_mean_kidney-left_fold0 88.82%\n",
      "scores/dice_mean_femur-right_fold0 88.77%\n",
      "scores/dice_mean_femur-left_fold0 90.33%\n",
      "scores/dice_mean_bladder_fold0 90.73%\n",
      "scores/dice_mean_heart_fold0 90.45%\n",
      "scores/dice_mean_lung-right_fold0 83.85%\n",
      "scores/dice_mean_lung-left_fold0 88.04%\n",
      "scores/dice_mean_spleen_fold0 87.84%\n",
      "scores/dice_mean_pancreas_fold0 91.64%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 14.99%\n",
      "scores/val_dice_mean_liver_fold0 7.32%\n",
      "scores/val_dice_mean_kidney-right_fold0 11.93%\n",
      "scores/val_dice_mean_kidney-left_fold0 7.30%\n",
      "scores/val_dice_mean_femur-right_fold0 10.39%\n",
      "scores/val_dice_mean_femur-left_fold0 10.42%\n",
      "scores/val_dice_mean_bladder_fold0 13.40%\n",
      "scores/val_dice_mean_heart_fold0 9.58%\n",
      "scores/val_dice_mean_lung-right_fold0 17.28%\n",
      "scores/val_dice_mean_lung-left_fold0 11.99%\n",
      "scores/val_dice_mean_spleen_fold0 12.59%\n",
      "scores/val_dice_mean_pancreas_fold0 40.31%\n",
      "\n",
      "### Log epoch 15 @ 194.48\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 91.22%\n",
      "scores/dice_mean_liver_fold0 91.06%\n",
      "scores/dice_mean_kidney-right_fold0 90.86%\n",
      "scores/dice_mean_kidney-left_fold0 89.99%\n",
      "scores/dice_mean_femur-right_fold0 91.92%\n",
      "scores/dice_mean_femur-left_fold0 91.89%\n",
      "scores/dice_mean_bladder_fold0 91.51%\n",
      "scores/dice_mean_heart_fold0 93.32%\n",
      "scores/dice_mean_lung-right_fold0 88.33%\n",
      "scores/dice_mean_lung-left_fold0 90.16%\n",
      "scores/dice_mean_spleen_fold0 89.62%\n",
      "scores/dice_mean_pancreas_fold0 92.63%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 15.34%\n",
      "scores/val_dice_mean_liver_fold0 9.51%\n",
      "scores/val_dice_mean_kidney-right_fold0 7.68%\n",
      "scores/val_dice_mean_kidney-left_fold0 5.82%\n",
      "scores/val_dice_mean_femur-right_fold0 5.34%\n",
      "scores/val_dice_mean_femur-left_fold0 12.85%\n",
      "scores/val_dice_mean_bladder_fold0 14.18%\n",
      "scores/val_dice_mean_heart_fold0 9.99%\n",
      "scores/val_dice_mean_lung-right_fold0 17.60%\n",
      "scores/val_dice_mean_lung-left_fold0 7.56%\n",
      "scores/val_dice_mean_spleen_fold0 13.47%\n",
      "scores/val_dice_mean_pancreas_fold0 36.45%\n",
      "\n",
      "### Log epoch 16 @ 206.60\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 92.72%\n",
      "scores/dice_mean_liver_fold0 92.56%\n",
      "scores/dice_mean_kidney-right_fold0 91.77%\n",
      "scores/dice_mean_kidney-left_fold0 90.88%\n",
      "scores/dice_mean_femur-right_fold0 91.90%\n",
      "scores/dice_mean_femur-left_fold0 92.69%\n",
      "scores/dice_mean_bladder_fold0 91.90%\n",
      "scores/dice_mean_heart_fold0 88.68%\n",
      "scores/dice_mean_lung-right_fold0 90.90%\n",
      "scores/dice_mean_lung-left_fold0 88.86%\n",
      "scores/dice_mean_spleen_fold0 89.44%\n",
      "scores/dice_mean_pancreas_fold0 89.03%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 13.22%\n",
      "scores/val_dice_mean_liver_fold0 9.16%\n",
      "scores/val_dice_mean_kidney-right_fold0 10.94%\n",
      "scores/val_dice_mean_kidney-left_fold0 9.11%\n",
      "scores/val_dice_mean_femur-right_fold0 4.27%\n",
      "scores/val_dice_mean_femur-left_fold0 13.13%\n",
      "scores/val_dice_mean_bladder_fold0 13.61%\n",
      "scores/val_dice_mean_heart_fold0 19.30%\n",
      "scores/val_dice_mean_lung-right_fold0 21.51%\n",
      "scores/val_dice_mean_lung-left_fold0 9.89%\n",
      "scores/val_dice_mean_spleen_fold0 7.68%\n",
      "scores/val_dice_mean_pancreas_fold0 29.38%\n",
      "\n",
      "### Log epoch 17 @ 219.15\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 90.72%\n",
      "scores/dice_mean_liver_fold0 90.58%\n",
      "scores/dice_mean_kidney-right_fold0 90.26%\n",
      "scores/dice_mean_kidney-left_fold0 88.23%\n",
      "scores/dice_mean_femur-right_fold0 90.58%\n",
      "scores/dice_mean_femur-left_fold0 90.62%\n",
      "scores/dice_mean_bladder_fold0 89.47%\n",
      "scores/dice_mean_heart_fold0 88.88%\n",
      "scores/dice_mean_lung-right_fold0 86.16%\n",
      "scores/dice_mean_lung-left_fold0 87.35%\n",
      "scores/dice_mean_spleen_fold0 88.15%\n",
      "scores/dice_mean_pancreas_fold0 89.80%\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 18.88%\n",
      "scores/val_dice_mean_liver_fold0 7.40%\n",
      "scores/val_dice_mean_kidney-right_fold0 10.45%\n",
      "scores/val_dice_mean_kidney-left_fold0 8.36%\n",
      "scores/val_dice_mean_femur-right_fold0 11.21%\n",
      "scores/val_dice_mean_femur-left_fold0 18.69%\n",
      "scores/val_dice_mean_bladder_fold0 10.31%\n",
      "scores/val_dice_mean_heart_fold0 14.62%\n",
      "scores/val_dice_mean_lung-right_fold0 14.12%\n",
      "scores/val_dice_mean_lung-left_fold0 11.12%\n",
      "scores/val_dice_mean_spleen_fold0 13.10%\n",
      "scores/val_dice_mean_pancreas_fold0 32.79%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_254077/3833717225.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_DL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_254077/1962608343.py\u001b[0m in \u001b[0;36mtrain_DL\u001b[0;34m(run_name, config, training_dataset)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mb_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_254077/2981782257.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, dataset_idx, yield_2d_override)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0myield_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mall_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_2d_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0m_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_data_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_254077/2981782257.py\u001b[0m in \u001b[0;36mget_2d_ids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_2d_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         return sorted(list(\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_data_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_data_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         ))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config_dict['debug'] = False\n",
    "config_dict['wandb_mode'] = 'disabled'\n",
    "config_dict['batch_size'] = 16\n",
    "print(len(training_dataset))\n",
    "\n",
    "run = wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "    config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "    mode=config_dict['wandb_mode']\n",
    ")\n",
    "run_name = run.name\n",
    "config = wandb.config\n",
    "\n",
    "train_DL(run_name, config, training_dataset)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fef1bd-ec6a-4bbd-a5d0-a2b006e8813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_DL(run_name, config, inf_dataset):\n",
    "\n",
    "    score_dicts = []\n",
    "    \n",
    "    fold_iter = range(config.num_folds)\n",
    "    if config_dict['only_first_fold']:\n",
    "        fold_iter = fold_iter[0:1]\n",
    "        \n",
    "    for fold_idx in fold_iter:\n",
    "        lraspp, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "        lraspp.eval()\n",
    "        inf_dataset.eval()   \n",
    "        stack_dim = config.yield_2d_normal_to\n",
    "        \n",
    "        inf_dices = []\n",
    "        inf_dices_tumour = []\n",
    "        inf_dices_cochlea = []\n",
    "        \n",
    "        for inf_sample in inf_dataset:\n",
    "            global_idx = get_global_idx(fold_idx, sample_idx, config.epochs)\n",
    "            crossmoda_id = sample['crossmoda_id']\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Create batch out of single val sample\n",
    "                    b_inf_img = inf_sample['image'].unsqueeze(0)\n",
    "                    b_inf_seg = inf_sample['label'].unsqueeze(0)\n",
    "\n",
    "                    B = b_inf_img.shape[0]\n",
    "\n",
    "                    b_inf_img = b_inf_img.unsqueeze(1).float().cuda()\n",
    "                    b_inf_seg = b_inf_seg.cuda()\n",
    "                    b_inf_img_2d = make_2d_stack_from_3d(b_inf_img, stack_dim=stack_dim)\n",
    "\n",
    "                    if config.use_mind:\n",
    "                        b_inf_img_2d = mindssc(b_inf_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                    output_inf = lraspp(b_inf_img_2d)['out']\n",
    "\n",
    "                    # Prepare logits for scoring\n",
    "                    # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                    inf_logits_for_score = make_3d_from_2d_stack(output_inf, stack_dim, B)\n",
    "                    inf_logits_for_score = inf_logits_for_score.argmax(1)\n",
    "\n",
    "                    inf_dice = dice3d(\n",
    "                        torch.nn.functional.one_hot(inf_logits_for_score, 3),\n",
    "                        torch.nn.functional.one_hot(b_inf_seg, 3), \n",
    "                        one_hot_torch_style=True\n",
    "                    )\n",
    "                    inf_dices.append(get_batch_dice_wo_bg(inf_dice))\n",
    "                    inf_dices_tumour.append(get_batch_dice_tumour(inf_dice))\n",
    "                    inf_dices_cochlea.append(get_batch_dice_cochlea(inf_dice))\n",
    "\n",
    "                    if config.do_plot:\n",
    "                        print(\"Inference 3D image label/ground-truth\")\n",
    "                        print(inf_dice)\n",
    "                        # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                        display_seg(in_type=\"single_3D\", \n",
    "                            reduce_dim=\"W\",\n",
    "                            img=inf_sample['image'].unsqueeze(0).cpu(), \n",
    "                            seg=inf_logits_for_score.squeeze(0).cpu(),\n",
    "                            ground_truth=b_inf_seg.squeeze(0).cpu(),\n",
    "                            crop_to_non_zero_seg=True,\n",
    "                            crop_to_non_zero_gt=True,\n",
    "                            alpha_seg=.4,\n",
    "                            alpha_gt=.2\n",
    "                        )\n",
    "                        \n",
    "            if config.debug:\n",
    "                break\n",
    "                \n",
    "        mean_inf_dice = np.nanmean(inf_dices)\n",
    "        mean_inf_dice_tumour = np.nanmean(inf_dices_tumour)\n",
    "        mean_inf_dice_cochlea = np.nanmean(inf_dices_cochlea)\n",
    "\n",
    "        print(f'inf_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_inf_dice*100:.2f}%\")\n",
    "        print(f'inf_dice_mean_tumour_fold{fold_idx}', f\"{mean_inf_dice_tumour*100:.2f}%\")\n",
    "        print(f'inf_dice_mean_cochlea_fold{fold_idx}', f\"{mean_inf_dice_cochlea*100:.2f}%\")\n",
    "        wandb.log({f'scores/inf_dice_mean_wo_bg_fold{fold_idx}': mean_inf_dice}, step=global_idx)\n",
    "        wandb.log({f'scores/inf_dice_mean_tumour_fold{fold_idx}': mean_inf_dice_tumour}, step=global_idx)\n",
    "        wandb.log({f'scores/inf_dice_mean_cochlea_fold{fold_idx}': mean_inf_dice_cochlea}, step=global_idx)\n",
    "\n",
    "        # Store data for inter-fold scoring\n",
    "        class_dice_list = inf_dices.tolist()[0]\n",
    "        for class_idx, class_dice in enumerate(class_dice_list):\n",
    "            score_dicts.append(\n",
    "                {\n",
    "                    'fold_idx': fold_idx,\n",
    "                    'crossmoda_id': crossmoda_id,\n",
    "                    'class_idx': class_idx,\n",
    "                    'class_dice': class_dice,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    mean_oa_inf_dice = np.nanmean(torch.tensor([score['class_dice'] for score in score_dicts]))\n",
    "    print(f\"Mean dice over all folds, classes and samples: {mean_oa_inf_dice*100:.2f}%\")\n",
    "    wandb.log({'scores/mean_dice_all_folds_samples_classes': mean_oa_inf_dice}, step=global_idx)\n",
    "\n",
    "    return score_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_scores = []\n",
    "run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"testing\", job_type=\"test\",\n",
    "        config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "        mode=config_dict['wandb_mode']\n",
    ")\n",
    "config = wandb.config\n",
    "score_dicts = inference_DL(run_name, config, validation_dataset)\n",
    "folds_scores.append(score_dicts)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
