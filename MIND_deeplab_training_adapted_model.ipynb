{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -3 -4\"))\n",
    "from curriculum_deeplab.crossmoda_dataloader import CrossMoDa_Data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vision_utils\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from sklearn.model_selection import KFold\n",
    "import cc3d\n",
    "from mdl_seg_class.metrics import dice3d\n",
    "\n",
    "from PIL import Image as pil_image\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca60d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            # nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        # x = F.adaptive_avg_pool3d(x, (1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')  # , align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = [nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())]\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    " \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "# Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "def create_model(output_classes: int = 14,input_channels: int = 1):\n",
    "    # in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "    in_channels = torch.Tensor([input_channels, 24, 24, 32, 48, 48, 48, 64]).long()\n",
    "    mid_channels = torch.Tensor([64, 128, 192, 192, 256, 256, 256, 384]).long()\n",
    "    out_channels = torch.Tensor([24, 24, 32, 48, 48, 48, 64, 64]).long()\n",
    "    mid_stride = torch.Tensor([1, 1, 1, 2, 1, 1, 1, 1])\n",
    "    net = [nn.Identity()]\n",
    "    for i in range(8):\n",
    "        inc = int(in_channels[i])\n",
    "        midc = int(mid_channels[i])\n",
    "        outc = int(out_channels[i])\n",
    "        strd = int(mid_stride[i])\n",
    "        layer = nn.Sequential(nn.Conv3d(inc, midc, 1, bias=False), nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, midc, 3, stride=strd, padding=1, bias=False, groups=midc),\n",
    "                              nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, outc, 1, bias=False), nn.BatchNorm3d(outc))\n",
    "        if i == 0:\n",
    "            layer[0] = nn.Conv3d(inc, midc, 3, padding=1, stride=2, bias=False)\n",
    "        if (inc == outc) & (strd == 1):\n",
    "            net.append(ResBlock(layer))\n",
    "        else:\n",
    "            net.append(layer)\n",
    "\n",
    "    backbone = nn.Sequential(*net)\n",
    "\n",
    "    count = 0\n",
    "    # weight initialization\n",
    "    for m in backbone.modules():\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            count += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    print('#CNN layer', count)\n",
    "    # complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "    # newer model (one more stride, no groups in head)\n",
    "    aspp = ASPP(64, (2, 4, 8, 16, 32), 128)\n",
    "    head = nn.Sequential(nn.Conv3d(128 + 24, 64, 1, padding=0, groups=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, 64, 3, groups=1, padding=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, output_classes, 1))\n",
    "    return backbone, aspp, head\n",
    "\n",
    "\n",
    "def apply_model(backbone, aspp, head, img, checkpointing=True, return_intermediate=False):\n",
    "    if checkpointing:\n",
    "        x1 = checkpoint(backbone[:3], img)\n",
    "        x2 = checkpoint(backbone[3:], x1)\n",
    "        y = checkpoint(aspp, x2)\n",
    "        y1 = torch.cat((x1[0:1], F.interpolate(y[0:1], scale_factor=2)), 1)\n",
    "        output_j = checkpoint(head, y1)\n",
    "    else:\n",
    "        x1 = backbone[:3](img)\n",
    "        x2 = backbone[3:](x1)\n",
    "        y = aspp(x2)\n",
    "        y1 = torch.cat((x1[0:1], F.interpolate(y[0:1], scale_factor=2)), 1)\n",
    "        output_j = head(y1)\n",
    "    if return_intermediate:\n",
    "        return y1,output_j\n",
    "    else:\n",
    "        return output_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossMoDa_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        use_additional_data=False, resample = True,\n",
    "        size:tuple = (96,96,60), normalize:bool = True):\n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "\n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "        path = base_dir + state_dir\n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "        if domain.lower() ==\"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "        #initialize variables\n",
    "        self.imgs = torch.zeros(0,size[0],size[1],size[2])\n",
    "        self.labels = torch.zeros(0,size[0],size[1],size[2])\n",
    "        self.img_nums = []\n",
    "        self.label_nums = []\n",
    "        #load data\n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "\n",
    "        for i,f in enumerate(tqdm(files)):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in f:\n",
    "                self.label_nums.append(int(re.findall(r'\\d+', os.path.basename(f))[0]))\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                self.labels = torch.cat((self.labels,tmp.unsqueeze(0)),dim=0)\n",
    "            elif domain in f:\n",
    "                self.img_nums.append(int(re.findall(r'\\d+', os.path.basename(f))[0]))\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                self.imgs = torch.cat((self.imgs,tmp.unsqueeze(0)),dim=0)\n",
    "        self.labels = self.labels.long()\n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(self.img_nums==self.label_nums))\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(self.imgs.shape,self.imgs.mean(),self.imgs.std()))\n",
    "        print(\"Label shape: {}, max.: {}\".format(self.labels.shape,torch.max(self.labels)))\n",
    "        print(\"Data import finished. Elapsed time: {:.1f} s\".format(time.time()-t0 ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.imgs.size(0))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label.long()\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.imgs,self.labels\n",
    "\n",
    "    def get_image_numbers(self):\n",
    "        return self.img_nums\n",
    "\n",
    "    def get_label_numbers(self):\n",
    "        return self.label_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossmoda_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "domain=\"source\", state=\"l3\")#, size=(128,128,192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_images_from_gray_tensor(s_tensor, scale_min_max=True):\n",
    "    assert s_tensor.dim() == 4 # S,C,H,W\n",
    "\n",
    "    s_tensor = s_tensor.detach().cpu()\n",
    "    _min, _max = s_tensor.min(), s_tensor.max()\n",
    "\n",
    "    if scale_min_max:\n",
    "        if _max == _min:\n",
    "            s_tensor = torch.zeros_like(s_tensor)\n",
    "        else:\n",
    "            s_tensor = s_tensor.sub(_min).div(_max-_min).mul(255)\n",
    "\n",
    "    s_tensor = s_tensor.permute(0,2,3,1)\n",
    "    if s_tensor.shape[-1] == 1:\n",
    "        # Got a stack of grayscale images\n",
    "        s_tensor = s_tensor.squeeze(-1)\n",
    "\n",
    "    s_numpy = s_tensor.numpy()\n",
    "    \n",
    "    images = [pil_image.fromarray(numpy_rgb).convert('RGBA') for numpy_rgb in s_numpy]\n",
    "    # images = [transforms.functional.to_pil_image(numpy_rgb).convert('RGBA') for numpy_rgb in b_tensor]\n",
    "    return images\n",
    "\n",
    "def pil_images_from_onehot_seg(s_tensor_onehot, onehot_colormap, alpha) -> list:\n",
    "    assert s_tensor_onehot.dim() == 4, \"Tensor must be 2d with onehot encoding: Dim = [S, H, W, E]\"\n",
    "    # S,H,W,E = b_tensor_onehot.shape\n",
    "    s_tensor_onehot = s_tensor_onehot.detach().cpu()\n",
    "    alpha_channel = (int(255.*alpha),)\n",
    "    # Create RBG tensor with shape S,H,W,RGBA\n",
    "    s_rgba_tensor = torch.stack([torch.zeros(s_tensor_onehot.shape[:-1])]*4, dim=-1).type(torch.uint8)\n",
    "\n",
    "    for onehot_id, rgb_val in onehot_colormap.items():\n",
    "        if isinstance(rgb_val, tuple):\n",
    "            bhw_idx = s_tensor_onehot.argmax(dim=-1) == onehot_id\n",
    "            s_rgba_tensor[bhw_idx] = torch.tensor(rgb_val + alpha_channel, dtype=torch.uint8)\n",
    "\n",
    "    b_rgb_numpy = s_rgba_tensor.numpy()\n",
    "\n",
    "    # Append a list of S,C,H,W pil images\n",
    "    list_images = [pil_image.fromarray(numpy_rgb).convert('RGBA') for numpy_rgb in b_rgb_numpy]\n",
    "    return list_images\n",
    "\n",
    "def get_stacked_overlays(s_2d_img_tensor, s_2d_seg_tensor_onehot, onehot_colormap, alpha=0.3):\n",
    "    assert s_2d_img_tensor.dim() == 4, \"\" #S,C,H,W\n",
    "    pil_imgs = pil_images_from_gray_tensor(s_2d_img_tensor, scale_min_max=True)\n",
    "    pil_segs = pil_images_from_onehot_seg(s_2d_seg_tensor_onehot, onehot_colormap=onehot_colormap, alpha=alpha)\n",
    "    pil_overlays = []\n",
    "    \n",
    "    for rgb_img, rgb_seg in zip(pil_imgs, pil_segs):\n",
    "        pil_overlay = pil_image.alpha_composite(rgb_img, rgb_seg)\n",
    "        pil_overlays.append(pil_overlay)\n",
    "\n",
    "    tensor_overlays = torch.stack([transforms.functional.to_tensor(ovl) for ovl in pil_overlays], dim=0)\n",
    "    return pil_overlays, tensor_overlays\n",
    "\n",
    "def get_overlay_grid(s_2d_img_tensor, s_2d_seg_tensor_onehot, onehot_colormap, alpha=0.3, n_per_row=4):\n",
    "    _, tensor_overlays = get_stacked_overlays(s_2d_img_tensor, s_2d_seg_tensor_onehot, onehot_colormap, alpha=alpha)\n",
    "    grid_tensor = vision_utils.make_grid(tensor_overlays, nrow=n_per_row)\n",
    "    return transforms.functional.to_pil_image(grid_tensor), grid_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, seg = crossmoda_dataset[1]\n",
    "img_slices = img.permute(2,0,1).unsqueeze(1)\n",
    "seg_slices = seg.permute(2,0,1)\n",
    "print(seg.unique(return_counts=True))\n",
    "\n",
    "def get_cmap_dict(class_max_id, pyplot_map_name='rainbow', include_background=False):\n",
    "    cmap = plt.get_cmap(pyplot_map_name)\n",
    "    cmap_dict = {}\n",
    "\n",
    "    if include_background:\n",
    "        num_ids = class_max_id+1\n",
    "        id_offset = 0\n",
    "    else:\n",
    "        cmap_dict[0] = None\n",
    "        num_ids = class_max_id\n",
    "        id_offset = 1\n",
    "\n",
    "    discretized_map = (cmap((np.arange(num_ids)/float(num_ids)))*255).astype(np.int32)\n",
    "    for onehot_idx, rgb_list in enumerate(discretized_map):\n",
    "        cmap_dict[onehot_idx+id_offset] = tuple(rgb_list)[:3] # Extract only RGB not alpha\n",
    "\n",
    "    return cmap_dict\n",
    "\n",
    "color_map = {\n",
    "    0: None, \n",
    "    1: (255,0,0), #ONEHOT id and RGB color\n",
    "    2: (0,255,0)\n",
    "}\n",
    "# color_map = get_cmap_dict(2, pyplot_map_name='rainbow')\n",
    "# pil_images_from_gray_tensor()\n",
    "pil_ov, _ = get_overlay_grid(\n",
    "    img_slices, \n",
    "    torch.nn.functional.one_hot(seg_slices,3), \n",
    "    color_map, n_per_row=10, alpha=.2)\n",
    "\n",
    "display(pil_ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49da3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlaySegment(gray1,seg1,colors,flag=False):\n",
    "    H, W = seg1.squeeze().size()\n",
    "    #colors=torch.FloatTensor([0,0,0,199,67,66,225,140,154,78,129,170,45,170,170,240,110,38,111,163,91,235,175,86,202,255,52,162,0,183]).view(-1,3)/255.0\n",
    "    segs1 = F.one_hot(seg1.long(),29).float().permute(2,0,1)\n",
    "\n",
    "    seg_color = torch.mm(segs1.view(29,-1).t(),colors).view(H,W,3)\n",
    "    alpha = torch.clamp(1.0 - 0.5*(seg1>0).float(),0,1.0)\n",
    "\n",
    "    overlay = (gray1*alpha).unsqueeze(2) + seg_color*(1.0-alpha).unsqueeze(2)\n",
    "    if(flag):\n",
    "        plt.imshow((overlay).numpy()); \n",
    "        plt.axis('off');\n",
    "        plt.show()\n",
    "    return overlay\n",
    "\n",
    "def plot_slice_overlay(img,seg,slc= [100,60,100]):\n",
    "    print('image size:',img.size())\n",
    "    \n",
    "    i1 = overlaySegment(img[0,:,slc[1],:], seg[:,slc[1],:], flag=False)\n",
    "    i2 = overlaySegment(img[0,:,:,slc[2]], seg[:,:,slc[2]], flag=False)\n",
    "    i3 = overlaySegment(img[0,slc[0],:,:], seg[slc[0],:,:], flag=False)\n",
    "    fig,axs = plt.subplots(1, 3)\n",
    "    axs[0].imshow(i1.cpu().numpy())\n",
    "    axs[1].imshow(i2.cpu().numpy())\n",
    "    axs[2].imshow(i3.cpu().numpy())\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentAffine(img_in, seg_in, strength=0.05):\n",
    "    \"\"\"\n",
    "    3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: img_in batch (torch.cuda.FloatTensor), seg_in batch (torch.cuda.LongTensor)\n",
    "    :return: augmented BxCxTxHxW image batch (torch.cuda.FloatTensor), augmented BxTxHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    B,C,D,H,W = img_in.size()\n",
    "    affine_matrix = (torch.eye(3,4).unsqueeze(0) + torch.randn(B, 3, 4) * strength).to(img_in.device)\n",
    "\n",
    "    meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)))\n",
    "\n",
    "    img_out = F.grid_sample(img_in, meshgrid,padding_mode='border')\n",
    "    seg_out = F.grid_sample(seg_in.float(), meshgrid, mode='nearest')\n",
    "\n",
    "    return img_out, seg_out\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(img_in,strength=0.05):\n",
    "    return img_in + strength*torch.randn_like(img_in)\n",
    "\n",
    "\n",
    "    \n",
    "def pad_center_plane(img,size):\n",
    "    s0,s1,s2 = img.size()\n",
    "    i0 = (size[0]-s0)//2\n",
    "    i1 = (size[1]-s1)//2\n",
    "    i2 = 0\n",
    "    pd = (i2,size[2]-s2-i2,i1,size[1]-s1-i1,i0,size[0]-s0-i0)\n",
    "    #print('pad',pd)\n",
    "    img = F.pad(img, pd, \"constant\", 0)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def crop_center_plane(img, size):\n",
    "    s0,s1,s2 = img.size()\n",
    "    i0 = (s0-size[0])//2\n",
    "    i1 = (s1-size[1])//2\n",
    "    i2 = 0\n",
    "    img = img[i0:i0+size[0],i1:i1+size[1],i2:i2+size[2]]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c902fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "#training routine\n",
    "\n",
    "color_map = {\n",
    "    0: None, \n",
    "    1: (255,0,0), #ONEHOT id and RGB color\n",
    "    2: (0,255,0)\n",
    "}\n",
    "\n",
    "def train_DL(dataset ,epochs=500, update_epx = 50, use_mind = True):\n",
    "    img_num,C  = len(dataset), dataset[0][0].shape[0]\n",
    "    if use_mind:\n",
    "        C =12\n",
    "    else:\n",
    "        C = 1\n",
    "    all_segs = torch.cat([seg for _, seg in dataset])\n",
    "\n",
    "    num_class = int(torch.max(all_segs).item()+1)\n",
    "    class_weight = torch.sqrt(1.0/(torch.bincount(all_segs.long().view(-1)).float()))\n",
    "    class_weight = class_weight/class_weight.mean()\n",
    "    class_weight[0] = 0.15\n",
    "    class_weight = class_weight.cuda()\n",
    "    print('inv sqrt class_weight',class_weight)\n",
    "\n",
    "    backbone, aspp, head = create_model(output_classes=num_class,input_channels=C)\n",
    "    optimizer = torch.optim.Adam(list(backbone.parameters())+list(aspp.parameters())+list(head.parameters()),lr=0.001)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(class_weight)\n",
    "    scaler = amp.GradScaler()\n",
    "    backbone.cuda() \n",
    "    backbone.train()\n",
    "    aspp.cuda() \n",
    "    aspp.train()\n",
    "    head.cuda() \n",
    "    head.train()\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epx in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        ind = torch.randint(0,img_num,(1,))\n",
    "        img, seg = dataset[ind:ind+1]\n",
    "        img, seg = img.unsqueeze(0).float().cuda(), seg.unsqueeze(0).float().cuda()\n",
    "        \n",
    "        if use_mind:\n",
    "            # img = mindssc(img)\n",
    "            pass\n",
    "\n",
    "        img, seg = augmentAffine(img, seg, strength=0.1)\n",
    "        img = augmentNoise(img, strength=0.02)\n",
    "        \n",
    "        if DEBUG:\n",
    "            img_slices = img.permute(0,1,4,2,3).squeeze().unsqueeze(1)\n",
    "            seg_slices = seg.permute(0,1,4,2,3).squeeze().to(dtype=torch.int64)\n",
    "            idx_dept_with_segs, *_ = torch.nonzero(seg_slices > 0, as_tuple=True)\n",
    "            idx_dept_with_segs = idx_dept_with_segs.unique()\n",
    "\n",
    "            img_slices = img_slices[idx_dept_with_segs]\n",
    "            seg_slices = seg_slices[idx_dept_with_segs]\n",
    "            \n",
    "            pil_ov, _ = get_overlay_grid(\n",
    "                img_slices, \n",
    "                torch.nn.functional.one_hot(seg_slices,3), \n",
    "                color_map, n_per_row=10, alpha=.5\n",
    "            )\n",
    "            display(pil_ov)\n",
    "            \n",
    "        seg = F.interpolate(seg, scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "\n",
    "        img.requires_grad = True\n",
    "        #img_mr.requires_grad = True\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=True)\n",
    "            loss = criterion(output_j, seg)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if epx%update_epx==update_epx-1 or epx == 0:\n",
    "            dice = dice3d(output_j.permute(0,2,3,4,1),torch.nn.functional.one_hot(seg, 3), one_hot_torch_style=True)\n",
    "            print('epx',epx,round(time.time()-t0,2),'s','loss',round(loss.item(),6),'dice mean', round(dice.mean().item(),4),'dice',dice)\n",
    "        \n",
    "        if DEBUG:\n",
    "            break\n",
    "#    stat_cuda('Visceral training')\n",
    "    backbone.cpu()\n",
    "    aspp.cpu() \n",
    "    head.cpu() \n",
    "    return backbone,aspp,head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f476a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "updates = 50\n",
    "# imgs = torch.cat((imgs_train_source,imgs_train_target),dim=0)\n",
    "# label = torch.cat((labels_train_source,labels_train_target),dim=0)\n",
    "backbone,aspp,head = train_DL(crossmoda_dataset, epochs=epochs, update_epx=updates, use_mind=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921a2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0439928",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/share/data_sam1/ckruse/image_data/CrossMoDa/target_training/'\n",
    "label_path = '/share/data_supergrover1/weihsbach/tmp/crossmoda_full_set/'\n",
    "plot = False\n",
    "backbone.cuda() \n",
    "aspp.cuda() \n",
    "head.cuda() \n",
    "target_dices = torch.zeros(32)\n",
    "source_dices = torch.zeros(32)\n",
    "for i in range(32):\n",
    "    ind = i+150\n",
    "    nii_img = nib.load(path + 'crossmoda_'+ str(ind) + '_hrT2.nii.gz')\n",
    "    nii_label = nib.load(label_path + 'crossmoda_'+ str(ind) + '_hrT2_Label.nii.gz')\n",
    "    tmp = torch.from_numpy(nii_img.get_fdata())\n",
    "    label = torch.from_numpy(nii_label.get_fdata()).cuda()\n",
    "    org_size = tmp.size()  \n",
    "    size = (192,192,64)\n",
    "    tmp = crop_center_plane(tmp,size)\n",
    "    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "    org_img = torch.from_numpy(nii_img.get_fdata())\n",
    "    \n",
    "    img= tmp.float().cuda()\n",
    "    img = mindssc(img.unsqueeze(0).unsqueeze(0))\n",
    "    with torch.no_grad():\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=False)#\n",
    "    modeled_seg = F.interpolate(output_j,scale_factor=2).argmax(1)\n",
    "    modeled_seg = pad_center_plane(modeled_seg.squeeze(),org_size)\n",
    "    #print(modeled_seg.shape,torch.max(modeled_seg))\n",
    "    connectivity = 18 # only 4,8 (2D) and 26, 18, and 6 (3D) are allowed\n",
    "    np_label = modeled_seg.long().cpu().numpy().astype('int32')\n",
    "    tmp = pad_center_plane(tmp.squeeze(),org_size)\n",
    "    #plot = True\n",
    "    if plot:\n",
    "        slc = 30\n",
    "        i0 = overlaySegment(tmp[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        i1 = overlaySegment(org_img[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        fig,axs = plt.subplots(1, 2,figsize=(18, 9))\n",
    "        axs[0].imshow((i0+i1).cpu().numpy())\n",
    "        axs[1].imshow(i1.cpu().numpy())\n",
    "        plt.show()\n",
    "        fig.set_figwidth(40)\n",
    "        fig.set_figheight(10)\n",
    "    #print(modeled_seg.shape,torch.max(modeled_seg))\n",
    "    target_dices[i] = dice_coeff(modeled_seg,label)\n",
    "    print(f'image: crossmoda_{ind}_hrT2.nii.gz, dice: {target_dices[i]*100:0.2f}')\n",
    "    \n",
    "print(f'target dice mean: {target_dices.mean()*100:0.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ae4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/share/data_sam1/ckruse/image_data/CrossMoDa/target_validation/'\n",
    "plot = False\n",
    "save = True\n",
    "backbone.cuda() \n",
    "aspp.cuda() \n",
    "head.cuda() \n",
    "for i in range(32):\n",
    "    ind = i+211\n",
    "    nii_img = nib.load(path + 'crossmoda_'+ str(ind) + '_hrT2.nii.gz')\n",
    "    img_affine = nii_img.affine\n",
    "    tmp = torch.from_numpy(nii_img.get_fdata())\n",
    "    org_img = torch.from_numpy(nii_img.get_fdata())\n",
    "    org_size = tmp.size()  \n",
    "    size = (192,192,64)\n",
    "    tmp = crop_center_plane(tmp,size)\n",
    "    #print('red. shape',tmp.shape)\n",
    "    #tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size)\n",
    "    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "        \n",
    "    img= tmp.float().cuda()\n",
    "    #print(img.shape)\n",
    "    img = mindssc(img.unsqueeze(0).unsqueeze(0))\n",
    "    with torch.no_grad():\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=False)#\n",
    "    modeled_seg = F.interpolate(output_j,scale_factor=2).argmax(1)\n",
    "    modeled_seg = pad_center_plane(modeled_seg.squeeze(),org_size)\n",
    "    tmp = pad_center_plane(tmp.squeeze(),org_size)\n",
    "    #print('org shape',tmp.shape)\n",
    "    if plot:\n",
    "        slc = 30\n",
    "        i0 = overlaySegment(tmp[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        i1 = overlaySegment(org_img[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        fig,axs = plt.subplots(1, 2,figsize=(18, 9))\n",
    "        axs[0].imshow((i0+i1).cpu().numpy())\n",
    "        axs[1].imshow(i1.cpu().numpy())\n",
    "        plt.show()\n",
    "        fig.set_figwidth(40)\n",
    "        fig.set_figheight(10)\n",
    "    if save:\n",
    "        label_nii = nib.Nifti1Image(modeled_seg.float().squeeze().cpu().numpy(), img_affine)\n",
    "        nib.save(label_nii, 'Deeplab_validation/crossmoda_'+ str(ind) + '_Label.nii.gz')  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca07731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm Deeplab_validation.zip\n",
    "#!zip -r Deeplab_validation_half_res_adapted_model_target_train.zip Deeplab_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(backbone,aspp,head,name):\n",
    "    torch.save(backbone.state_dict(), name + '_backbone.pth')\n",
    "    torch.save(aspp.state_dict(), name + '_aspp.pth')\n",
    "    torch.save(head.state_dict(), name + '_head.pth')\n",
    "    return None\n",
    "\n",
    "def load_model(name,output_classes,input_channels):\n",
    "    backbone, aspp, head = create_model(output_classes=output_classes,input_channels=input_channels)\n",
    "    backbone.load_state_dict(torch.load( name + '_backbone.pth'))\n",
    "    aspp.load_state_dict(torch.load(name + '_aspp.pth'))\n",
    "    head.load_state_dict(torch.load(name + '_head.pth'))\n",
    "    return backbone,aspp,head\n",
    "\n",
    "save_model(backbone,aspp,head,'Models/half_res_adapted_model_target_training')\n",
    "#backbone,aspp,head=load_model('Models/half_res_adapted_model_target_training',3,12)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
