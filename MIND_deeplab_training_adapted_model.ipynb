{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2fc395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name              Util    Mem free  Cuda             User(s)\n",
      "----  -------------------  ------  ----------  ---------------  -----------------------\n",
      "   0  Tesla T4                0 %   15106 MiB  11.1(455.45.01)  hansen, graf\n",
      "   1  Tesla T4                0 %   13633 MiB  11.1(455.45.01)  weihsbach, hansen, graf\n",
      "   2  GeForce RTX 2080 Ti     0 %   11016 MiB  11.1(455.45.01)  hansen, graf\n",
      "   4  GeForce RTX 2080 Ti     0 %   11016 MiB  11.1(455.45.01)  hansen, graf\n",
      "   5  GeForce RTX 2080 Ti     0 %   11016 MiB  11.1(455.45.01)  hansen, graf\n",
      "   3  Quadro RTX 8000      ! 90 %   28928 MiB  11.1(455.45.01)  hansen, graf\n",
      "   6  GeForce RTX 2080 Ti  ! 94 %    2829 MiB  11.1(455.45.01)  hansen, graf, bigalke\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name        torch\n",
      "----  -----------  --  -------\n",
      "   0  Tesla T4     ->  cuda:0\n",
      "1.9.1+cu102\n",
      "7605\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -3 -4\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d\n",
    "from mdl_seg_class.visualization import get_overlay_grid\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ca60d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            # nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        # x = F.adaptive_avg_pool3d(x, (1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')  # , align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = [nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())]\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    " \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "# Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "def create_model(output_classes: int = 14,input_channels: int = 1):\n",
    "    # in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "    in_channels = torch.Tensor([input_channels, 24, 24, 32, 48, 48, 48, 64]).long()\n",
    "    mid_channels = torch.Tensor([64, 128, 192, 192, 256, 256, 256, 384]).long()\n",
    "    out_channels = torch.Tensor([24, 24, 32, 48, 48, 48, 64, 64]).long()\n",
    "    mid_stride = torch.Tensor([1, 1, 1, 2, 1, 1, 1, 1])\n",
    "    net = [nn.Identity()]\n",
    "    for i in range(8):\n",
    "        inc = int(in_channels[i])\n",
    "        midc = int(mid_channels[i])\n",
    "        outc = int(out_channels[i])\n",
    "        strd = int(mid_stride[i])\n",
    "        layer = nn.Sequential(nn.Conv3d(inc, midc, 1, bias=False), nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, midc, 3, stride=strd, padding=1, bias=False, groups=midc),\n",
    "                              nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, outc, 1, bias=False), nn.BatchNorm3d(outc))\n",
    "        if i == 0:\n",
    "            layer[0] = nn.Conv3d(inc, midc, 3, padding=1, stride=2, bias=False)\n",
    "        if (inc == outc) & (strd == 1):\n",
    "            net.append(ResBlock(layer))\n",
    "        else:\n",
    "            net.append(layer)\n",
    "\n",
    "    backbone = nn.Sequential(*net)\n",
    "\n",
    "    count = 0\n",
    "    # weight initialization\n",
    "    for m in backbone.modules():\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            count += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    print('#CNN layer', count)\n",
    "    # complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "    # newer model (one more stride, no groups in head)\n",
    "    aspp = ASPP(64, (2, 4, 8, 16, 32), 128)\n",
    "    head = nn.Sequential(nn.Conv3d(128 + 24, 64, 1, padding=0, groups=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, 64, 3, groups=1, padding=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, output_classes, 1))\n",
    "    return backbone, aspp, head\n",
    "\n",
    "\n",
    "def apply_model(backbone, aspp, head, img, checkpointing=True, return_intermediate=False):\n",
    "    if checkpointing:\n",
    "        x1 = checkpoint(backbone[:3], img)\n",
    "        x2 = checkpoint(backbone[3:], x1)\n",
    "        y = checkpoint(aspp, x2)\n",
    "        y1 = torch.cat((x1[0:1], F.interpolate(y[0:1], scale_factor=2)), 1)\n",
    "        output_j = checkpoint(head, y1)\n",
    "    else:\n",
    "        x1 = backbone[:3](img)\n",
    "        x2 = backbone[3:](x1)\n",
    "        y = aspp(x2)\n",
    "        y1 = torch.cat((x1[0:1], F.interpolate(y[0:1], scale_factor=2)), 1)\n",
    "        output_j = head(y1)\n",
    "    if return_intermediate:\n",
    "        return y1,output_j\n",
    "    else:\n",
    "        return output_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossMoDa_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        use_additional_data=False, resample = True,\n",
    "        size:tuple = (96,96,60), normalize:bool = True):\n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "\n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "        path = base_dir + state_dir\n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "        if domain.lower() ==\"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "        #initialize variables\n",
    "        self.imgs = torch.zeros(0,size[0],size[1],size[2])\n",
    "        self.labels = torch.zeros(0,size[0],size[1],size[2])\n",
    "        self.img_nums = []\n",
    "        self.label_nums = []\n",
    "        #load data\n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "\n",
    "        for i,f in enumerate(tqdm(files)):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in f:\n",
    "                self.label_nums.append(int(re.findall(r'\\d+', os.path.basename(f))[0]))\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                self.labels = torch.cat((self.labels,tmp.unsqueeze(0)),dim=0)\n",
    "            elif domain in f:\n",
    "                self.img_nums.append(int(re.findall(r'\\d+', os.path.basename(f))[0]))\n",
    "                tmp = torch.from_numpy(nib.load(f).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "                self.imgs = torch.cat((self.imgs,tmp.unsqueeze(0)),dim=0)\n",
    "        self.labels = self.labels.long()\n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(self.img_nums==self.label_nums))\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(self.imgs.shape,self.imgs.mean(),self.imgs.std()))\n",
    "        print(\"Label shape: {}, max.: {}\".format(self.labels.shape,torch.max(self.labels)))\n",
    "        print(\"Data import finished. Elapsed time: {:.1f} s\".format(time.time()-t0 ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.imgs.size(0))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label.long()\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.imgs,self.labels\n",
    "\n",
    "    def get_image_numbers(self):\n",
    "        return self.img_nums\n",
    "\n",
    "    def get_label_numbers(self):\n",
    "        return self.label_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CrossMoDa ceT1 images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 418/418 [02:28<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal image and label numbers: True\n",
      "Image shape: torch.Size([209, 96, 96, 60]), mean.: 0.00, std.: 1.00\n",
      "Label shape: torch.Size([209, 96, 96, 60]), max.: 2\n",
      "Data import finished. Elapsed time: 150.1 s\n",
      "Loading CrossMoDa validation images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:13<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal image and label numbers: False\n",
      "Image shape: torch.Size([63, 96, 96, 60]), mean.: -0.00, std.: 1.00\n",
      "Label shape: torch.Size([19, 96, 96, 60]), max.: 2\n",
      "Data import finished. Elapsed time: 13.3 s\n",
      "Loading CrossMoDa hrT2 images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:21<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal image and label numbers: True\n",
      "Image shape: torch.Size([60, 96, 96, 60]), mean.: 0.00, std.: 1.00\n",
      "Label shape: torch.Size([60, 96, 96, 60]), max.: 2\n",
      "Data import finished. Elapsed time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "training_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "domain=\"source\", state=\"l4\")\n",
    "validation_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "domain=\"validation\", state=\"l4\")\n",
    "target_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\", \n",
    "domain=\"target\", state=\"l4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "684c5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentAffine(img_in, seg_in, strength=0.05):\n",
    "    \"\"\"\n",
    "    3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: img_in batch (torch.cuda.FloatTensor), seg_in batch (torch.cuda.LongTensor)\n",
    "    :return: augmented BxCxTxHxW image batch (torch.cuda.FloatTensor), augmented BxTxHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    B,C,D,H,W = img_in.size()\n",
    "    affine_matrix = (torch.eye(3,4).unsqueeze(0) + torch.randn(B, 3, 4) * strength).to(img_in.device)\n",
    "\n",
    "    meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)))\n",
    "\n",
    "    img_out = F.grid_sample(img_in, meshgrid,padding_mode='border')\n",
    "    seg_out = F.grid_sample(seg_in.float(), meshgrid, mode='nearest')\n",
    "\n",
    "    return img_out, seg_out\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(img_in,strength=0.05):\n",
    "    return img_in + strength*torch.randn_like(img_in)\n",
    "\n",
    "\n",
    "    \n",
    "def pad_center_plane(img,size):\n",
    "    s0,s1,s2 = img.size()\n",
    "    i0 = (size[0]-s0)//2\n",
    "    i1 = (size[1]-s1)//2\n",
    "    i2 = 0\n",
    "    pd = (i2,size[2]-s2-i2,i1,size[1]-s1-i1,i0,size[0]-s0-i0)\n",
    "    #print('pad',pd)\n",
    "    img = F.pad(img, pd, \"constant\", 0)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def crop_center_plane(img, size):\n",
    "    s0,s1,s2 = img.size()\n",
    "    i0 = (s0-size[0])//2\n",
    "    i1 = (s1-size[1])//2\n",
    "    i2 = 0\n",
    "    img = img[i0:i0+size[0],i1:i1+size[1],i2:i2+size[2]]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c902fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training routine\n",
    "\n",
    "def train_DL(training_dataset, target_dataset, validation_dataset,\n",
    "    epochs=500, update_epx = 50, use_mind = True, debug=False):\n",
    "\n",
    "    if use_mind:\n",
    "        C =12\n",
    "    else:\n",
    "        C = 1\n",
    "    all_segs = torch.cat([seg for _, seg in training_dataset])\n",
    "\n",
    "    num_class = int(torch.max(all_segs).item()+1)\n",
    "    class_weight = torch.sqrt(1.0/(torch.bincount(all_segs.long().view(-1)).float()))\n",
    "    class_weight = class_weight/class_weight.mean()\n",
    "    class_weight[0] = 0.15\n",
    "    class_weight = class_weight.cuda()\n",
    "    print('inv sqrt class_weight', class_weight)\n",
    "\n",
    "    kf = KFold(n_splits=2)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "    train_idx, val_idx = next(kf.split(training_dataset))\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_dataloader = DataLoader(training_dataset, batch_size=1, shuffle=False,sampler=train_subsampler)\n",
    "    target_dataloader = DataLoader(target_dataset, batch_size=1,shuffle=False)\n",
    "    val_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False,sampler=val_subsampler)\n",
    "\n",
    "    backbone, aspp, head = create_model(output_classes=num_class,input_channels=C)\n",
    "    optimizer = torch.optim.Adam(list(backbone.parameters())+list(aspp.parameters())+list(head.parameters()),lr=0.001)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(class_weight)\n",
    "    scaler = amp.GradScaler()\n",
    "    backbone.cuda() \n",
    "    backbone.train()\n",
    "    aspp.cuda() \n",
    "    aspp.train()\n",
    "    head.cuda() \n",
    "    head.train()\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epx in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        img, seg = next(iter(train_dataloader))\n",
    "        img, seg = img.unsqueeze(0).float().cuda(), seg.unsqueeze(0).float().cuda()\n",
    "        \n",
    "        if use_mind:\n",
    "            # img = mindssc(img)\n",
    "            pass\n",
    "\n",
    "        img, seg = augmentAffine(img, seg, strength=0.1)\n",
    "        img = augmentNoise(img, strength=0.02)\n",
    "        \n",
    "        if debug:\n",
    "            color_map = {\n",
    "                0: None, \n",
    "                1: (255,0,0), #ONEHOT id and RGB color\n",
    "                2: (0,255,0)\n",
    "            }\n",
    "            img_slices = img[0:1].permute(0,1,4,2,3).squeeze().unsqueeze(1)\n",
    "            seg_slices = seg[0:1].permute(0,1,4,2,3).squeeze().to(dtype=torch.int64)\n",
    "            idx_dept_with_segs, *_ = torch.nonzero(seg_slices > 0, as_tuple=True)\n",
    "            idx_dept_with_segs = idx_dept_with_segs.unique()\n",
    "\n",
    "            img_slices = img_slices[idx_dept_with_segs]\n",
    "            seg_slices = seg_slices[idx_dept_with_segs]\n",
    "            \n",
    "            pil_ov, _ = get_overlay_grid(\n",
    "                img_slices, \n",
    "                torch.nn.functional.one_hot(seg_slices,3), \n",
    "                color_map, n_per_row=10, alpha=.5\n",
    "            )\n",
    "            display(pil_ov)\n",
    "            \n",
    "        interpolated_seg = F.interpolate(seg, scale_factor=0.5, mode='nearest').squeeze(0).long()\n",
    "\n",
    "        img.requires_grad = True\n",
    "        #img_mr.requires_grad = True\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=True)\n",
    "            loss = criterion(output_j, interpolated_seg)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if epx%update_epx==update_epx-1 or epx == 0:\n",
    "            dice = dice3d(\n",
    "                torch.nn.functional.one_hot(output_j.argmax(1), 3),\n",
    "                torch.nn.functional.one_hot(interpolated_seg, 3), one_hot_torch_style=True)\n",
    "            print('epx',epx,round(time.time()-t0,2),'s','loss',round(loss.item(),6),'dice mean', round(dice.mean().item(),4),'dice',dice)\n",
    "        \n",
    "        if debug:\n",
    "            break\n",
    "#    stat_cuda('Visceral training')\n",
    "    backbone.cpu()\n",
    "    aspp.cpu() \n",
    "    head.cpu()\n",
    "\n",
    "    return backbone,aspp,head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6f476a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv sqrt class_weight tensor([0.1500, 0.6111, 2.3484], device='cuda:0')\n",
      "#CNN layer 24\n",
      "epx 0 0.33 s loss 1.200961 dice mean 0.1176 dice tensor([[3.5275e-01, 0.0000e+00, 1.4750e-04]])\n",
      "epx 49 10.95 s loss 0.169627 dice mean 0.3775 dice tensor([[0.9972, 0.0000, 0.1353]])\n",
      "epx 99 21.8 s loss 0.054101 dice mean 0.4736 dice tensor([[0.9998, 0.0000, 0.4211]])\n",
      "epx 149 32.93 s loss 0.031083 dice mean 0.4835 dice tensor([[0.9995, 0.0000, 0.4510]])\n",
      "epx 199 43.97 s loss 0.019888 dice mean 0.4602 dice tensor([[0.9996, 0.0000, 0.3810]])\n",
      "epx 249 54.97 s loss 0.029535 dice mean 0.4008 dice tensor([[0.9958, 0.0000, 0.2066]])\n",
      "epx 299 65.93 s loss 0.020487 dice mean 0.4372 dice tensor([[0.9982, 0.0000, 0.3134]])\n",
      "epx 349 76.8 s loss 0.012713 dice mean 0.4284 dice tensor([[0.9995, 0.0000, 0.2857]])\n",
      "epx 399 87.66 s loss 0.007376 dice mean 0.4346 dice tensor([[0.9995, 0.0000, 0.3043]])\n",
      "epx 449 98.52 s loss 0.016268 dice mean 0.6855 dice tensor([[0.9979, 0.8399, 0.2188]])\n",
      "epx 499 109.38 s loss 0.01205 dice mean 0.7376 dice tensor([[0.9993, 0.8481, 0.3656]])\n",
      "epx 549 120.24 s loss 0.00588 dice mean 0.4458 dice tensor([[0.9996, 0.0000, 0.3377]])\n",
      "epx 599 131.09 s loss 0.024892 dice mean 0.6757 dice tensor([[0.9957, 0.4160, 0.6154]])\n",
      "epx 649 141.95 s loss 0.01246 dice mean 0.7684 dice tensor([[0.9989, 0.7943, 0.5122]])\n",
      "epx 699 152.8 s loss 0.010082 dice mean 0.4396 dice tensor([[0.9995, 0.0000, 0.3191]])\n",
      "epx 749 163.68 s loss 0.009457 dice mean 0.4348 dice tensor([[0.9991, 0.0000, 0.3053]])\n",
      "epx 799 174.54 s loss 0.014286 dice mean 0.456 dice tensor([[0.9978, 0.0000, 0.3704]])\n",
      "epx 849 185.41 s loss 0.024054 dice mean 0.7696 dice tensor([[0.9962, 0.8937, 0.4190]])\n",
      "epx 899 196.26 s loss 0.03857 dice mean 0.7759 dice tensor([[0.9970, 0.8932, 0.4375]])\n",
      "epx 949 207.13 s loss 0.015437 dice mean 0.7394 dice tensor([[0.9975, 0.8778, 0.3429]])\n",
      "epx 999 218.01 s loss 0.021797 dice mean 0.7181 dice tensor([[0.9954, 0.7663, 0.3925]])\n",
      "epx 1049 228.87 s loss 0.003707 dice mean 0.5068 dice tensor([[0.9997, 0.0000, 0.5208]])\n",
      "epx 1099 239.76 s loss 0.005175 dice mean 0.4819 dice tensor([[0.9993, 0.0000, 0.4464]])\n",
      "epx 1149 250.66 s loss 0.004416 dice mean 0.4653 dice tensor([[0.9995, 0.0000, 0.3962]])\n",
      "epx 1199 261.55 s loss 0.003627 dice mean 0.4847 dice tensor([[0.9996, 0.0000, 0.4545]])\n",
      "epx 1249 272.42 s loss 0.142534 dice mean 0.7194 dice tensor([[0.9930, 0.7390, 0.4262]])\n",
      "epx 1299 283.3 s loss 0.008713 dice mean 0.7067 dice tensor([[0.9984, 0.8438, 0.2778]])\n",
      "epx 1349 294.17 s loss 0.009793 dice mean 0.7262 dice tensor([[0.9993, 0.7985, 0.3810]])\n",
      "epx 1399 305.08 s loss 0.005586 dice mean 0.462 dice tensor([[0.9996, 0.0000, 0.3864]])\n"
     ]
    }
   ],
   "source": [
    "def save_model(backbone,aspp,head,name):\n",
    "    torch.save(backbone.state_dict(), name + '_backbone.pth')\n",
    "    torch.save(aspp.state_dict(), name + '_aspp.pth')\n",
    "    torch.save(head.state_dict(), name + '_head.pth')\n",
    "    return None\n",
    "\n",
    "def load_model(name,output_classes,input_channels):\n",
    "    backbone, aspp, head = create_model(output_classes=output_classes,input_channels=input_channels)\n",
    "    backbone.load_state_dict(torch.load( name + '_backbone.pth'))\n",
    "    aspp.load_state_dict(torch.load(name + '_aspp.pth'))\n",
    "    head.load_state_dict(torch.load(name + '_head.pth'))\n",
    "    return backbone,aspp,head\n",
    "\n",
    "epochs = 2000\n",
    "updates = 50\n",
    "# imgs = torch.cat((imgs_train_source,imgs_train_target),dim=0)\n",
    "# label = torch.cat((labels_train_source,labels_train_target),dim=0)\n",
    "backbone,aspp,head = train_DL(training_dataset, target_dataset, validation_dataset, \n",
    "    epochs=epochs, update_epx=updates, use_mind=False, debug=False\n",
    ")\n",
    "\n",
    "save_model(backbone, aspp, head,'data/models')\n",
    "#backbone,aspp,head=load_model('Models/half_res_adapted_model_target_training',3,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0439928",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/share/data_sam1/ckruse/image_data/CrossMoDa/target_training/'\n",
    "label_path = '/share/data_supergrover1/weihsbach/tmp/crossmoda_full_set/'\n",
    "plot = False\n",
    "backbone.cuda() \n",
    "aspp.cuda() \n",
    "head.cuda() \n",
    "target_dices = torch.zeros(32)\n",
    "source_dices = torch.zeros(32)\n",
    "for i in range(32):\n",
    "    ind = i+150\n",
    "    nii_img = nib.load(path + 'crossmoda_'+ str(ind) + '_hrT2.nii.gz')\n",
    "    nii_label = nib.load(label_path + 'crossmoda_'+ str(ind) + '_hrT2_Label.nii.gz')\n",
    "    tmp = torch.from_numpy(nii_img.get_fdata())\n",
    "    label = torch.from_numpy(nii_label.get_fdata()).cuda()\n",
    "    org_size = tmp.size()  \n",
    "    size = (192,192,64)\n",
    "    tmp = crop_center_plane(tmp,size)\n",
    "    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "    org_img = torch.from_numpy(nii_img.get_fdata())\n",
    "    \n",
    "    img= tmp.float().cuda()\n",
    "    img = mindssc(img.unsqueeze(0).unsqueeze(0))\n",
    "    with torch.no_grad():\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=False)#\n",
    "    modeled_seg = F.interpolate(output_j,scale_factor=2).argmax(1)\n",
    "    modeled_seg = pad_center_plane(modeled_seg.squeeze(),org_size)\n",
    "    #print(modeled_seg.shape,torch.max(modeled_seg))\n",
    "    connectivity = 18 # only 4,8 (2D) and 26, 18, and 6 (3D) are allowed\n",
    "    np_label = modeled_seg.long().cpu().numpy().astype('int32')\n",
    "    tmp = pad_center_plane(tmp.squeeze(),org_size)\n",
    "    #plot = True\n",
    "    if plot:\n",
    "        slc = 30\n",
    "        i0 = overlaySegment(tmp[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        i1 = overlaySegment(org_img[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        fig,axs = plt.subplots(1, 2,figsize=(18, 9))\n",
    "        axs[0].imshow((i0+i1).cpu().numpy())\n",
    "        axs[1].imshow(i1.cpu().numpy())\n",
    "        plt.show()\n",
    "        fig.set_figwidth(40)\n",
    "        fig.set_figheight(10)\n",
    "    #print(modeled_seg.shape,torch.max(modeled_seg))\n",
    "    target_dices[i] = dice_coeff(modeled_seg,label)\n",
    "    print(f'image: crossmoda_{ind}_hrT2.nii.gz, dice: {target_dices[i]*100:0.2f}')\n",
    "    \n",
    "print(f'target dice mean: {target_dices.mean()*100:0.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ae4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/share/data_sam1/ckruse/image_data/CrossMoDa/target_validation/'\n",
    "plot = False\n",
    "save = True\n",
    "backbone.cuda() \n",
    "aspp.cuda() \n",
    "head.cuda() \n",
    "for i in range(32):\n",
    "    ind = i+211\n",
    "    nii_img = nib.load(path + 'crossmoda_'+ str(ind) + '_hrT2.nii.gz')\n",
    "    img_affine = nii_img.affine\n",
    "    tmp = torch.from_numpy(nii_img.get_fdata())\n",
    "    org_img = torch.from_numpy(nii_img.get_fdata())\n",
    "    org_size = tmp.size()  \n",
    "    size = (192,192,64)\n",
    "    tmp = crop_center_plane(tmp,size)\n",
    "    #print('red. shape',tmp.shape)\n",
    "    #tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size)\n",
    "    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "        \n",
    "    img= tmp.float().cuda()\n",
    "    #print(img.shape)\n",
    "    img = mindssc(img.unsqueeze(0).unsqueeze(0))\n",
    "    with torch.no_grad():\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=False)#\n",
    "    modeled_seg = F.interpolate(output_j,scale_factor=2).argmax(1)\n",
    "    modeled_seg = pad_center_plane(modeled_seg.squeeze(),org_size)\n",
    "    tmp = pad_center_plane(tmp.squeeze(),org_size)\n",
    "    #print('org shape',tmp.shape)\n",
    "    if plot:\n",
    "        slc = 30\n",
    "        i0 = overlaySegment(tmp[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        i1 = overlaySegment(org_img[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        fig,axs = plt.subplots(1, 2,figsize=(18, 9))\n",
    "        axs[0].imshow((i0+i1).cpu().numpy())\n",
    "        axs[1].imshow(i1.cpu().numpy())\n",
    "        plt.show()\n",
    "        fig.set_figwidth(40)\n",
    "        fig.set_figheight(10)\n",
    "    if save:\n",
    "        label_nii = nib.Nifti1Image(modeled_seg.float().squeeze().cpu().numpy(), img_affine)\n",
    "        nib.save(label_nii, 'Deeplab_validation/crossmoda_'+ str(ind) + '_Label.nii.gz')  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca07731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm Deeplab_validation.zip\n",
    "#!zip -r Deeplab_validation_half_res_adapted_model_target_train.zip Deeplab_validation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
