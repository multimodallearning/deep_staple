{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fc395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "7605\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# general\n",
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.update(get_vars(select=\"* -4\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from sklearn.model_selection import KFold\n",
    "import cc3d\n",
    "from mdl_seg_class\n",
    "\n",
    "# custom functions\n",
    "from mindssc import mindssc\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca60d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlaySegment(gray1,seg1,colors,flag=False):\n",
    "    H, W = seg1.squeeze().size()\n",
    "    #colors=torch.FloatTensor([0,0,0,199,67,66,225,140,154,78,129,170,45,170,170,240,110,38,111,163,91,235,175,86,202,255,52,162,0,183]).view(-1,3)/255.0\n",
    "    segs1 = F.one_hot(seg1.long(),29).float().permute(2,0,1)\n",
    "\n",
    "    seg_color = torch.mm(segs1.view(29,-1).t(),colors).view(H,W,3)\n",
    "    alpha = torch.clamp(1.0 - 0.5*(seg1>0).float(),0,1.0)\n",
    "\n",
    "    overlay = (gray1*alpha).unsqueeze(2) + seg_color*(1.0-alpha).unsqueeze(2)\n",
    "    if(flag):\n",
    "        plt.imshow((overlay).numpy()); \n",
    "        plt.axis('off');\n",
    "        plt.show()\n",
    "    return overlay# Atrous Spatial Pyramid Pooling (Segmentation Network)\n",
    "\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            # nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        # x = F.adaptive_avg_pool3d(x, (1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')  # , align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = [nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())]\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    " \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "# Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "def create_model(output_classes: int = 14,input_channels: int = 1):\n",
    "    # in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "    in_channels = torch.Tensor([input_channels, 24, 24, 32, 48, 48, 48, 64]).long()\n",
    "    mid_channels = torch.Tensor([64, 128, 192, 192, 256, 256, 256, 384]).long()\n",
    "    out_channels = torch.Tensor([24, 24, 32, 48, 48, 48, 64, 64]).long()\n",
    "    mid_stride = torch.Tensor([1, 1, 1, 2, 1, 1, 1, 1])\n",
    "    net = [nn.Identity()]\n",
    "    for i in range(8):\n",
    "        inc = int(in_channels[i])\n",
    "        midc = int(mid_channels[i])\n",
    "        outc = int(out_channels[i])\n",
    "        strd = int(mid_stride[i])\n",
    "        layer = nn.Sequential(nn.Conv3d(inc, midc, 1, bias=False), nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, midc, 3, stride=strd, padding=1, bias=False, groups=midc),\n",
    "                              nn.BatchNorm3d(midc), nn.ReLU6(True),\n",
    "                              nn.Conv3d(midc, outc, 1, bias=False), nn.BatchNorm3d(outc))\n",
    "        if i == 0:\n",
    "            layer[0] = nn.Conv3d(inc, midc, 3, padding=1, stride=2, bias=False)\n",
    "        if (inc == outc) & (strd == 1):\n",
    "            net.append(ResBlock(layer))\n",
    "        else:\n",
    "            net.append(layer)\n",
    "\n",
    "    backbone = nn.Sequential(*net)\n",
    "\n",
    "    count = 0\n",
    "    # weight initialization\n",
    "    for m in backbone.modules():\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            count += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    print('#CNN layer', count)\n",
    "    # complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "    # newer model (one more stride, no groups in head)\n",
    "    aspp = ASPP(64, (2, 4, 8, 16, 32), 128)\n",
    "    head = nn.Sequential(nn.Conv3d(128 + 24, 64, 1, padding=0, groups=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, 64, 3, groups=1, padding=1, bias=False), nn.BatchNorm3d(64), nn.ReLU(), \\\n",
    "                         nn.Conv3d(64, output_classes, 1))\n",
    "    return backbone, aspp, head\n",
    "\n",
    "\n",
    "def apply_model(backbone, aspp, head, img, checkpointing=True, return_intermediate=False):\n",
    "    if checkpointing:\n",
    "        x1 = checkpoint(backbone[:3], img)\n",
    "        x2 = checkpoint(backbone[3:], x1)\n",
    "        y = checkpoint(aspp, x2)\n",
    "        y1 = torch.cat((x1[0:1], F.interpolate(y[0:1], scale_factor=2)), 1)\n",
    "        output_j = checkpoint(head, y1)\n",
    "    else:\n",
    "        x1 = backbone[:3](img)\n",
    "        x2 = backbone[3:](x1)\n",
    "        y = aspp(x2)\n",
    "        y1 = torch.cat((x1[0:1], F.interpolate(y[0:1], scale_factor=2)), 1)\n",
    "        output_j = head(y1)\n",
    "    if return_intermediate:\n",
    "        return y1,output_j\n",
    "    else:\n",
    "        return output_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc94a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_plane(img,size):\n",
    "    s0,s1,s2 = img.size()\n",
    "    i0 = (s0-size[0])//2\n",
    "    i1 = (s1-size[1])//2\n",
    "    i2 = 0\n",
    "    img = img[i0:i0+size[0],i1:i1+size[1],i2:i2+size[2]]\n",
    "    return img\n",
    "\n",
    "    \n",
    "\n",
    "def load_source_data(normalize:bool = True,size:tuple = (192,192,120)):\n",
    "    t0 = time.time()\n",
    "\n",
    "    source_train_num = 105\n",
    "    img_train = torch.zeros(source_train_num,1,size[0],size[1],size[2])\n",
    "    label_train = torch.zeros(source_train_num,size[0],size[1],size[2])\n",
    "    path = '/share/data_sam1/ckruse/image_data/CrossMoDa/source_training_resampled_05mm/'\n",
    "    for i in range(source_train_num):\n",
    "        ind = i+1\n",
    "        tmp = torch.from_numpy(\n",
    "                    nib.load(path + 'crossmoda_'+ str(ind) + '_ceT1.nii.gz').get_fdata())\n",
    "        #print(tmp.shape) 421 421 120/107\n",
    "        #tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size)\n",
    "#        if tmp.size(-1)<120:\n",
    "#            tmp = F.pad(tmp,(0,120-tmp.size(-1)))\n",
    "        tmp = crop_center_plane(tmp,size)\n",
    "        if normalize:\n",
    "            tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "        img_train[i, 0, :, :, :] = tmp.squeeze()\n",
    "        tmp = torch.from_numpy(\n",
    "            nib.load(path + 'crossmoda_'+ str(ind) + '_Label.nii.gz').get_fdata())\n",
    "        tmp = crop_center_plane(tmp,size)\n",
    "#        if tmp.size(-1)<120:\n",
    "#            tmp = F.pad(tmp,(0,120-tmp.size(-1)))\n",
    "        #tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size)\n",
    "        label_train[i] = tmp.squeeze()\n",
    "\n",
    "    print('Time for data import:', round(time.time() - t0, 2), 's')\n",
    "    return img_train, label_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a4e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staples_training_data(normalize:bool = True,size:tuple = (192,192,120)):\n",
    "    t0 = time.time()\n",
    "\n",
    "    target_train_num = 105\n",
    "    img_train = torch.zeros(target_train_num,1,size[0],size[1],size[2])\n",
    "    label_train = torch.zeros(target_train_num,size[0],size[1],size[2])\n",
    "    path = '/share/data_supergrover1/hansen/temp/crossmoda/data/target_training/'\n",
    "    for i in range(target_train_num):\n",
    "        ind = i+106\n",
    "        tmp = torch.from_numpy(\n",
    "                    nib.load(path + 'crossmoda_'+ str(ind) + '_hrT2.nii.gz').get_fdata())\n",
    "        if tmp.size(-1)<size[-1]:\n",
    "            tmp = F.pad(tmp,(0,size[-1]-tmp.size(-1)))\n",
    "        tmp = crop_center_plane(tmp,size)\n",
    "        if normalize:\n",
    "            tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "        img_train[i, 0, :, :, :] = tmp.squeeze()\n",
    "        tmp = torch.from_numpy(\n",
    "                    nib.load(path + 'crossmoda_'+ str(ind) + '_Label.nii.gz').get_fdata())\n",
    "        if tmp.size(-1)<size[-1]:\n",
    "            tmp = F.pad(tmp,(0,size[-1]-tmp.size(-1)))\n",
    "        tmp = crop_center_plane(tmp,size)\n",
    "\n",
    "        label_train[i] = tmp.squeeze()\n",
    "\n",
    "    print('Time for data import:', round(time.time() - t0, 2), 's')\n",
    "    return img_train, label_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49da3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slice_overlay(img,seg,slc= [100,60,100]):\n",
    "    print('image size:',img.size())\n",
    "    \n",
    "    i1 = overlaySegment(img[0,:,slc[1],:], seg[:,slc[1],:], flag=False)\n",
    "    i2 = overlaySegment(img[0,:,:,slc[2]], seg[:,:,slc[2]], flag=False)\n",
    "    i3 = overlaySegment(img[0,slc[0],:,:], seg[slc[0],:,:], flag=False)\n",
    "    fig,axs = plt.subplots(1, 3)\n",
    "    axs[0].imshow(i1.cpu().numpy())\n",
    "    axs[1].imshow(i2.cpu().numpy())\n",
    "    axs[2].imshow(i3.cpu().numpy())\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6489b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for data import: 46.79 s\n",
      "Time for data import: 129.2 s\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "imgs_train_target, labels_train_target = load_staples_training_data(normalize = True,size = (192,192,64))\n",
    "imgs_train_source, labels_train_source = load_source_data(normalize = True,size = (192,192,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684c5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentAffine(img_in, seg_in, strength=0.05):\n",
    "    \"\"\"\n",
    "    3D affine augmentation on image and segmentation mini-batch on GPU.\n",
    "    (affine transf. is centered: trilinear interpolation and zero-padding used for sampling)\n",
    "    :input: img_in batch (torch.cuda.FloatTensor), seg_in batch (torch.cuda.LongTensor)\n",
    "    :return: augmented BxCxTxHxW image batch (torch.cuda.FloatTensor), augmented BxTxHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "    B,C,D,H,W = img_in.size()\n",
    "    affine_matrix = (torch.eye(3,4).unsqueeze(0) + torch.randn(B, 3, 4) * strength).to(img_in.device)\n",
    "\n",
    "    meshgrid = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)))\n",
    "\n",
    "    img_out = F.grid_sample(img_in, meshgrid,padding_mode='border')\n",
    "    seg_out = F.grid_sample(seg_in.float().unsqueeze(1), meshgrid, mode='nearest').long().squeeze(1)\n",
    "\n",
    "    return img_out, seg_out\n",
    "\n",
    "def augmentNoise(img_in,strength=0.05):\n",
    "    return img_in + strength*torch.randn_like(img_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c902fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training routine\n",
    "def train_DL(imgs,segs,epochs=500,update_epx = 50,use_mind = True):\n",
    "    img_num,C,_,_,_ = imgs.size()\n",
    "    if use_mind:\n",
    "        C =12\n",
    "    else:\n",
    "        C = 1\n",
    "    ds_segs = F.interpolate(segs.unsqueeze(0),scale_factor=0.5,mode='nearest').squeeze(0)\n",
    "    num_class = int(torch.max(segs).item()+1)\n",
    "    backbone, aspp, head = create_model(output_classes=num_class,input_channels=C)\n",
    "    optimizer = torch.optim.Adam(list(backbone.parameters())+list(aspp.parameters())+list(head.parameters()),lr=0.001)\n",
    "    class_weight = torch.sqrt(1.0/(torch.bincount(segs.long().view(-1)).float()))\n",
    "    class_weight = class_weight/class_weight.mean()\n",
    "    class_weight[0] = 0.15\n",
    "    class_weight = class_weight.cuda()\n",
    "    print('inv sqrt class_weight',class_weight)\n",
    "    criterion = nn.CrossEntropyLoss(class_weight)\n",
    "    scaler = amp.GradScaler()\n",
    "    backbone.cuda() \n",
    "    backbone.train()\n",
    "    aspp.cuda() \n",
    "    aspp.train()\n",
    "    head.cuda() \n",
    "    head.train()\n",
    "    t0 = time.time()\n",
    "    for epx in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        ind = torch.randint(0,img_num,(1,))\n",
    "        img = imgs[ind:ind+1].cuda()\n",
    "        seg = segs[ind:ind+1].long().cuda()\n",
    "        if use_mind:\n",
    "            img = mindssc(img)\n",
    "        img, seg = augmentAffine(img, seg, strength=0.1)\n",
    "        img = augmentNoise(img,strength=0.02)\n",
    "        seg = F.interpolate(seg.float().unsqueeze(0),scale_factor=0.5,mode='nearest').squeeze(0).long()\n",
    "        img.requires_grad = True\n",
    "        #img_mr.requires_grad = True\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=True)\n",
    "            loss = criterion(output_j,seg)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if epx%update_epx==update_epx-1 or epx == 0:\n",
    "            dice = dice_coeff(output_j.argmax(1),seg)\n",
    "            print('epx',epx,round(time.time()-t0,2),'s','loss',round(loss.item(),6),'dice mean', round(dice.mean().item(),4),'dice',dice)\n",
    "#    stat_cuda('Visceral training')\n",
    "    backbone.cpu()\n",
    "    aspp.cpu() \n",
    "    head.cpu() \n",
    "    return backbone,aspp,head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6f476a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/data_sam1/ckruse/miniconda3/envs/pt181cu102/lib/python3.8/site-packages/torch/nn/functional.py:3502: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#CNN layer 24\n",
      "inv sqrt class_weight tensor([0.1500, 0.5172, 2.4564], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/data_sam1/ckruse/miniconda3/envs/pt181cu102/lib/python3.8/site-packages/torch/nn/functional.py:3890: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/share/data_sam1/ckruse/miniconda3/envs/pt181cu102/lib/python3.8/site-packages/torch/nn/functional.py:3828: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epx 0 5.48 s loss 1.173589 dice mean 0.006 dice tensor([0.0060])\n",
      "epx 49 261.58 s loss 0.14849 dice mean 0.0 dice tensor([0.])\n",
      "epx 99 527.87 s loss 0.072097 dice mean 0.0 dice tensor([0., 0.])\n",
      "epx 149 795.07 s loss 0.058573 dice mean 0.1088 dice tensor([0.2177, 0.0000])\n",
      "epx 199 1062.63 s loss 0.03156 dice mean 0.0 dice tensor([0., 0.])\n",
      "epx 249 1330.62 s loss 0.026504 dice mean 0.2031 dice tensor([0.4062, 0.0000])\n",
      "epx 299 1598.62 s loss 0.015467 dice mean 0.2812 dice tensor([0.5625, 0.0000])\n",
      "epx 349 1866.45 s loss 0.035605 dice mean 0.0 dice tensor([0., 0.])\n",
      "epx 399 2134.39 s loss 0.013029 dice mean 0.2774 dice tensor([0.2930, 0.2618])\n",
      "epx 449 2402.47 s loss 0.009394 dice mean 0.2839 dice tensor([0.2267, 0.3410])\n",
      "epx 499 2670.51 s loss 0.010311 dice mean 0.4167 dice tensor([0.6934, 0.1401])\n",
      "epx 549 2938.68 s loss 0.032879 dice mean 0.5712 dice tensor([0.8256, 0.3167])\n",
      "epx 599 3207.06 s loss 0.012772 dice mean 0.1363 dice tensor([0.0519, 0.2206])\n",
      "epx 649 3475.59 s loss 0.018782 dice mean 0.4024 dice tensor([0.8047, 0.0000])\n",
      "epx 699 3743.98 s loss 0.007529 dice mean 0.4368 dice tensor([0.6997, 0.1739])\n",
      "epx 749 4012.31 s loss 0.010797 dice mean 0.0 dice tensor([0.])\n",
      "epx 799 4280.22 s loss 0.011382 dice mean 0.5699 dice tensor([0.8400, 0.2998])\n",
      "epx 849 4547.89 s loss 0.010078 dice mean 0.6071 dice tensor([0.8204, 0.3939])\n",
      "epx 899 4815.24 s loss 0.012043 dice mean 0.5201 dice tensor([0.6984, 0.3419])\n",
      "epx 949 5082.15 s loss 0.008825 dice mean 0.438 dice tensor([0.4338, 0.4422])\n",
      "epx 999 5349.23 s loss 0.009808 dice mean 0.0857 dice tensor([0.0000, 0.1714])\n",
      "epx 1049 5616.15 s loss 0.006303 dice mean 0.2569 dice tensor([0.2752, 0.2385])\n",
      "epx 1099 5883.82 s loss 0.010471 dice mean 0.4608 dice tensor([0.5523, 0.3692])\n",
      "epx 1149 6151.28 s loss 0.006368 dice mean 0.4875 dice tensor([0.6439, 0.3310])\n",
      "epx 1199 6418.97 s loss 0.010831 dice mean 0.4862 dice tensor([0.5411, 0.4314])\n",
      "epx 1249 6686.85 s loss 0.003209 dice mean 0.4998 dice tensor([0.6619, 0.3377])\n",
      "epx 1299 6954.43 s loss 0.008929 dice mean 0.4881 dice tensor([0.6513, 0.3250])\n",
      "epx 1349 7222.2 s loss 0.004655 dice mean 0.5427 dice tensor([0.7871, 0.2984])\n",
      "epx 1399 7489.62 s loss 0.004645 dice mean 0.6158 dice tensor([0.8571, 0.3745])\n",
      "epx 1449 7757.11 s loss 0.008025 dice mean 0.632 dice tensor([0.8967, 0.3673])\n",
      "epx 1499 8024.13 s loss 0.004393 dice mean 0.5204 dice tensor([0.5146, 0.5263])\n",
      "epx 1549 8291.19 s loss 0.021715 dice mean 0.6781 dice tensor([0.8786, 0.4776])\n",
      "epx 1599 8558.34 s loss 0.002651 dice mean 0.0 dice tensor([0.])\n",
      "epx 1649 8825.04 s loss 0.00283 dice mean 0.5735 dice tensor([0.8095, 0.3375])\n",
      "epx 1699 9091.97 s loss 0.003959 dice mean 0.5804 dice tensor([0.8440, 0.3169])\n",
      "epx 1749 9359.13 s loss 0.007597 dice mean 0.905 dice tensor([0.9050])\n",
      "epx 1799 9626.64 s loss 0.014565 dice mean 0.6515 dice tensor([0.9202, 0.3828])\n",
      "epx 1849 9893.69 s loss 0.008374 dice mean 0.6294 dice tensor([0.8509, 0.4080])\n",
      "epx 1899 10160.72 s loss 0.00323 dice mean 0.5636 dice tensor([0.7785, 0.3487])\n",
      "epx 1949 10427.84 s loss 0.007515 dice mean 0.5926 dice tensor([0.7070, 0.4781])\n",
      "epx 1999 10695.45 s loss 0.002734 dice mean 0.2026 dice tensor([0.2531, 0.1522])\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "updates = 50\n",
    "imgs = torch.cat((imgs_train_source,imgs_train_target),dim=0)\n",
    "label = torch.cat((labels_train_source,labels_train_target),dim=0)\n",
    "backbone,aspp,head = train_DL(imgs,label,epochs=epochs,update_epx =updates,use_mind = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2921a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def pad_center_plane(img,size):\n",
    "    s0,s1,s2 = img.size()\n",
    "    i0 = (size[0]-s0)//2\n",
    "    i1 = (size[1]-s1)//2\n",
    "    i2 = 0\n",
    "    pd = (i2,size[2]-s2-i2,i1,size[1]-s1-i1,i0,size[0]-s0-i0)\n",
    "    #print('pad',pd)\n",
    "    img = F.pad(img, pd, \"constant\", 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0439928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: crossmoda_150_hrT2.nii.gz, dice: 77.95\n",
      "image: crossmoda_151_hrT2.nii.gz, dice: 75.39\n",
      "image: crossmoda_152_hrT2.nii.gz, dice: 60.75\n",
      "image: crossmoda_153_hrT2.nii.gz, dice: 76.53\n",
      "image: crossmoda_154_hrT2.nii.gz, dice: 58.11\n",
      "image: crossmoda_155_hrT2.nii.gz, dice: 51.08\n",
      "image: crossmoda_156_hrT2.nii.gz, dice: 69.33\n",
      "image: crossmoda_157_hrT2.nii.gz, dice: 19.09\n",
      "image: crossmoda_158_hrT2.nii.gz, dice: 68.11\n",
      "image: crossmoda_159_hrT2.nii.gz, dice: 79.19\n",
      "image: crossmoda_160_hrT2.nii.gz, dice: 74.65\n",
      "image: crossmoda_161_hrT2.nii.gz, dice: 51.15\n",
      "image: crossmoda_162_hrT2.nii.gz, dice: 78.01\n",
      "image: crossmoda_163_hrT2.nii.gz, dice: 58.91\n",
      "image: crossmoda_164_hrT2.nii.gz, dice: 63.81\n",
      "image: crossmoda_165_hrT2.nii.gz, dice: 44.12\n",
      "image: crossmoda_166_hrT2.nii.gz, dice: 63.75\n",
      "image: crossmoda_167_hrT2.nii.gz, dice: 80.05\n",
      "image: crossmoda_168_hrT2.nii.gz, dice: 69.89\n",
      "image: crossmoda_169_hrT2.nii.gz, dice: 31.71\n",
      "image: crossmoda_170_hrT2.nii.gz, dice: 31.81\n",
      "image: crossmoda_171_hrT2.nii.gz, dice: 71.96\n",
      "image: crossmoda_172_hrT2.nii.gz, dice: 83.10\n",
      "image: crossmoda_173_hrT2.nii.gz, dice: 75.48\n",
      "image: crossmoda_174_hrT2.nii.gz, dice: 78.68\n",
      "image: crossmoda_175_hrT2.nii.gz, dice: 64.28\n",
      "image: crossmoda_176_hrT2.nii.gz, dice: 79.15\n",
      "image: crossmoda_177_hrT2.nii.gz, dice: 76.74\n",
      "image: crossmoda_178_hrT2.nii.gz, dice: 75.47\n",
      "image: crossmoda_179_hrT2.nii.gz, dice: 72.21\n",
      "image: crossmoda_180_hrT2.nii.gz, dice: 58.39\n",
      "image: crossmoda_181_hrT2.nii.gz, dice: 57.99\n",
      "target dice mean: 64.90\n"
     ]
    }
   ],
   "source": [
    "path = '/share/data_sam1/ckruse/image_data/CrossMoDa/target_training/'\n",
    "label_path = '/share/data_supergrover1/weihsbach/tmp/crossmoda_full_set/'\n",
    "plot = False\n",
    "backbone.cuda() \n",
    "aspp.cuda() \n",
    "head.cuda() \n",
    "target_dices = torch.zeros(32)\n",
    "source_dices = torch.zeros(32)\n",
    "for i in range(32):\n",
    "    ind = i+150\n",
    "    nii_img = nib.load(path + 'crossmoda_'+ str(ind) + '_hrT2.nii.gz')\n",
    "    nii_label = nib.load(label_path + 'crossmoda_'+ str(ind) + '_hrT2_Label.nii.gz')\n",
    "    tmp = torch.from_numpy(nii_img.get_fdata())\n",
    "    label = torch.from_numpy(nii_label.get_fdata()).cuda()\n",
    "    org_size = tmp.size()  \n",
    "    size = (192,192,64)\n",
    "    tmp = crop_center_plane(tmp,size)\n",
    "    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "    org_img = torch.from_numpy(nii_img.get_fdata())\n",
    "    \n",
    "    img= tmp.float().cuda()\n",
    "    img = mindssc(img.unsqueeze(0).unsqueeze(0))\n",
    "    with torch.no_grad():\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=False)#\n",
    "    modeled_seg = F.interpolate(output_j,scale_factor=2).argmax(1)\n",
    "    modeled_seg = pad_center_plane(modeled_seg.squeeze(),org_size)\n",
    "    #print(modeled_seg.shape,torch.max(modeled_seg))\n",
    "    connectivity = 18 # only 4,8 (2D) and 26, 18, and 6 (3D) are allowed\n",
    "    np_label = modeled_seg.long().cpu().numpy().astype('int32')\n",
    "    tmp = pad_center_plane(tmp.squeeze(),org_size)\n",
    "    #plot = True\n",
    "    if plot:\n",
    "        slc = 30\n",
    "        i0 = overlaySegment(tmp[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        i1 = overlaySegment(org_img[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        fig,axs = plt.subplots(1, 2,figsize=(18, 9))\n",
    "        axs[0].imshow((i0+i1).cpu().numpy())\n",
    "        axs[1].imshow(i1.cpu().numpy())\n",
    "        plt.show()\n",
    "        fig.set_figwidth(40)\n",
    "        fig.set_figheight(10)\n",
    "    #print(modeled_seg.shape,torch.max(modeled_seg))\n",
    "    target_dices[i] = dice_coeff(modeled_seg,label)\n",
    "    print(f'image: crossmoda_{ind}_hrT2.nii.gz, dice: {target_dices[i]*100:0.2f}')\n",
    "    \n",
    "print(f'target dice mean: {target_dices.mean()*100:0.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "809ae4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/share/data_sam1/ckruse/image_data/CrossMoDa/target_validation/'\n",
    "plot = False\n",
    "save = True\n",
    "backbone.cuda() \n",
    "aspp.cuda() \n",
    "head.cuda() \n",
    "for i in range(32):\n",
    "    ind = i+211\n",
    "    nii_img = nib.load(path + 'crossmoda_'+ str(ind) + '_hrT2.nii.gz')\n",
    "    img_affine = nii_img.affine\n",
    "    tmp = torch.from_numpy(nii_img.get_fdata())\n",
    "    org_img = torch.from_numpy(nii_img.get_fdata())\n",
    "    org_size = tmp.size()  \n",
    "    size = (192,192,64)\n",
    "    tmp = crop_center_plane(tmp,size)\n",
    "    #print('red. shape',tmp.shape)\n",
    "    #tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size)\n",
    "    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "        \n",
    "    img= tmp.float().cuda()\n",
    "    #print(img.shape)\n",
    "    img = mindssc(img.unsqueeze(0).unsqueeze(0))\n",
    "    with torch.no_grad():\n",
    "        with amp.autocast(enabled=True):\n",
    "            output_j = apply_model(backbone,aspp,head,img,checkpointing=False)#\n",
    "    modeled_seg = F.interpolate(output_j,scale_factor=2).argmax(1)\n",
    "    modeled_seg = pad_center_plane(modeled_seg.squeeze(),org_size)\n",
    "    tmp = pad_center_plane(tmp.squeeze(),org_size)\n",
    "    #print('org shape',tmp.shape)\n",
    "    if plot:\n",
    "        slc = 30\n",
    "        i0 = overlaySegment(tmp[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        i1 = overlaySegment(org_img[:,:,slc].cpu(), modeled_seg[:,:,slc].cpu(), flag=False)\n",
    "        fig,axs = plt.subplots(1, 2,figsize=(18, 9))\n",
    "        axs[0].imshow((i0+i1).cpu().numpy())\n",
    "        axs[1].imshow(i1.cpu().numpy())\n",
    "        plt.show()\n",
    "        fig.set_figwidth(40)\n",
    "        fig.set_figheight(10)\n",
    "    if save:\n",
    "        label_nii = nib.Nifti1Image(modeled_seg.float().squeeze().cpu().numpy(), img_affine)\n",
    "        nib.save(label_nii, 'Deeplab_validation/crossmoda_'+ str(ind) + '_Label.nii.gz')  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ca07731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'Deeplab_validation.zip': No such file or directory\n",
      "  adding: Deeplab_validation/ (stored 0%)\n",
      "  adding: Deeplab_validation/crossmoda_240_Label.nii.gz (deflated 95%)\n",
      "  adding: Deeplab_validation/crossmoda_242_Label.nii.gz (deflated 99%)\n",
      "  adding: Deeplab_validation/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: Deeplab_validation/crossmoda_211_Label.nii.gz (deflated 97%)\n",
      "  adding: Deeplab_validation/crossmoda_212_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_213_Label.nii.gz (deflated 93%)\n",
      "  adding: Deeplab_validation/crossmoda_214_Label.nii.gz (deflated 99%)\n",
      "  adding: Deeplab_validation/crossmoda_215_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_216_Label.nii.gz (deflated 97%)\n",
      "  adding: Deeplab_validation/crossmoda_217_Label.nii.gz (deflated 99%)\n",
      "  adding: Deeplab_validation/crossmoda_218_Label.nii.gz (deflated 96%)\n",
      "  adding: Deeplab_validation/crossmoda_219_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_220_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_221_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_222_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_223_Label.nii.gz (deflated 97%)\n",
      "  adding: Deeplab_validation/crossmoda_224_Label.nii.gz (deflated 97%)\n",
      "  adding: Deeplab_validation/crossmoda_225_Label.nii.gz (deflated 97%)\n",
      "  adding: Deeplab_validation/crossmoda_226_Label.nii.gz (deflated 99%)\n",
      "  adding: Deeplab_validation/crossmoda_227_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_228_Label.nii.gz (deflated 97%)\n",
      "  adding: Deeplab_validation/crossmoda_229_Label.nii.gz (deflated 94%)\n",
      "  adding: Deeplab_validation/crossmoda_230_Label.nii.gz (deflated 97%)\n",
      "  adding: Deeplab_validation/crossmoda_231_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_232_Label.nii.gz (deflated 97%)\n",
      "  adding: Deeplab_validation/crossmoda_233_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_234_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_235_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_236_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_237_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_238_Label.nii.gz (deflated 96%)\n",
      "  adding: Deeplab_validation/crossmoda_239_Label.nii.gz (deflated 98%)\n",
      "  adding: Deeplab_validation/crossmoda_241_Label.nii.gz (deflated 99%)\n"
     ]
    }
   ],
   "source": [
    "#!rm Deeplab_validation.zip\n",
    "#!zip -r Deeplab_validation_half_res_adapted_model_target_train.zip Deeplab_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f5a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(backbone,aspp,head,name):\n",
    "    torch.save(backbone.state_dict(), name + '_backbone.pth')\n",
    "    torch.save(aspp.state_dict(), name + '_aspp.pth')\n",
    "    torch.save(head.state_dict(), name + '_head.pth')\n",
    "    return None\n",
    "\n",
    "def load_model(name,output_classes,input_channels):\n",
    "    backbone, aspp, head = create_model(output_classes=output_classes,input_channels=input_channels)\n",
    "    backbone.load_state_dict(torch.load( name + '_backbone.pth'))\n",
    "    aspp.load_state_dict(torch.load(name + '_aspp.pth'))\n",
    "    head.load_state_dict(torch.load(name + '_head.pth'))\n",
    "    return backbone,aspp,head\n",
    "\n",
    "save_model(backbone,aspp,head,'Models/half_res_adapted_model_target_training')\n",
    "#backbone,aspp,head=load_model('Models/half_res_adapted_model_target_training',3,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8b927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
