{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name      Util    Mem free  Cuda             User(s)\n",
      "----  -----------  ------  ----------  ---------------  ------------\n",
      "   0  TITAN RTX       0 %   10341 MiB  11.2(460.73.01)  root, hansen\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   1  NVIDIA GeForce RTX 2080 Ti  ->  cuda:0\n",
      "1.9.0a0+gitdfbd030\n",
      "8204\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -4\", force=True))\n",
    "import pickle\n",
    "import gzip\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "from curriculum_deeplab.data_parameters import DataParamMode\n",
    "import torchio as tio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import nibabel as nib\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import visualize_seg\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in: /share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab\n"
     ]
    }
   ],
   "source": [
    "def in_notebook():\n",
    "    try:\n",
    "        get_ipython().__class__.__name__\n",
    "        return True\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "if in_notebook:\n",
    "    THIS_SCRIPT_DIR = os.path.abspath('')\n",
    "else:\n",
    "    THIS_SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "print(f\"Running in: {THIS_SCRIPT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/output/_summer-sound-913_fold0_epx39/train_label_snapshot.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30303/3634594306.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/output/**/*.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdp_mean_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_2d_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_2d_modified_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mgt_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_2d_modified_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/.venv/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "corrs = {}\n",
    "def get_corr_coeff(inp):\n",
    "    return np.corrcoef(torch.tensor(dp_mean_weight).cpu().detach().numpy(), inp)[0,1]\n",
    "\n",
    "for pth in glob.glob('./data/output/**/*.pth'):\n",
    "    print(pth)\n",
    "    data = torch.load(pth)\n",
    "    dp_mean_weight, *_, d_ids, dataset_idxs, _2d_labels, _2d_modified_labels = data\n",
    "    gt_sum = _2d_modified_labels.sum((-2,-1))\n",
    "    pp_sum = _2d_labels.sum((-2,-1))\n",
    "    intersect = torch.logical_and(_2d_labels, _2d_modified_labels).sum((-2,-1))\n",
    "    dice = 2*intersect/(pp_sum+gt_sum)\n",
    "    all_elems = dict(\n",
    "            gt_sum=get_corr_coeff(gt_sum),\n",
    "            intersect=get_corr_coeff(intersect),\n",
    "            pp_sum=get_corr_coeff(pp_sum),\n",
    "            sqrt=get_corr_coeff(np.sqrt(gt_sum)),\n",
    "            log=get_corr_coeff(np.log(gt_sum+gt_sum.max().sqrt())), # this seems to be right\n",
    "            dice=get_corr_coeff(dice)\n",
    "    )\n",
    "    corrs[pth.split('/')[-2]] = all_elems\n",
    "    # break\n",
    "df = pd.DataFrame(corrs)\n",
    "df.to_csv('pd.csv')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2212\n"
     ]
    }
   ],
   "source": [
    "data = torch.load('/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/data/output/_clean-morning-918_fold0_epx39/train_label_snapshot.pth')\n",
    "dp_mean_weight, disturb_flags, d_ids, dataset_idxs, _2d_labels, _2d_modified_labels, _2d_predictions = data\n",
    "print(len(dp_mean_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_sum = _2d_modified_labels.sum((-2,-1))\n",
    "pp_sum = _2d_predictions.sum((-2,-1)).squeeze(1).cpu()\n",
    "rgt_sum =  _2d_labels.sum((-2,-1)).cpu()\n",
    "intersect = torch.logical_and(_2d_labels, _2d_modified_labels).sum((-2,-1))\n",
    "dice = 2*intersect/(rgt_sum+gt_sum+1)\n",
    "all_elems = \\\n",
    "    dict(\n",
    "        gt_sum=gt_sum, \n",
    "        dice=dice, \n",
    "        pp_sum=pp_sum,\n",
    "        sqrt_gt_sum=np.sqrt(gt_sum),\n",
    "        log_gt_sum=np.log(gt_sum+1)+1, # this seems to be right\n",
    "        e_log_gt_sum=torch.tensor(dp_mean_weight)/(np.log(gt_sum+np.exp(1))+np.exp(1))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_sum 0.8242536552818129\n",
      "dice 0.6335904766732602\n",
      "pp_sum 0.799543165879448\n",
      "sqrt_gt_sum 0.7038407582628009\n",
      "log_gt_sum 0.49262799510469135\n",
      "e_log_gt_sum 0.8325085837655076\n"
     ]
    }
   ],
   "source": [
    "for name, metric in all_elems.items():\n",
    "    print(name, np.corrcoef(torch.tensor(dp_mean_weight).cpu().detach().numpy(), metric)[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corr coeff DP weight vs. DP reweight with e_log_gt_sum 0.7811313871631291\n"
     ]
    }
   ],
   "source": [
    "print(\"Corr coeff DP weight vs. DP reweight with\", name, np.corrcoef(torch.tensor(dp_mean_weight)/all_elems['e_log_gt_sum'].cpu().detach().numpy(), dice)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cerulean-thunder-1018_fold0_epx59/train_label_snapshot.pth has 3566 parameters.\n",
      "disturbed_bool_vect nan\n",
      "gt_sum 0.8278054469506166\n",
      "dice 0.8095933224664398\n",
      "pp_sum 0.8200475012664931\n",
      "sqrt_gt_sum 0.7831021758690532\n",
      "log_gt_sum 0.6618948166652153\n",
      "e_log_gt_sum 0.6739964577386311\n",
      "inv_e_log_gt_sum -0.6168721418098405\n",
      "Corr coeff DP_weight/dice 0.81 vs. DP_reweight/dice with gt_sum 0.67\n",
      "Corr coeff DP_weight/dice 0.81 vs. DP_reweight/dice with pp_sum 0.69\n",
      "Corr coeff DP_weight/dice 0.81 vs. DP_reweight/dice with sqrt_gt_sum 0.67\n",
      "Corr coeff DP_weight/dice 0.81 vs. DP_reweight/dice with log_gt_sum 0.75\n",
      "Corr coeff DP_weight/dice 0.81 vs. DP_reweight/dice with e_log_gt_sum 0.83\n",
      "Corr coeff DP_weight/dice 0.81 vs. DP_reweight/dice with inv_e_log_gt_sum 0.68\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 960x320 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "reweighted = torch.tensor(dp_mean_weight)/(all_elems['log_gt_sum'])\n",
    "\n",
    "sel_idxs = reweighted<100\n",
    "srtd_reweighted = reweighted[sel_idxs]\n",
    "srtd_dice_reweighted = dice[sel_idxs]\n",
    "\n",
    "srtd = sorted(zip(srtd_reweighted, srtd_dice_reweighted))\n",
    "srtd_reweighted, srtd_dice_reweighted = zip(*srtd)\n",
    "srtd_reweighted, srtd_dice_reweighted = torch.stack(srtd_reweighted), torch.stack(srtd_dice_reweighted)\n",
    "\n",
    "# Show weights and weights with compensation\n",
    "fig, axs = plt.subplots(1,4, figsize=(12, 4), dpi=80)\n",
    "sc1 = axs[0].scatter(\n",
    "    range(len(dp_mean_weight)), \n",
    "    torch.tensor(dp_mean_weight).cpu().detach().numpy(), c=dice,s=1, cmap='plasma');\n",
    "sc2 = axs[1].scatter(\n",
    "    range(len(dp_mean_weight)), \n",
    "    reweighted.cpu().detach().numpy(), \n",
    "    s=1,c=dice, cmap='plasma')\n",
    "sc3 = axs[2].scatter(\n",
    "    range(len(srtd_reweighted)), \n",
    "    srtd_reweighted.detach().numpy(), \n",
    "    s=1,c=srtd_dice_reweighted, cmap='plasma');\n",
    "# plt.colorbar()\n",
    "sc4 = axs[3].scatter(\n",
    "    range(len(srtd_reweighted)), \n",
    "    torch.sigmoid(srtd_reweighted-srtd_reweighted.mean()).detach().numpy(), \n",
    "    s=1,c=srtd_dice_reweighted, cmap='plasma');\n",
    "plt.show()\n",
    "\n",
    "sg_reweighted = torch.sigmoid(reweighted-reweighted.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sample(b_image=None, b_label=None, scale_factor=1.,\n",
    "                       yield_2d=False):\n",
    "    if yield_2d:\n",
    "        scale = [scale_factor]*2\n",
    "        im_mode = 'bilinear'\n",
    "    else:\n",
    "        scale = [scale_factor]*3\n",
    "        im_mode = 'trilinear'\n",
    "\n",
    "    if b_image is not None:\n",
    "        b_image = F.interpolate(\n",
    "            b_image.unsqueeze(1), scale_factor=scale, mode=im_mode, align_corners=True,\n",
    "            recompute_scale_factor=False\n",
    "        )\n",
    "        b_image = b_image.squeeze(1)\n",
    "\n",
    "    if b_label is not None:\n",
    "        b_label = F.interpolate(\n",
    "            b_label.unsqueeze(1).float(), scale_factor=scale, mode='nearest',\n",
    "            recompute_scale_factor=False\n",
    "        ).long()\n",
    "        b_label = b_label.squeeze(1)\n",
    "\n",
    "    return b_image, b_label\n",
    "    \n",
    "data = zip(sg_reweighted, d_ids, _2d_labels, _2d_modified_labels, _2d_predictions)\n",
    "\n",
    "\n",
    "samples_sorted = sorted(data, key=lambda tpl: tpl[0])\n",
    "(dp_weightss, d_idss, _2d_lbl, _2d_ml, _2d_p) = zip(*samples_sorted)\n",
    "\n",
    "overlay_text_list = [f\"id:{d_id} gwd:{instance_p.item():.2f}\" \\\n",
    "    for d_id, instance_p in zip(d_idss, dp_weightss)]\n",
    "\n",
    "visualize_seg(in_type=\"batch_2D\",\n",
    "    img=interpolate_sample(b_label=torch.stack(_2d_lbl), scale_factor=.5, yield_2d=True)[1].unsqueeze(1),\n",
    "    seg=interpolate_sample(b_label=4*torch.stack(_2d_p).squeeze(1), scale_factor=.5, yield_2d=True)[1],\n",
    "    ground_truth=interpolate_sample(b_label=torch.stack(_2d_ml), scale_factor=.5, yield_2d=True)[1],\n",
    "    crop_to_non_zero_seg=False,\n",
    "    alpha_seg = .5,\n",
    "    alpha_gt = .5,\n",
    "    n_per_row=70,\n",
    "    overlay_text=overlay_text_list,\n",
    "    annotate_color=(0,255,255),\n",
    "    file_path=\"out_reweight.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        'reweighted_weigths': sg_reweighted,\n",
    "        'd_ids': d_ids \n",
    "    }, \n",
    "    'fixed_weights.pth'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction = \\\n",
    "    [\n",
    "        1.0,\n",
    "        1/(1+gt_sum).numpy(), \n",
    "        1/(1+np.sqrt(gt_sum)).numpy(),\n",
    "        1/np.log(gt_sum+gt_sum.max().sqrt()).numpy(),\n",
    "    ]\n",
    "\n",
    "for crr in correction:\n",
    "    print(np.corrcoef(torch.tensor(dp_mean_weight).cpu().detach().numpy()*crr, dice)[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "corrs = {}\n",
    "def get_corr_coeff(inp):\n",
    "    return np.corrcoef(torch.tensor(dp_mean_weight).cpu().detach().numpy(), inp)[0,1]\n",
    "\n",
    "            if do_affine:\n",
    "                affine_matrix = (torch.eye(2,3).unsqueeze(0) + \\\n",
    "                    affine_strength * (1/10*torch.randn(B,2,3)+1.)).to(common_device)\n",
    "\n",
    "                affine_disp = F.affine_grid(affine_matrix, torch.Size((B,1,H,W)),\n",
    "                                        align_corners=False)\n",
    "                grid += (affine_disp-id_grid)\n",
    "\n",
    "        else:\n",
    "            assert len(common_shape) == 4, \\\n",
    "                f\"Augmenting 3D. Input batch \" \\\n",
    "                f\"should be BxDxHxW but is {common_shape}.\"\n",
    "            B,D,H,W = common_shape\n",
    "\n",
    "            identity = torch.eye(3,4).expand(B,3,4).to(common_device)\n",
    "            id_grid = F.affine_grid(identity, torch.Size((B,3,D,H,W)),\n",
    "                align_corners=False)\n",
    "\n",
    "            grid = id_grid\n",
    "\n",
    "            if do_bspline:\n",
    "                bspline = torch.nn.Sequential(\n",
    "                    nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "                    nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "                    nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2))\n",
    "                ).to(b_image.device)\n",
    "                dim_strength = (torch.tensor([D,H,W]).float()*bspline_strength).to(common_device)\n",
    "\n",
    "                rand_control_points = dim_strength.view(1,3,1,1,1)  * \\\n",
    "                    (\n",
    "                        1/10*torch.randn(B, 3, bspline_num_ctl_points, bspline_num_ctl_points, bspline_num_ctl_points)+1.\n",
    "                    ).to(b_image.device)\n",
    "\n",
    "                bspline_disp = bspline(rand_control_points)\n",
    "\n",
    "                bspline_disp = torch.nn.functional.interpolate(\n",
    "                    bspline_disp, size=(D,H,W), mode='trilinear', align_corners=True\n",
    "                ).permute(0,2,3,4,1)\n",
    "\n",
    "                grid += bspline_disp\n",
    "\n",
    "            if do_affine:\n",
    "                affine_matrix = (torch.eye(3,4).unsqueeze(0) + \\\n",
    "                    affine_strength * (1/10*torch.randn(B,3,4)+1.)).to(common_device)\n",
    "\n",
    "                affine_disp = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)),\n",
    "                                        align_corners=False)\n",
    "\n",
    "                grid += (affine_disp-id_grid)\n",
    "    else:\n",
    "        # Override grid with external value\n",
    "        grid = b_grid_override\n",
    "\n",
    "    if b_image is not None:\n",
    "        b_image_out = F.grid_sample(\n",
    "            b_image.unsqueeze(1).float(), grid,\n",
    "            padding_mode='border', align_corners=False)\n",
    "        b_image_out = b_image_out.squeeze(1)\n",
    "    else:\n",
    "        b_image_out = None\n",
    "\n",
    "    if b_label is not None:\n",
    "        b_label_out = F.grid_sample(\n",
    "            b_label.unsqueeze(1).float(), grid,\n",
    "            mode='nearest', align_corners=False)\n",
    "        b_label_out = b_label_out.squeeze(1).long()\n",
    "    else:\n",
    "        b_label_out = None\n",
    "\n",
    "    b_out_grid = grid\n",
    "\n",
    "    if pre_interpolation_factor:\n",
    "        b_image_out, b_label_out = interpolate_sample(\n",
    "            b_image_out, b_label_out,\n",
    "            1/pre_interpolation_factor, yield_2d\n",
    "        )\n",
    "\n",
    "        b_out_grid = F.interpolate(b_out_grid.permute(0,3,1,2),\n",
    "                      scale_factor=1/pre_interpolation_factor, mode='bilinear',\n",
    "                      align_corners=True, recompute_scale_factor=False).permute(0,2,3,1)\n",
    "\n",
    "    return b_image_out, b_label_out, b_out_grid\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(b_image, strength=0.05):\n",
    "    return b_image + strength*torch.randn_like(b_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossMoDaHealed_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        ensure_labeled_pairs=True, use_additional_data=False, resample=True,\n",
    "        size:tuple=(96,96,60), normalize:bool=True,\n",
    "        max_load_num=None, crop_3d_w_dim_range=None, crop_2d_slices_gt_num_threshold=None,\n",
    "        disturbed_idxs=None, disturbance_mode=None, disturbance_strength=1.0,\n",
    "        yield_2d_normal_to=None, flip_r_samples=True,\n",
    "        dilate_kernel_sz=1,\n",
    "        debug=False\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                ensure_labeled_pairs (bool): Only images with corresponding labels will be loaded (default: True)\n",
    "\n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "                max_load_num (int): maximum number of pairs to load (uses first max_load_num samples for either images and labels found)\n",
    "                crop_3d_w_dim_range (tuple): Tuple of ints defining the range to which dimension W of (D,H,W) is cropped\n",
    "                yield_2d_normal_to (bool):\n",
    "\n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.disturbed_idxs = disturbed_idxs\n",
    "        self.yield_2d_normal_to = yield_2d_normal_to\n",
    "        self.crop_2d_slices_gt_num_threshold = crop_2d_slices_gt_num_threshold\n",
    "        self.do_augment = False\n",
    "        self.do_disturb = False\n",
    "        self.augment_at_collate = False\n",
    "        self.dilate_kernel_sz = dilate_kernel_sz\n",
    "        self.disturbance_mode = disturbance_mode\n",
    "        self.disturbance_strength = disturbance_strength\n",
    "\n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "\n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "\n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "\n",
    "        path = base_dir + state_dir\n",
    "\n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "\n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "\n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "\n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "\n",
    "        if domain.lower() == \"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "\n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "        # First read filepaths\n",
    "        self.img_paths = {}\n",
    "        self.label_paths = {}\n",
    "\n",
    "        if debug:\n",
    "            files = files[:4]\n",
    "\n",
    "        for _path in files:\n",
    "\n",
    "            numeric_id = int(re.findall(r'\\d+', os.path.basename(_path))[0])\n",
    "            if \"_l.nii.gz\" in _path or \"_l_Label.nii.gz\" in _path:\n",
    "                lr_id = 'l'\n",
    "            elif \"_r.nii.gz\" in _path or \"_r_Label.nii.gz\" in _path:\n",
    "                lr_id = 'r'\n",
    "            else:\n",
    "                lr_id = \"\"\n",
    "\n",
    "            # Generate crossmoda id like 004r\n",
    "            crossmoda_id = f\"{numeric_id:03d}{lr_id}\"\n",
    "\n",
    "            if \"Label\" in _path:\n",
    "                self.label_paths[crossmoda_id] = _path\n",
    "\n",
    "            elif domain in _path:\n",
    "                self.img_paths[crossmoda_id] = _path\n",
    "\n",
    "        if ensure_labeled_pairs:\n",
    "            pair_idxs = set(self.img_paths).intersection(set(self.label_paths))\n",
    "            self.label_paths = {_id: _path for _id, _path in self.label_paths.items() if _id in pair_idxs}\n",
    "            self.img_paths = {_id: _path for _id, _path in self.img_paths.items() if _id in pair_idxs}\n",
    "\n",
    "\n",
    "        # Populate data\n",
    "        self.img_data_3d = {}\n",
    "        self.label_data_3d = {}\n",
    "\n",
    "        self.img_data_2d = {}\n",
    "        self.label_data_2d = {}\n",
    "        self.modified_label_data_2d = {}\n",
    "\n",
    "        #load data\n",
    "\n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "        id_paths_to_load = list(self.label_paths.items()) + list(self.img_paths.items())\n",
    "\n",
    "        description = f\"{len(self.img_paths)} images, {len(self.label_paths)} labels\"\n",
    "\n",
    "        for _3d_id, _file in tqdm(id_paths_to_load, desc=description):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in _file:\n",
    "                tmp = torch.from_numpy(nib.load(_file).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "\n",
    "                if crop_3d_w_dim_range:\n",
    "                    tmp = tmp[..., crop_3d_w_dim_range[0]:crop_3d_w_dim_range[1]]\n",
    "\n",
    "                # Only use tumour class, remove TODO\n",
    "                tmp[tmp==2] = 0\n",
    "                self.label_data_3d[_3d_id] = tmp.long()\n",
    "\n",
    "            elif domain in _file:\n",
    "                tmp = torch.from_numpy(nib.load(_file).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "\n",
    "                if crop_3d_w_dim_range:\n",
    "                    tmp = tmp[..., crop_3d_w_dim_range[0]:crop_3d_w_dim_range[1]]\n",
    "\n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "\n",
    "                self.img_data_3d[_3d_id] = tmp\n",
    "\n",
    "        # Postprocessing of 3d volumes\n",
    "        for _3d_id in list(self.label_data_3d.keys()):\n",
    "            if self.label_data_3d[_3d_id].unique().numel() != 2: #TODO use 3 classes again\n",
    "                del self.img_data_3d[_3d_id]\n",
    "                del self.label_data_3d[_3d_id]\n",
    "            elif \"r\" in _3d_id:\n",
    "                self.img_data_3d[_3d_id] = self.img_data_3d[_3d_id].flip(dims=(1,))\n",
    "                self.label_data_3d[_3d_id] = self.label_data_3d[_3d_id].flip(dims=(1,))\n",
    "\n",
    "        if max_load_num and ensure_labeled_pairs:\n",
    "            for _3d_id in list(self.label_data_3d.keys())[max_load_num:]:\n",
    "                del self.img_data_3d[_3d_id]\n",
    "                del self.label_data_3d[_3d_id]\n",
    "\n",
    "        elif max_load_num:\n",
    "            for del_key in list(self.img_data_3d.keys())[max_load_num:]:\n",
    "                del self.img_data_3d[del_key]\n",
    "            for del_key in list(self.label_data_3d.keys())[max_load_num:]:\n",
    "                del self.label_data_3d[del_key]\n",
    "\n",
    "        #check for consistency\n",
    "        print(\"Equal image and label numbers: {}\".format(set(self.img_data_3d)==set(self.label_data_3d)))\n",
    "\n",
    "        img_stack = torch.stack(list(self.img_data_3d.values()), dim=0)\n",
    "        img_mean, img_std = img_stack.mean(), img_stack.std()\n",
    "\n",
    "        label_stack = torch.stack(list(self.label_data_3d.values()), dim=0)\n",
    "\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(img_stack.shape, img_mean, img_std))\n",
    "        print(\"Label shape: {}, max.: {}\".format(label_stack.shape,torch.max(label_stack)))\n",
    "\n",
    "        if yield_2d_normal_to:\n",
    "            if yield_2d_normal_to == \"D\":\n",
    "                slice_dim = -3\n",
    "            if yield_2d_normal_to == \"H\":\n",
    "                slice_dim = -2\n",
    "            if yield_2d_normal_to == \"W\":\n",
    "                slice_dim = -1\n",
    "\n",
    "            for _3d_id, image in self.img_data_3d.items():\n",
    "                for idx, img_slc in [(slice_idx, image.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(image.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.img_data_2d[f\"{_3d_id}{yield_2d_normal_to}{idx:03d}\"] = img_slc\n",
    "\n",
    "            for _3d_id, label in self.label_data_3d.items():\n",
    "                for idx, lbl_slc in [(slice_idx, label.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(label.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.label_data_2d[f\"{_3d_id}{yield_2d_normal_to}{idx:03d}\"] = lbl_slc\n",
    "\n",
    "        # Postprocessing of 2d slices\n",
    "\n",
    "        rem_sum = 0\n",
    "        for key, label in list(self.label_data_2d.items()):\n",
    "            uniq_vals = label.unique()\n",
    "\n",
    "            # if uniq_vals.max() == 0:\n",
    "            #     # Delete empty 2D slices (but keep 3d data)\n",
    "            #     del self.label_data_2d[key]\n",
    "            #     del self.img_data_2d[key]\n",
    "            if sum(label[label > 0]) < self.crop_2d_slices_gt_num_threshold:\n",
    "                # Delete 2D slices with less than n gt-pixels (but keep 3d data)\n",
    "                del self.label_data_2d[key]\n",
    "                del self.img_data_2d[key]\n",
    "\n",
    "        print(\"Data import finished.\")\n",
    "        print(f\"CrossMoDa loader will yield {'2D' if self.yield_2d_normal_to else '3D'} samples\")\n",
    "\n",
    "    def get_3d_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data_3d.keys())\n",
    "            .union(set(self.label_data_3d.keys()))\n",
    "        ))\n",
    "\n",
    "    def get_2d_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data_2d.keys())\n",
    "            .union(set(self.label_data_2d.keys()))\n",
    "        ))\n",
    "\n",
    "    def get_id_dicts(self):\n",
    "\n",
    "        all_3d_ids = self.get_3d_ids()\n",
    "        id_dicts = []\n",
    "\n",
    "        for _2d_dataset_idx, _2d_id in enumerate(self.get_2d_ids()):\n",
    "            _3d_id = _2d_id[:-4]\n",
    "            id_dicts.append(\n",
    "                {\n",
    "                    '2d_id': _2d_id,\n",
    "                    '2d_dataset_idx': _2d_dataset_idx,\n",
    "                    '3d_id': _3d_id,\n",
    "                    '3d_dataset_idx': all_3d_ids.index(_3d_id),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return id_dicts\n",
    "\n",
    "    def __len__(self, yield_2d_override=None):\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data length\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "\n",
    "        if yield_2d:\n",
    "            return len(self.img_data_2d)\n",
    "\n",
    "        return len(self.img_data_3d)\n",
    "\n",
    "    def __getitem__(self, dataset_idx, yield_2d_override=None):\n",
    "\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data can be overridden\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "\n",
    "        if yield_2d:\n",
    "            all_ids = self.get_2d_ids()\n",
    "            _id = all_ids[dataset_idx]\n",
    "            image = self.img_data_2d.get(_id, torch.tensor([]))\n",
    "            label = self.label_data_2d.get(_id, torch.tensor([]))\n",
    "\n",
    "            # For 2D crossmoda id cut last 4 \"003rW100\"\n",
    "            image_path = self.img_paths[_id[:-4]]\n",
    "            label_path = self.label_paths[_id[:-4]]\n",
    "\n",
    "        else:\n",
    "            all_ids = self.get_3d_ids()\n",
    "            _id = all_ids[dataset_idx]\n",
    "            image = self.img_data_3d.get(_id, torch.tensor([]))\n",
    "            label = self.label_data_3d.get(_id, torch.tensor([]))\n",
    "\n",
    "            image_path = self.img_paths[_id]\n",
    "            label_path = self.label_paths[_id]\n",
    "\n",
    "        modified_label = self.modified_label_data_2d.get(_id, label.detach().clone())\n",
    "        spat_augment_grid = []\n",
    "\n",
    "\n",
    "        if self.do_augment and not self.augment_at_collate:\n",
    "            b_image = image.unsqueeze(0).cuda()\n",
    "            b_label = label.unsqueeze(0).cuda()\n",
    "            b_modified_label = modified_label.unsqueeze(0).cuda()\n",
    "\n",
    "            b_image, b_label, b_spat_augment_grid = self.augment(\n",
    "                b_image, b_label, yield_2d\n",
    "            )\n",
    "            _, b_modified_label, _ = spatial_augment(\n",
    "                b_label=b_modified_label, yield_2d=yield_2d, b_grid_override=b_spat_augment_grid\n",
    "            )\n",
    "\n",
    "            image = b_image.squeeze(0).cpu()\n",
    "            label = b_label.squeeze(0).cpu()\n",
    "            modified_label = b_modified_label.squeeze(0).cpu()\n",
    "            spat_augment_grid = b_spat_augment_grid.squeeze(0).detach().cpu().clone()\n",
    "\n",
    "        if yield_2d:\n",
    "            assert image.dim() == label.dim() == 2\n",
    "        else:\n",
    "            assert image.dim() == label.dim() == 3\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'modified_label': modified_label,\n",
    "            # if disturbance is off, modified label is equals label\n",
    "            'dataset_idx': dataset_idx,\n",
    "            'id': _id,\n",
    "            'image_path': image_path,\n",
    "            'label_path': label_path,\n",
    "            'spat_augment_grid': spat_augment_grid\n",
    "        }\n",
    "\n",
    "    def get_3d_item(self, _3d_dataset_idx):\n",
    "        return self.__getitem__(_3d_dataset_idx, yield_2d_override=False)\n",
    "\n",
    "    def get_data(self, yield_2d_override=None):\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data can be overridden\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "\n",
    "        if yield_2d:\n",
    "            img_stack = torch.stack(list(self.img_data_2d.values()), dim=0)\n",
    "            label_stack = torch.stack(list(self.label_data_2d.values()), dim=0)\n",
    "        else:\n",
    "            img_stack = torch.stack(list(self.img_data_3d.values()), dim=0)\n",
    "            label_stack = torch.stack(list(self.label_data_3d.values()), dim=0)\n",
    "\n",
    "        return img_stack, label_stack\n",
    "\n",
    "    def set_disturbed_idxs(self, idxs):\n",
    "        if idxs is not None:\n",
    "            if isinstance(idxs, (np.ndarray, torch.Tensor)):\n",
    "                idxs = idxs.tolist()\n",
    "\n",
    "            self.disturbed_idxs = idxs\n",
    "        else:\n",
    "            self.disturbed_idxs = []\n",
    "\n",
    "\n",
    "    def train(self, augment=True, disturb=True):\n",
    "        self.do_augment = augment\n",
    "        self.do_disturb = disturb\n",
    "\n",
    "    def eval(self, augment=False, disturb=False):\n",
    "        self.train(augment, disturb)\n",
    "\n",
    "    def set_augment_at_collate(self):\n",
    "        self.augment_at_collate = True\n",
    "\n",
    "    def unset_augment_at_collate(self):\n",
    "        self.augment_at_collate = False\n",
    "\n",
    "    def set_dilate_kernel_size(self, sz):\n",
    "        self.dilate_kernel_sz = max(1,sz)\n",
    "\n",
    "    def set_disturbance_strength(self, strength):\n",
    "        self.disturbance_strength = strength\n",
    "\n",
    "    def get_dilate_kernel_size(self):\n",
    "        return self.dilate_kernel_sz\n",
    "\n",
    "    def get_efficient_augmentation_collate_fn(self):\n",
    "        yield_2d = True if self.yield_2d_normal_to else False\n",
    "\n",
    "        def collate_closure(batch):\n",
    "            batch = torch.utils.data._utils.collate.default_collate(batch)\n",
    "            if self.augment_at_collate:\n",
    "                # Augment the whole batch not just one sample\n",
    "                b_image = batch['image'].cuda()\n",
    "                b_label = batch['label'].cuda()\n",
    "                b_image, b_label = self.augment(b_image, b_label, yield_2d)\n",
    "                batch['image'], batch['label'] = b_image.cpu(), b_label.cpu()\n",
    "\n",
    "            return batch\n",
    "\n",
    "        return collate_closure\n",
    "\n",
    "    def augment(self, b_image, b_label, yield_2d,\n",
    "        noise_strength=0.05,\n",
    "        bspline_num_ctl_points=6, bspline_strength=0.002, bspline_probability=.95,\n",
    "        affine_strength=0.03, affine_probability=.45,\n",
    "        pre_interpolation_factor=2.):\n",
    "\n",
    "        if yield_2d:\n",
    "            assert b_image.dim() == b_label.dim() == 3, \\\n",
    "                f\"Augmenting 2D. Input batch of image and \" \\\n",
    "                f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        else:\n",
    "            assert b_image.dim() == b_label.dim() == 4, \\\n",
    "                f\"Augmenting 3D. Input batch of image and \" \\\n",
    "                f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "\n",
    "        b_image = augmentNoise(b_image, strength=noise_strength)\n",
    "        b_image, b_label, b_spat_augment_grid = spatial_augment(\n",
    "            b_image, b_label,\n",
    "            bspline_num_ctl_points=bspline_num_ctl_points, bspline_strength=bspline_strength, bspline_probability=bspline_probability,\n",
    "            affine_strength=affine_strength, affine_probability=affine_probability,\n",
    "            pre_interpolation_factor=2., yield_2d=yield_2d)\n",
    "\n",
    "        b_label = b_label.long()\n",
    "\n",
    "        return b_image, b_label, b_spat_augment_grid\n",
    "\n",
    "\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def torch_manual_seeded(seed):\n",
    "    saved_state = torch.get_rng_state()\n",
    "    torch.manual_seed(seed)\n",
    "    yield\n",
    "    torch.set_rng_state(saved_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "class LabelDisturbanceMode(Enum):\n",
    "    FLIP_ROLL = auto()\n",
    "    AFFINE = auto()\n",
    "    \n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "        See https://stackoverflow.com/questions/49901590/python-using-copy-deepcopy-on-dotdict\n",
    "    \"\"\"\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        try:\n",
    "            return self[item]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError from e\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    # 'fold_override': 0,\n",
    "    # 'epx_override': 0,\n",
    "\n",
    "    'use_mind': True,\n",
    "    'epochs': 40,\n",
    "\n",
    "    'batch_size': 64,\n",
    "    'val_batch_size': 1,\n",
    "\n",
    "    'dataset': 'crossmoda_healed',\n",
    "    'train_set_max_len': 100,\n",
    "    'crop_3d_w_dim_range': (24, 110),\n",
    "    'crop_2d_slices_gt_num_threshold': 25,\n",
    "    'yield_2d_normal_to': \"W\",\n",
    "\n",
    "    'lr': 0.0005,\n",
    "    'use_cosine_annealing': True,\n",
    "\n",
    "    # Data parameter config\n",
    "    'data_param_mode': DataParamMode.DISABLED,\n",
    "        # init_class_param=0.01,\n",
    "        # lr_class_param=0.1,\n",
    "    # 'init_inst_param': 1.0,\n",
    "    # 'lr_inst_param': 0.1,\n",
    "    # 'wd_inst_param': 0.01,\n",
    "        # wd_class_param=0.0,\n",
    "        # skip_clamp_data_param=False,\n",
    "    # 'clamp_sigma_min': 0.,\n",
    "    # 'clamp_sigma_max': 1.,\n",
    "        # optim_algorithm=DataParamOptim.ADAM,\n",
    "        # optim_options=dict(\n",
    "        #     betas=(0.9, 0.999)\n",
    "        # )\n",
    "    # 'grid_size_y': 32,\n",
    "    # 'grid_size_x': 32,\n",
    "    # ),\n",
    "\n",
    "    'save_every': 200,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'do_plot': False,\n",
    "    'debug': False,\n",
    "    'wandb_mode': \"online\",\n",
    "    'wandb_name_override': None,\n",
    "    'do_sweep': False,\n",
    "\n",
    "    'disturbance_mode': None,\n",
    "    'disturbance_strength': .0,\n",
    "    'disturbed_percentage': .0,\n",
    "    'start_disturbing_after_ep': 100000,\n",
    "\n",
    "    'start_dilate_kernel_sz': 1\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict.label_tags = ['background', 'tumour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CrossMoDaHealed_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "    domain=\"source\", state=\"l4\", size=(128, 128, 128),\n",
    "    ensure_labeled_pairs=True,\n",
    "    max_load_num=config_dict['train_set_max_len'],\n",
    "    crop_3d_w_dim_range=config_dict['crop_3d_w_dim_range'], crop_2d_slices_gt_num_threshold=config_dict['crop_2d_slices_gt_num_threshold'],\n",
    "    yield_2d_normal_to=config_dict['yield_2d_normal_to'],\n",
    "    disturbed_idxs=None, disturbance_mode=config_dict['disturbance_mode'], disturbance_strength=config_dict['disturbance_strength'],\n",
    "    dilate_kernel_sz=config_dict['start_dilate_kernel_sz'],\n",
    "    debug=config_dict['debug']\n",
    ")\n",
    "config_dict.label_tags = ['background', 'tumour']\n",
    "training_dataset.eval()\n",
    "print(\"Nonzero slice ratio: \",\n",
    "    sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.eval()\n",
    "_2d_imgs = [training_dataset[idx]['image'] for idx in dataset_idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, bins = np.histogram(dp_weightmap.view(-1).cpu().detach().numpy(), bins=90);\n",
    "plt.bar(bins[:-1],h,width=1)\n",
    "perc=3\n",
    "margin = np.percentile(dp_weightmap.view(-1).cpu().detach().numpy(), perc)\n",
    "margin\n",
    "margin = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizied_weightmap = torch.zeros_like(dp_weightmap).to(dp_weightmap)\n",
    "binarizied_weightmap[dp_weightmap > margin] = 1\n",
    "\n",
    "# binarizied_weightmap_o = dilate_label_class(binarizied_weightmap_o, 1, 1, yield_2d=True, kernel_sz=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std_is_small = []\n",
    "# STD_MARGIN = .2\n",
    "# masked_binarized = binarizied_weightmap.cuda() * _2d_modified_labels.cuda().long()\n",
    "# for candidate, orig_label in zip(masked_binarized, _2d_modified_labels):\n",
    "#     std_is_small.append(candidate[orig_label != 0].std() < STD_MARGIN)\n",
    "# std_is_small = torch.stack(std_is_small).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate_label_class(b_label, class_max_idx, class_dilate_idx,\n",
    "                       yield_2d, kernel_sz=3, erode=False):\n",
    "\n",
    "    if kernel_sz < 2:\n",
    "        return b_label\n",
    "\n",
    "    b_dilated_label = torch.zeros_like(b_label)\n",
    "\n",
    "    b_onehot = torch.nn.functional.one_hot(b_label.long(), class_max_idx+1)\n",
    "    class_slice = b_onehot[...,class_dilate_idx]\n",
    "\n",
    "    if yield_2d:\n",
    "        B, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz]).to(b_label.device).float()\n",
    "        kernel = kernel.view(1,1,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv2d(\n",
    "            class_slice.view(B,1,H,W).float(), kernel, padding='same')\n",
    "\n",
    "    else:\n",
    "        B, D, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz,kernel_sz]).to(b_label.device).float()\n",
    "        kernel = kernel.long().view(1,1,kernel_sz,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv3d(\n",
    "            class_slice.view(B,1,D,H,W).float(), kernel, padding='same')\n",
    "    if erode:\n",
    "        class_slice[class_slice<kernel.numel()] = 0\n",
    "        \n",
    "    dilated_class_slice = torch.clamp(class_slice.squeeze(0), 0, 1)\n",
    "    b_dilated_label[dilated_class_slice.squeeze(1).bool()] = class_dilate_idx\n",
    "\n",
    "    return b_dilated_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_SAMPLE = 50\n",
    "\n",
    "healed_labels = \\\n",
    "    (torch.logical_and(_2d_modified_labels.cuda(), binarizied_weightmap.cuda()) \\\n",
    "    + torch.logical_and(torch.logical_not(_2d_modified_labels.cuda()), torch.logical_not(binarizied_weightmap.cuda()))).long()\n",
    "\n",
    "healed_labels = dilate_label_class(healed_labels, 1, 1, yield_2d=True, kernel_sz=3, erode=True)\n",
    "healed_labels = dilate_label_class(healed_labels, 1, 1, yield_2d=True, kernel_sz=3)\n",
    "\n",
    "keep_orig_label_idxs = (torch.tensor(dp_mean_weight)<0.).bool()\n",
    "healed_labels[keep_orig_label_idxs] = _2d_modified_labels[keep_orig_label_idxs].cuda()\n",
    "\n",
    "plt.imshow(binarizied_weightmap[SHOW_SAMPLE].detach().cpu().numpy(), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(_2d_modified_labels[SHOW_SAMPLE].detach().cpu().numpy(), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(healed_labels[SHOW_SAMPLE].detach().cpu().numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_seg(in_type=\"batch_2D\",\n",
    "            img=binarizied_weightmap.unsqueeze(1),\n",
    "            seg=_2d_modified_labels,\n",
    "            alpha_seg = 0.,\n",
    "            n_per_row=70,\n",
    "            annotate_color=(0,255,255),\n",
    "            file_path=f'bin_weightmap{perc}percent.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_seg(in_type=\"batch_2D\",\n",
    "            img=torch.stack(_2d_imgs).unsqueeze(1),\n",
    "            seg=healed_labels.long(),\n",
    "            alpha_seg = .4,\n",
    "            n_per_row=70,\n",
    "            annotate_color=(0,255,255),\n",
    "            file_path=f'healed_labels{perc}percent.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_seg(in_type=\"batch_2D\",\n",
    "            img=torch.stack(_2d_imgs).unsqueeze(1),\n",
    "            seg=(4*_2d_modified_labels-_2d_labels).abs(),\n",
    "            crop_to_non_zero_seg=False,\n",
    "            alpha_seg = .2,\n",
    "            n_per_row=70,\n",
    "            annotate_color=(0,255,255),\n",
    "            file_path='viz_seg.png',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_seg(in_type=\"batch_2D\",\n",
    "            img=dp_weightmap.unsqueeze(1),\n",
    "            seg=_2d_modified_labels,\n",
    "            alpha_seg = 0.,\n",
    "            n_per_row=70,\n",
    "            annotate_color=(0,255,255),\n",
    "            file_path='viz_weightmap.png',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((d_ids, healed_labels), 'healing_polished-river.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _id, healed in zip(d_ids, healed_labels):\n",
    "    training_dataset.modified_label_data_2d[_id] = healed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_stack = []\n",
    "label_stack = []\n",
    "mod_label_stack = []\n",
    "training_dataset.eval()\n",
    "for sample in (training_dataset[idx] for idx in dataset_idxs):\n",
    "    img_stack.append(sample['image'])\n",
    "    label_stack.append(sample['label'])\n",
    "    mod_label_stack.append(sample['modified_label'])\n",
    "\n",
    "# Change label num == hue shift for display\n",
    "img_stack = torch.stack(img_stack).unsqueeze(1).cpu()\n",
    "label_stack = torch.stack(label_stack).cpu()\n",
    "mod_label_stack = torch.stack(mod_label_stack).cpu()\n",
    "\n",
    "mod_label_stack*=4\n",
    "\n",
    "visualize_seg(in_type=\"batch_2D\",\n",
    "    img=img_stack,\n",
    "    # ground_truth=label_stack,\n",
    "    seg=(mod_label_stack-label_stack).abs(),\n",
    "    # crop_to_non_zero_gt=True,\n",
    "    crop_to_non_zero_seg=True,\n",
    "    alpha_seg = .6,\n",
    "    # alpha_gt = .6,\n",
    "    n_per_row=70,\n",
    "    file_path='healed_set.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sample(b_image=None, b_label=None, scale_factor=1.,\n",
    "                       yield_2d=False):\n",
    "    if yield_2d:\n",
    "        scale = [scale_factor]*2\n",
    "        im_mode = 'bilinear'\n",
    "    else:\n",
    "        scale = [scale_factor]*3\n",
    "        im_mode = 'trilinear'\n",
    "\n",
    "    if b_image is not None:\n",
    "        b_image = F.interpolate(\n",
    "            b_image.unsqueeze(1), scale_factor=scale, mode=im_mode, align_corners=True,\n",
    "            recompute_scale_factor=False\n",
    "        )\n",
    "        b_image = b_image.squeeze(1)\n",
    "\n",
    "    if b_label is not None:\n",
    "        b_label = F.interpolate(\n",
    "            b_label.unsqueeze(1).float(), scale_factor=scale, mode='nearest',\n",
    "            recompute_scale_factor=False\n",
    "        ).long()\n",
    "        b_label = b_label.squeeze(1)\n",
    "\n",
    "    return b_image, b_label\n",
    "\n",
    "\n",
    "\n",
    "def dilate_label_class(b_label, class_max_idx, class_dilate_idx,\n",
    "                       yield_2d, kernel_sz=3):\n",
    "\n",
    "    if kernel_sz < 2:\n",
    "        return b_label\n",
    "\n",
    "    b_dilated_label = b_label\n",
    "\n",
    "    b_onehot = torch.nn.functional.one_hot(b_label.long(), class_max_idx+1)\n",
    "    class_slice = b_onehot[...,class_dilate_idx]\n",
    "\n",
    "    if yield_2d:\n",
    "        B, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz]).long()\n",
    "        kernel = kernel.view(1,1,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv2d(\n",
    "            class_slice.view(B,1,H,W), kernel, padding='same')\n",
    "\n",
    "    else:\n",
    "        B, D, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz,kernel_sz])\n",
    "        kernel = kernel.long().view(1,1,kernel_sz,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv3d(\n",
    "            class_slice.view(B,1,D,H,W), kernel, padding='same')\n",
    "\n",
    "    dilated_class_slice = torch.clamp(class_slice.squeeze(0), 0, 1)\n",
    "    b_dilated_label[dilated_class_slice.bool()] = class_dilate_idx\n",
    "\n",
    "    return b_dilated_label\n",
    "\n",
    "\n",
    "def get_batch_dice_per_class(b_dice, class_tags, exclude_bg=True) -> dict:\n",
    "    score_dict = {}\n",
    "    for cls_idx, cls_tag in enumerate(class_tags):\n",
    "        if exclude_bg and cls_idx == 0:\n",
    "            continue\n",
    "\n",
    "        if torch.all(torch.isnan(b_dice[:,cls_idx])):\n",
    "            score = float('nan')\n",
    "        else:\n",
    "            score = np.nanmean(b_dice[:,cls_idx]).item()\n",
    "\n",
    "        score_dict[cls_tag] = score\n",
    "\n",
    "    return score_dict\n",
    "\n",
    "def get_batch_dice_over_all(b_dice, exclude_bg=True) -> float:\n",
    "\n",
    "    start_idx = 1 if exclude_bg else 0\n",
    "    if torch.all(torch.isnan(b_dice[:,start_idx:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,start_idx:]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "\n",
    "import functools\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use get_named_layers_leaves(module) to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(lraspp, optimizer, scaler, _path):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "    _path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    torch.save(lraspp.state_dict(), _path.joinpath('lraspp.pth'))\n",
    "    torch.save(optimizer.state_dict(), _path.joinpath('optimizer.pth'))\n",
    "    torch.save(scaler.state_dict(), _path.joinpath('grad_scaler.pth'))\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config, dataset_len, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(\n",
    "        pretrained=False, progress=True, num_classes=len(config.label_tags)\n",
    "    )\n",
    "    set_module(lraspp, 'backbone.0.0',\n",
    "        torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2),\n",
    "                        padding=(1, 1), bias=False)\n",
    "    )\n",
    "    # set_module(lraspp, 'classifier.scale.2',\n",
    "    #     torch.nn.Identity()\n",
    "    # )\n",
    "\n",
    "    lraspp.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(lraspp.parameters(), lr=config.lr)\n",
    "\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading lr-aspp model, optimizer and grad scalers from {_path}\")\n",
    "        lraspp.load_state_dict(torch.load(_path.joinpath('lraspp.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('grad_scaler.pth'), map_location=device))\n",
    "\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "    print(f\"Param count lraspp: {sum(p.numel() for p in lraspp.parameters())}\")\n",
    "\n",
    "    return (lraspp, optimizer, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "\n",
    "\n",
    "\n",
    "def log_data_parameters(log_path, parameter_idxs, parameters):\n",
    "    data = [[idx, param] for (idx, param) in \\\n",
    "        zip(parameter_idxs, torch.exp(parameters).tolist())]\n",
    "\n",
    "    table = wandb.Table(data=data, columns = [\"parameter_idx\", \"value\"])\n",
    "    wandb.log({log_path:wandb.plot.bar(table, \"parameter_idx\", \"value\", title=log_path)})\n",
    "\n",
    "\n",
    "\n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "\n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "\n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "\n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "\n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio\n",
    "\n",
    "def log_data_parameter_stats(log_path, epx, data_parameters):\n",
    "    \"\"\"Log stats for data parameters on wandb.\"\"\"\n",
    "    data_parameters = data_parameters.exp()\n",
    "    wandb.log({f'{log_path}/highest': torch.max(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/lowest': torch.min(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/mean': torch.mean(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/std': torch.std(data_parameters).item()}, step=epx)\n",
    "\n",
    "\n",
    "\n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_class_dices(log_prefix, log_postfix, class_dices, log_idx):\n",
    "    if not class_dices:\n",
    "        return\n",
    "\n",
    "    for cls_name in class_dices[0].keys():\n",
    "        log_path = f\"{log_prefix}{cls_name}{log_postfix}\"\n",
    "\n",
    "        cls_dices = list(map(lambda dct: dct[cls_name], class_dices))\n",
    "        mean_per_class =np.nanmean(cls_dices)\n",
    "        print(log_path, f\"{mean_per_class*100:.2f}%\")\n",
    "        wandb.log({log_path: mean_per_class}, step=log_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_healedDL(run_name, config, training_dataset):\n",
    "    fold_idx = 0\n",
    "    reset_determinism()\n",
    "    train_idxs = dataset_idxs\n",
    "    # train_idxs = torch.tensor(train_idxs)\n",
    "\n",
    "    # Training happens in 2D, validation happens in 3D:\n",
    "    # Read 2D dataset idxs which are used for training,\n",
    "    # get their 3D super-ids by 3d dataset length\n",
    "    # and substract these from all 3D ids to get val_3d_idxs\n",
    "    trained_3d_dataset_idxs = {dct['3d_dataset_idx'] \\\n",
    "            for dct in training_dataset.get_id_dicts() if dct['2d_dataset_idx'] in train_idxs.tolist()}\n",
    "    val_3d_idxs = set(range(training_dataset.__len__(yield_2d_override=False))) - trained_3d_dataset_idxs\n",
    "    val_3d_idxs = val_3d_idxs[:15]\n",
    "    print(\"Will run validation with these 3D samples:\", val_3d_idxs)\n",
    "\n",
    "    ### Configure MIND ###\n",
    "    if config.use_mind:\n",
    "        in_channels =12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    _, all_segs = training_dataset.get_data()\n",
    "\n",
    "    ### Add train sampler and dataloaders ##\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "    # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "    train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "        sampler=train_subsampler, pin_memory=True, drop_last=False,\n",
    "        # collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "    )\n",
    "    corrs[pth.split('/')[-2]] = all_elems\n",
    "    # break\n",
    "df = pd.DataFrame(corrs)\n",
    "df.to_csv('pd.csv')\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
