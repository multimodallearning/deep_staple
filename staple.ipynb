{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name                     Util    Mem free  Cuda             User(s)\n",
      "----  --------------------------  ------  ----------  ---------------  ---------\n",
      "   1  NVIDIA GeForce RTX 2080 Ti     0 %   11018 MiB  11.5(495.29.05)\n",
      "   2  NVIDIA GeForce RTX 2080 Ti     0 %   11018 MiB  11.5(495.29.05)\n",
      "   3  NVIDIA GeForce RTX 2080 Ti     0 %   11018 MiB  11.5(495.29.05)\n",
      "   0  NVIDIA GeForce RTX 2080 Ti     0 %    2575 MiB  11.5(495.29.05)  schneider\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   1  NVIDIA GeForce RTX 2080 Ti  ->  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/how-to-use-the-staple-algorithm-to-combine-multiple-image-segmentations-ce91ebeb451e\n",
    "# packages\n",
    "import nibabel as nib # https://nipy.org/nibabel/\n",
    "import SimpleITK as sitk # https://simpleitk.org/\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"*\"))\n",
    "from collections import OrderedDict\n",
    "from mdl_seg_class.metrics import dice3d\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220114_crossmoda_multiple_registrations/crossmoda_deeds_registered.pth\"\n",
    "bare_data = torch.load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc STAPLE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_state = \"acummulate_deeds_FT2_MT1\"\n",
    "\n",
    "staple_filter = sitk.STAPLEImageFilter()\n",
    "# sitk.ProcessObject.SetGlobalDefaultDebugOff()\n",
    "FOREGROUND = 1.0\n",
    "weight_data = {}\n",
    "EVERY = 1\n",
    "staple_filter.SetForegroundValue(FOREGROUND)\n",
    "DEBUG = False\n",
    "if reg_state == \"acummulate_deeds_FT2_MT1\":\n",
    "    for fixed_id, moving_dict in bare_data.items():\n",
    "        # if fixed_id != '111l': continue\n",
    "        print(fixed_id)\n",
    "        # print(moving_dict)\n",
    "        sorted_moving_dict = OrderedDict(moving_dict)\n",
    "        moving_data = []\n",
    "        selected_moving_ids = []\n",
    "        \n",
    "        for idx_mov, (moving_id, moving_sample) in enumerate(sorted_moving_dict.items()):\n",
    "            # Only use every third warped sample\n",
    "            print(idx_mov)\n",
    "            # idx_mov = 29\n",
    "            if idx_mov % EVERY == 0:\n",
    "                moving_data.append(moving_sample['warped_label'][slice_idx].cpu())\n",
    "                moving_slice_id = f\"{fixed_id}:m{moving_id}\"\n",
    "                selected_moving_ids.append(moving_slice_id)\n",
    "\n",
    "        sitk_moving_data = [sitk.GetImageFromArray(reg_seg.to_dense().numpy().astype(np.int16)) for reg_seg in moving_data]\n",
    "        _ = staple_filter.Execute(sitk_moving_data)\n",
    "        # del staple_filter\n",
    "        # del sitk_moving_data\n",
    "        # staple_out = sitk.STAPLE(sitk_moving_data, FOREGROUND)\n",
    "        # consensus = sitk.GetArrayFromImage(staple_out)\n",
    "        \n",
    "        # specitivity = staple_filter.GetSpecificity()\n",
    "        # sensitivity = staple_filter.GetSensitivity()\n",
    "        # specitivity = sensitivity = [0]*60\n",
    "        # f_weight_dict = weight_data.get(fixed_id, {})\n",
    "        # staple_consensus = sitk.GetArrayFromImage(staple_out)\n",
    "        # for moving_id, sens, spec in zip(selected_moving_ids, sensitivity, specitivity):\n",
    "        #     weight_data[moving_id] = dict(sensitivity=sens, specitivity=spec)\n",
    "        # weight_data[fixed_id] = f_weight_dict\n",
    "    \n",
    "    if DEBUG: break\n",
    "else:\n",
    "    raise ValueError()\n",
    "\n",
    "torch.save(weight_data, f\"./data/staple_calc/{reg_state}_every_{EVERY}_3d_volumes.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps = []\n",
    "for key in weight_data.keys():\n",
    "    if key == 'data_path': continue\n",
    "    if np.isnan(weight_data[key]['sensitivity']):\n",
    "        pass\n",
    "    else:\n",
    "        dps.append(weight_data[key]['sensitivity'])\n",
    "plt.hist(dps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store STAPLE single rater scores (data from mattias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "\n",
    "staple_weights = None\n",
    "staple_description_files = glob(\"/share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_*_L_staple*.txt\")\n",
    "\n",
    "target_t2_keys_w_tumour = ['108r', '111l', '112r', '115l', '118r', '120r', '117l', '123r', '127r', '125l', '134r', '135r', '126l', '142r', '144r', '133l', '148r', '154r', '136l', '160r', '165r', '140l', '166r', '167r', '141l', '168r', '171r', '143l', '173r', '174r', '145l', '179r', '146l', '180r', '181r', '185r', '147l', '195r', '149l', '198r', '204r', '152l', '205r', '209r', '158l', '210r', '162l', '164l', '175l', '177l', '178l', '183l', '187l', '188l', '189l', '190l', '192l', '199l', '200l', '202l']\n",
    "source_t1_keys_w_tumour = ['100r', '101r', '102l', '103l', '104l', '105l', '010l', '011r', '012r', '013l', '014l', '015l', '016r', '017r', '018l', '019r', '001r', '020l', '021l', '022l', '023r', '024r', '025r', '026l', '027r', '028l', '029r', '002l', '030r', '031l', '032r', '033r', '034l', '035r', '036r', '037r', '038l', '039r', '003r', '040r', '041l', '042l', '042r', '043l', '044r', '045r', '046r', '047r', '048r', '049l', '004r', '050r', '051l', '052r', '053r', '054r', '055l', '056l', '057r', '058l', '059r', '005r', '060r', '061l', '062l', '063l', '064l', '065r', '066l', '067l', '068r', '069l', '006r', '070r', '071l', '072r', '073r', '074r', '075l', '076r', '077r', '078r', '079l', '007r', '080r', '081l', '082r', '083r', '084r', '085r', '086r', '087l', '088l', '089l', '008l', '090l', '091r', '092r', '093l', '094r', '095r', '096r', '097r', '098r', '099l', '099r', '009r']\n",
    "\n",
    "def get_num_lr_id(num_str):\n",
    "    num_str = int(num_str)\n",
    "    if f\"{num_str:03d}r\" in source_t1_keys_w_tumour:\n",
    "        _id_str = f\"{num_str:03d}r\"\n",
    "    elif f\"{num_str:03d}l\" in source_t1_keys_w_tumour:\n",
    "        _id_str = f\"{num_str:03d}l\"\n",
    "    elif f\"{num_str:03d}r\" in target_t2_keys_w_tumour:\n",
    "        _id_str = f\"{num_str:03d}r\"\n",
    "    elif f\"{num_str:03d}l\" in target_t2_keys_w_tumour:\n",
    "        _id_str = f\"{num_str:03d}l\"\n",
    "    return _id_str\n",
    "\n",
    "def get_rater_dict(fixed_id, moving_id, rater_id):\n",
    "    fixed_id_dict = consensus_description_dict.get(fixed_id, {}) \n",
    "    moving_id_dict = fixed_id_dict.get(moving_id, {})\n",
    "    rater_dict = moving_id_dict.get(f\"rater_{int(rater_id):03d}\", {})\n",
    "    return rater_dict\n",
    "\n",
    "def set_rater_dict(consensus_description_dict, rater_dict, fixed_id, moving_id, rater_id):\n",
    "    fixed_id_dict = consensus_description_dict.get(fixed_id, {})\n",
    "    fixed_id_dict[moving_id] = rater_dict\n",
    "    consensus_description_dict[fixed_id] = fixed_id_dict\n",
    "\n",
    "consensus_description_dict = {}\n",
    "\n",
    "for _file in staple_description_files:\n",
    "    for staple_split in ['01','23','45','689']:\n",
    "        result = re.findall(rf'/share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_(\\w+)_L_staple{staple_split}\\.txt', _file)\n",
    "        if not result: \n",
    "            continue\n",
    "        else:\n",
    "            intra_file_dict = {}\n",
    "            with open(_file, 'r') as description:\n",
    "                for line in description:\n",
    "                    # print(line)\n",
    "                    seg_info = re.findall(rf'Reading #([0-9]+) from (Fcrossmoda_(\\w+)_L_M(\\w+)_deformed_seg\\.nii\\.gz)', line)\n",
    "                    if seg_info:\n",
    "                        rater_id, seg_path, fixed_id, moving_id = seg_info[0]\n",
    "                        fixed_id, moving_id = get_num_lr_id(fixed_id), get_num_lr_id(moving_id)\n",
    "                        # print(fixed_id, moving_id)\n",
    "                        rater_dict = get_rater_dict(fixed_id, moving_id, rater_id)\n",
    "                        rater_dict['file'] = seg_path\n",
    "                        rater_dict['fixed_id'] = fixed_id\n",
    "                        rater_dict['moving_id'] = moving_id\n",
    "                        intra_file_dict[int(rater_id)-1] = rater_dict\n",
    "\n",
    "                        set_rater_dict(consensus_description_dict, rater_dict, fixed_id, moving_id, rater_id)\n",
    "\n",
    "                    rater_info = re.findall(rf'Rater ([0-9]+): Sensitivity = ([\\.\\-\\w]+); Specificity = ([\\.\\w]+)', line)\n",
    "                    if rater_info:\n",
    "                        score_rater_id, sensitivity, specitivity = rater_info[0]\n",
    "\n",
    "                        score_rater_dict = intra_file_dict[int(score_rater_id)]\n",
    "                        score_fixed_id = score_rater_dict['fixed_id']\n",
    "                        score_moving_id = rater_dict['moving_id']\n",
    "                        score_rater_dict['sensitivity'] = float(sensitivity)\n",
    "                        score_rater_dict['specitivity'] = float(specitivity)\n",
    "                        set_rater_dict(consensus_description_dict, score_rater_dict, score_fixed_id, score_moving_id, score_rater_id)\n",
    "                        # print(rater_id, sensitivity, specitivity)\n",
    "print(consensus_description_dict.keys())\n",
    "\n",
    "torch.save(consensus_description_dict, \"staple_consensus_text_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store STAPLED samples (data from mattias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import nibabel as nib\n",
    "staple_descri = glob(\"/share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_*_L_staple*.nii.gz\")\n",
    "stapled_files = glob(\"/share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_*_L_staple*.nii.gz\")\n",
    "\n",
    "staple_dict = {}\n",
    "target_t2_keys_w_tumour = ['108r', '111l', '112r', '115l', '118r', '120r', '117l', '123r', '127r', '125l', '134r', '135r', '126l', '142r', '144r', '133l', '148r', '154r', '136l', '160r', '165r', '140l', '166r', '167r', '141l', '168r', '171r', '143l', '173r', '174r', '145l', '179r', '146l', '180r', '181r', '185r', '147l', '195r', '149l', '198r', '204r', '152l', '205r', '209r', '158l', '210r', '162l', '164l', '175l', '177l', '178l', '183l', '187l', '188l', '189l', '190l', '192l', '199l', '200l', '202l']\n",
    "\n",
    "for _file in stapled_files:\n",
    "    for staple_split in ['01','23','45','689']:\n",
    "    # result = re.findall(r'/share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_(\\w+)_L_staple([0-9]+)\\.nii\\.gz', _file)\n",
    "        result = re.findall(rf'/share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_(\\w+)_L_staple{staple_split}\\.nii\\.gz', _file)\n",
    "        if not result: \n",
    "            continue\n",
    "        else:\n",
    "            fixed_num_str = result[0]\n",
    "        \n",
    "        \"/share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_*_L_staple{01,23,45,689}.nii.gz\"\n",
    "        if fixed_num_str+'r' in target_t2_keys_w_tumour:\n",
    "            fixed_id = fixed_num_str+'r'\n",
    "        elif fixed_num_str+'l' in target_t2_keys_w_tumour:\n",
    "            fixed_id = fixed_num_str+'l'\n",
    "        split_dict = staple_dict.get(fixed_id, {})\n",
    "        split_dict[staple_split] = dict(\n",
    "            warped_label=torch.tensor(nib.load(_file).get_fdata()).long().to_sparse(),\n",
    "            file_path=_file\n",
    "        )\n",
    "        staple_dict[fixed_id] = split_dict\n",
    "        print(staple_split, fixed_id)\n",
    "    \n",
    "torch.save(staple_dict, \"./crossmoda_staple_consensus.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare STAPLE scores to network scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staple_scores = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220120_crossmoda_staple/staple_consensus_text_dict.pth\")\n",
    "# network_scores = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/classic-sunset-1245_fold0_epx39/train_label_snapshot.pth\")\n",
    "network_scores = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/data/output/swept-wind-1249_fold0_epx39/train_label_snapshot.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ids = network_scores['d_ids']\n",
    "network_parameter = network_scores['data_parameters']\n",
    "\n",
    "staple_sensitivities = {}\n",
    "\n",
    "for fixed_id, moving_dict in staple_scores.items():\n",
    "    for moving_id, rater_dict in moving_dict.items():\n",
    "        if 'sensitivity' in rater_dict:\n",
    "            staple_sensitivities[f\"{fixed_id}:m{moving_id}\"] = rater_dict['sensitivity']\n",
    "        else:\n",
    "            print(\"No sensitivity value for id \", f\"{fixed_id}:m{moving_id}\")\n",
    "\n",
    "matching_ids = set(d_ids).intersection(set(staple_sensitivities.keys()))\n",
    "print(\"Matching ids #\", len(matching_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_ids = set(d_ids).difference(set(staple_sensitivities.keys()))\n",
    "\n",
    "print(len(staple_sensitivities), len(d_ids))\n",
    "non_matching_ids\n",
    "\n",
    "# non_matching_ids_inv = set(staple_sensitivities.keys()).difference(set(d_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wise_dices = {}\n",
    "\n",
    "for _id in tqdm(d_ids):\n",
    "    network_data_lookup_idx = d_ids.index(_id)\n",
    "\n",
    "    registered_label = network_scores['modified_labels'][network_data_lookup_idx]\n",
    "    ground_truth = network_scores['labels'][network_data_lookup_idx]\n",
    "\n",
    "    wise_dices[_id] = dice3d(\n",
    "        torch.nn.functional.one_hot(registered_label.to_dense().unsqueeze(0), 2),\n",
    "        torch.nn.functional.one_hot(ground_truth.to_dense().unsqueeze(0), 2),\n",
    "        one_hot_torch_style=True, nan_for_unlabeled_target=False\n",
    "    )\n",
    "torch.save(wise_dices, \"wise_dices_400.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color mov/fixed ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "staple_scores = []\n",
    "data_params = []\n",
    "dices = []\n",
    "fcolors = []\n",
    "mcolors = []\n",
    "\n",
    "f_ids = set([_id[:4] for _id in d_ids])\n",
    "fid_colors = {_id: color for color, _id in enumerate(sorted(list(f_ids)))}\n",
    "\n",
    "m_ids = set([_id[6:] for _id in d_ids])\n",
    "mid_colors = {_id: color for color, _id in enumerate(sorted(list(m_ids)))}\n",
    "\n",
    "for _id in d_ids:\n",
    "    network_data_lookup_idx = d_ids.index(_id)\n",
    "    # staple_scores.append(staple_sensitivities[_id])\n",
    "    data_params.append(network_scores['data_parameters'][network_data_lookup_idx].cpu().detach())\n",
    "    dices.append(wise_dices[_id][0,1].item())\n",
    "    fcolors.append(fid_colors[_id[:4]])\n",
    "    mcolors.append(mid_colors[_id[6:]])\n",
    "\n",
    "dices = np.array(dices)\n",
    "staple_scores = np.array(staple_scores)\n",
    "data_params = np.array([dp.item() for dp in data_params])\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(16, 4), dpi=80)\n",
    "\n",
    "sc4 = axs[0].scatter(\n",
    "    data_params, \n",
    "    dices, c=fcolors, s=10, cmap='rainbow')\n",
    "sc5 = axs[1].scatter(\n",
    "    data_params, \n",
    "    dices, c=mcolors, s=10, cmap='rainbow')\n",
    "\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.82, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(sc5, cax=cbar_ax)\n",
    "\n",
    "axs[1].set_title(\"DP/dice c=mov_id\")\n",
    "\n",
    "plt.show()\n",
    "print(len(f_ids), len(m_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create consensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_scores = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/data/output/swept-wind-1249_fold0_epx39/train_label_snapshot.pth\")\n",
    "consensus_dicts = {}\n",
    "d_ids = network_scores['d_ids']\n",
    "\n",
    "for _id in d_ids:\n",
    "    network_data_lookup_idx = d_ids.index(_id)\n",
    "    f_id = _id[:4]\n",
    "    m_id = _id[6:]\n",
    "    if f_id in consensus_dicts:\n",
    "        fixed_dict = consensus_dicts.get(f_id)\n",
    "    else:\n",
    "        fixed_dict = {}\n",
    "        #Only add expert label in first hit of fixed_dict\n",
    "        fixed_dict['expert_label'] = network_scores['labels'][network_data_lookup_idx]\n",
    "        fixed_dict['prediction'] = network_scores['train_predictions'][network_data_lookup_idx]\n",
    "\n",
    "    moving_dict = fixed_dict.get(m_id, {})\n",
    "    moving_dict['warped_label'] = network_scores['modified_labels'][network_data_lookup_idx]\n",
    "    moving_dict['data_parameter'] = network_scores['data_parameters'][network_data_lookup_idx]\n",
    "    fixed_dict[m_id] = moving_dict\n",
    "    \n",
    "    consensus_dicts[f_id] = fixed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_consensus(lbl_list, weighting_list):\n",
    "    LIMIT = .5\n",
    "    label_stack = torch.stack(lbl_list)\n",
    "    weightings = torch.tensor(weighting_list)\n",
    "    weightings = torch.softmax(weightings, 0)\n",
    "    weighted_stack = label_stack.to_dense()*weightings.view(-1,1,1,1)\n",
    "    weighted_stack = weighted_stack.sum((0))\n",
    "    consensus = (weighted_stack > LIMIT).long()\n",
    "    # print()\n",
    "    # print(\"\")\n",
    "    # plt.imshow(weighted_stack.sum((-1)))\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "\n",
    "    # print()\n",
    "    # print(\"Consensus W sum\")\n",
    "    # plt.imshow(consensus.sum((-1)))\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "\n",
    "    return consensus\n",
    "\n",
    "\n",
    "def staple_consensus(lbl_list, weighting_list):\n",
    "    staple_filter = sitk.STAPLEImageFilter()\n",
    "    weightings = torch.tensor(weighting_list)\n",
    "    weightings = torch.softmax(weightings, 0)\n",
    "    # sitk.ProcessObject.SetGlobalDefaultDebugOff()\n",
    "    FOREGROUND = 1.0\n",
    "    staple_filter.SetForegroundValue(FOREGROUND)\n",
    "    staple_filter.SetMaximumIterations(200)\n",
    "    sitk_moving_data = [sitk.GetImageFromArray(lbl.to_dense().numpy().astype(np.int16)) for lbl in lbl_list]\n",
    "    \n",
    "    staple_out = staple_filter.Execute(sitk_moving_data)\n",
    "    # staple_filter.SetConfidenceWeight(weightings.tolist())\n",
    "    consensus = (torch.tensor(sitk.GetArrayFromImage(staple_out)) > .5).long()\n",
    "\n",
    "    return consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['147l', '143l', '188l', '192l', '146l', '145l', '164l', '178l', '140l', '175l', '179r', '189l', '177l', '205r', '152l', '149l', '190l', '187l', '162l', '141l', '158l', '202l', '183l', '200l', '165r', '166r', '185r', '167r', '174r', '199l', '181r', '171r', '198r', '209r', '195r', '204r', '168r', '180r', '173r', '210r'])\n",
      "DP consensus dice: tensor([[0.9990, 0.8031]])\n",
      "STAPLE consensus dice: tensor([[0.9910, 0.3601]])\n",
      "DP consensus dice: tensor([[0.9931, 0.6378]])\n",
      "STAPLE consensus dice: tensor([[0.9930, 0.7607]])\n",
      "DP consensus dice: tensor([[0.9881, 0.5278]])\n",
      "STAPLE consensus dice: tensor([[0.9905, 0.7167]])\n",
      "DP consensus dice: tensor([[0.9983, 0.6660]])\n",
      "STAPLE consensus dice: tensor([[0.9962, 0.5997]])\n",
      "DP consensus dice: tensor([[0.9895, 0.5185]])\n",
      "STAPLE consensus dice: tensor([[0.9907, 0.6815]])\n",
      "DP consensus dice: tensor([[0.9984, 0.7974]])\n",
      "STAPLE consensus dice: tensor([[0.9959, 0.6335]])\n",
      "DP consensus dice: tensor([[0.9947, 0.6758]])\n",
      "STAPLE consensus dice: tensor([[0.9949, 0.7166]])\n",
      "DP consensus dice: tensor([[0.9935, 0.6933]])\n",
      "STAPLE consensus dice: tensor([[0.9933, 0.6969]])\n",
      "DP consensus dice: tensor([[0.9986, 0.7989]])\n",
      "STAPLE consensus dice: tensor([[0.9910, 0.4326]])\n",
      "DP consensus dice: tensor([[0.9962, 0.6842]])\n",
      "STAPLE consensus dice: tensor([[0.9954, 0.6867]])\n",
      "DP consensus dice: tensor([[0.9968, 0.8949]])\n",
      "STAPLE consensus dice: tensor([[0.9961, 0.8817]])\n",
      "DP consensus dice: tensor([[0.9958, 0.0817]])\n",
      "STAPLE consensus dice: tensor([[0.9945, 0.1552]])\n",
      "DP consensus dice: tensor([[0.9983, 0.8257]])\n",
      "STAPLE consensus dice: tensor([[0.9982, 0.8231]])\n",
      "DP consensus dice: tensor([[0.9924, 0.8008]])\n",
      "STAPLE consensus dice: tensor([[0.9950, 0.8827]])\n",
      "DP consensus dice: tensor([[0.9978, 0.6204]])\n",
      "STAPLE consensus dice: tensor([[0.9893, 0.2717]])\n",
      "DP consensus dice: tensor([[0.9956, 0.7339]])\n",
      "STAPLE consensus dice: tensor([[0.9890, 0.5520]])\n",
      "DP consensus dice: tensor([[0.9981, 0.6768]])\n",
      "STAPLE consensus dice: tensor([[0.9964, 0.5347]])\n",
      "DP consensus dice: tensor([[0.9870, 0.5696]])\n",
      "STAPLE consensus dice: tensor([[0.9870, 0.5918]])\n",
      "DP consensus dice: tensor([[0.9902, 0.6114]])\n",
      "STAPLE consensus dice: tensor([[0.9901, 0.6277]])\n",
      "DP consensus dice: tensor([[0.9983, 0.2116]])\n",
      "STAPLE consensus dice: tensor([[0.9921, 0.0745]])\n",
      "DP consensus dice: tensor([[0.9844, 0.5337]])\n",
      "STAPLE consensus dice: tensor([[0.9862, 0.6083]])\n",
      "DP consensus dice: tensor([[0.9938, 0.6205]])\n",
      "STAPLE consensus dice: tensor([[0.9953, 0.7666]])\n",
      "DP consensus dice: tensor([[0.9977, 0.8100]])\n",
      "STAPLE consensus dice: tensor([[0.9978, 0.8501]])\n",
      "DP consensus dice: tensor([[0.9966, 0.2091]])\n",
      "STAPLE consensus dice: tensor([[0.9950, 0.1952]])\n",
      "DP consensus dice: tensor([[0.9974, 0.6173]])\n",
      "STAPLE consensus dice: tensor([[0.9936, 0.4465]])\n",
      "DP consensus dice: tensor([[0.9908, 0.6600]])\n",
      "STAPLE consensus dice: tensor([[0.9940, 0.8162]])\n",
      "DP consensus dice: tensor([[0.9894, 0.6726]])\n",
      "STAPLE consensus dice: tensor([[0.9916, 0.7768]])\n",
      "DP consensus dice: tensor([[0.9964, 0.7946]])\n",
      "STAPLE consensus dice: tensor([[0.9959, 0.8003]])\n",
      "DP consensus dice: tensor([[0.9960, 0.8556]])\n",
      "STAPLE consensus dice: tensor([[0.9959, 0.8635]])\n",
      "DP consensus dice: tensor([[0.9972, 0.5461]])\n",
      "STAPLE consensus dice: tensor([[0.9959, 0.4792]])\n",
      "DP consensus dice: tensor([[0.9960, 0.7900]])\n",
      "STAPLE consensus dice: tensor([[0.9948, 0.7606]])\n",
      "DP consensus dice: tensor([[0.9977, 0.7152]])\n",
      "STAPLE consensus dice: tensor([[0.9945, 0.5168]])\n",
      "DP consensus dice: tensor([[0.9974, 0.3579]])\n",
      "STAPLE consensus dice: tensor([[0.9935, 0.2467]])\n",
      "DP consensus dice: tensor([[0.9969, 0.5340]])\n",
      "STAPLE consensus dice: tensor([[0.9931, 0.3733]])\n",
      "DP consensus dice: tensor([[0.9971, 0.6002]])\n",
      "STAPLE consensus dice: tensor([[0.9935, 0.4234]])\n",
      "DP consensus dice: tensor([[0.9980, 0.5798]])\n",
      "STAPLE consensus dice: tensor([[0.9938, 0.3465]])\n",
      "DP consensus dice: tensor([[0.9978, 0.7676]])\n",
      "STAPLE consensus dice: tensor([[0.9949, 0.6385]])\n",
      "DP consensus dice: tensor([[0.9978, 0.7170]])\n",
      "STAPLE consensus dice: tensor([[0.9946, 0.5254]])\n",
      "DP consensus dice: tensor([[0.9983, 0.8110]])\n",
      "STAPLE consensus dice: tensor([[0.9962, 0.6739]])\n",
      "DP consensus dice: tensor([[0.9981, 0.6939]])\n",
      "STAPLE consensus dice: tensor([[0.9947, 0.4594]])\n"
     ]
    }
   ],
   "source": [
    "dp_consensus_dices = []\n",
    "staple_consensus_dices = []\n",
    "\n",
    "print(consensus_dicts.keys())\n",
    "for f_id, fixed_dict in consensus_dicts.items():\n",
    "    # print(len(fixed_dict))\n",
    "    lbls = []\n",
    "    lbls = [elem['warped_label'] for elem in fixed_dict.values() if isinstance(elem, dict)]\n",
    "    data_parameters = []\n",
    "    data_parameters = [elem['data_parameter'] for elem in fixed_dict.values() if isinstance(elem, dict)]\n",
    "    expert_label = fixed_dict['expert_label'].to_dense()\n",
    "\n",
    "    # lbls.append(fixed_dict['prediction'].to_dense().squeeze().to_sparse())\n",
    "    # data_parameters.append(max(data_parameters))\n",
    "    # data_parameters.append(0.)\n",
    "\n",
    "    simple_consensus = create_simple_consensus(lbls, data_parameters)\n",
    "    staaple_consensus = staple_consensus(lbls, data_parameters)\n",
    "    fixed_dict['simple_consensus'] = simple_consensus.to_sparse()\n",
    "    fixed_dict['staple_consensus'] = staaple_consensus.to_sparse()\n",
    "    consensus_dicts[f_id] = fixed_dict\n",
    "    # print()\n",
    "    # print(\"Expert label W sum\")\n",
    "    # plt.imshow(expert_label.float().mean((-1)))\n",
    "    # plt.show()\n",
    "    dp_dsc = dice3d(\n",
    "        torch.nn.functional.one_hot(simple_consensus.unsqueeze(0), 2),\n",
    "        torch.nn.functional.one_hot(expert_label.unsqueeze(0), 2),\n",
    "        one_hot_torch_style=True, nan_for_unlabeled_target=False\n",
    "    )\n",
    "    staple_dsc = dice3d(\n",
    "        torch.nn.functional.one_hot(staaple_consensus.unsqueeze(0), 2),\n",
    "        torch.nn.functional.one_hot(expert_label.unsqueeze(0), 2),\n",
    "        one_hot_torch_style=True, nan_for_unlabeled_target=False\n",
    "    )\n",
    "    print(f\"DP consensus dice:\", dp_dsc)\n",
    "    print(f\"STAPLE consensus dice:\", staple_dsc)\n",
    "    dp_consensus_dices.append(dp_dsc)\n",
    "    staple_consensus_dices.append(staple_dsc)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP consensus mean dice: 0.643\n",
      "STAPLE consensus mean dice: 0.581\n"
     ]
    }
   ],
   "source": [
    "mean_dp_consensus = torch.cat(dp_consensus_dices)[:,1].mean()\n",
    "print(f\"DP consensus mean dice: {mean_dp_consensus.item():.3f}\")\n",
    "mean_staple = torch.cat(staple_consensus_dices)[:,1].mean()\n",
    "print(f\"STAPLE consensus mean dice: {mean_staple.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f_id, dct in consensus_dicts.items():\n",
    "    lbls = list(dct.values())\n",
    "\n",
    "    for lbl in \n",
    "print(len(consensus_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ba18e7235aac9e9578e5cb0339dda8530a2db7f5b95196862af1240ab4b857"
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
