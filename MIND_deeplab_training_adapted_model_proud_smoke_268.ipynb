{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3175666",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name                     Util    Mem free  Cuda             User(s)\n",
      "----  --------------------------  ------  ----------  ---------------  ---------\n",
      "   2  NVIDIA GeForce RTX 2080 Ti     0 %   11018 MiB  11.5(495.29.05)\n",
      "   3  NVIDIA GeForce RTX 2080 Ti     0 %    9269 MiB  11.5(495.29.05)  nicke\n",
      "   1  NVIDIA GeForce RTX 2080 Ti     0 %    8801 MiB  11.5(495.29.05)  weihsbach\n",
      "   0  NVIDIA GeForce RTX 2080 Ti     0 %    2575 MiB  11.5(495.29.05)  schneider\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   2  NVIDIA GeForce RTX 2080 Ti  ->  cuda:0\n",
      "1.9.0a0+gitdfbd030\n",
      "8204\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Running in: /share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "import glob\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -4\"))\n",
    "import pickle\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import functools\n",
    "from enum import Enum, auto\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "import scipy\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import visualize_seg\n",
    "\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "from curriculum_deeplab.utils import interpolate_sample, in_notebook, dilate_label_class, LabelDisturbanceMode, ensure_dense\n",
    "from curriculum_deeplab.CrossmodaHybridIdLoader import CrossmodaHybridIdLoader, get_crossmoda_data_load_closure\n",
    "from curriculum_deeplab.MobileNet_LR_ASPP_3D import MobileNet_LRASPP_3D, MobileNet_ASPP_3D\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "if in_notebook:\n",
    "    THIS_SCRIPT_DIR = os.path.abspath('')\n",
    "else:\n",
    "    THIS_SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "print(f\"Running in: {THIS_SCRIPT_DIR}\")\n",
    "\n",
    "def get_batch_dice_per_class(b_dice, class_tags, exclude_bg=True) -> dict:\n",
    "    score_dict = {}\n",
    "    for cls_idx, cls_tag in enumerate(class_tags):\n",
    "        if exclude_bg and cls_idx == 0:\n",
    "            continue\n",
    "\n",
    "        if torch.all(torch.isnan(b_dice[:,cls_idx])):\n",
    "            score = float('nan')\n",
    "        else:\n",
    "            score = np.nanmean(b_dice[:,cls_idx]).item()\n",
    "\n",
    "        score_dict[cls_tag] = score\n",
    "\n",
    "    return score_dict\n",
    "\n",
    "def get_batch_dice_over_all(b_dice, exclude_bg=True) -> float:\n",
    "\n",
    "    start_idx = 1 if exclude_bg else 0\n",
    "    if torch.all(torch.isnan(b_dice[:,start_idx:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,start_idx:]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5750fa2f-e706-4c65-ac09-c3ab8050b479",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DataParamMode(Enum):\n",
    "    INSTANCE_PARAMS = auto()\n",
    "    GRIDDED_INSTANCE_PARAMS = auto()\n",
    "    DISABLED = auto()\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "        See https://stackoverflow.com/questions/49901590/python-using-copy-deepcopy-on-dotdict\n",
    "    \"\"\"\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        try:\n",
    "            return self[item]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError from e\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "\n",
    "    'use_mind': False,\n",
    "    'epochs': 30,\n",
    "\n",
    "    'batch_size': 8,\n",
    "    'val_batch_size': 1,\n",
    "    'use_2d_normal_to': None,\n",
    "    'train_patchwise': False,\n",
    "\n",
    "    'dataset': 'crossmoda',\n",
    "    'reg_state': \"acummulate_every_deeds_FT2_MT1\",\n",
    "    'train_set_max_len': None,\n",
    "    'crop_3d_w_dim_range': (45, 95),\n",
    "    'crop_2d_slices_gt_num_threshold': 0,\n",
    "\n",
    "    'lr': 0.01,\n",
    "    'use_scheduling': True,\n",
    "\n",
    "    # Data parameter config\n",
    "    'data_param_mode': DataParamMode.INSTANCE_PARAMS,\n",
    "    'init_inst_param': 0.0,\n",
    "    'lr_inst_param': 0.1,\n",
    "    'use_risk_regularization': True,\n",
    "    'use_parallel_dp_loss': True,\n",
    "\n",
    "    'grid_size_y': 64,\n",
    "    'grid_size_x': 64,\n",
    "\n",
    "    'fixed_weight_file': None,#\"/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/data/output/swept-wind-1249_fold0_epx39/train_label_snapshot.pth\",\n",
    "    'fixed_weight_min_quantile': None,#.9,\n",
    "    'fixed_weight_min_value': None,\n",
    "    'override_embedding_weights': False,\n",
    "    # ),\n",
    "\n",
    "    'save_every': 200,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'debug': False,\n",
    "    'wandb_mode': 'online', # e.g. online, disabled\n",
    "    'do_sweep': False,\n",
    "\n",
    "    'checkpoint_name': None,\n",
    "    'do_plot': False,\n",
    "    'save_dp_figures': False,\n",
    "\n",
    "    'disturbance_mode': None,\n",
    "    'disturbance_strength': 0.,\n",
    "    'disturbed_percentage': 0.,\n",
    "    'start_disturbing_after_ep': 0,\n",
    "\n",
    "    'start_dilate_kernel_sz': 1\n",
    "})\n",
    "\n",
    "if config_dict.train_patchwise:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de18b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(config):\n",
    "    reset_determinism()\n",
    "    if config.reg_state:\n",
    "        print(\"Loading registered data.\")\n",
    "\n",
    "        if config.reg_state == \"mix_combined_best\":\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "\n",
    "            perm = np.random.permutation(len(loaded_identifier))\n",
    "            _clen = int(.5*len(loaded_identifier))\n",
    "            best_choice = perm[:_clen]\n",
    "            combined_choice = perm[_clen:]\n",
    "\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)[best_choice]\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)[combined_choice]\n",
    "            label_data = torch.zeros([107,128,128,128])\n",
    "            label_data[best_choice] = best_label_data\n",
    "            label_data[combined_choice] = combined_label_data\n",
    "            var_identifier = [\"mBST\" if idx in best_choice else \"mCMB\" for idx in range(len(loaded_identifier))]\n",
    "            loaded_identifier = [f\"{_id}:{var_id}\" for _id, var_id in zip(loaded_identifier, var_identifier)]\n",
    "\n",
    "        elif config.reg_state == \"acummulate_combined_best\":\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)\n",
    "            label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            loaded_identifier = [_id+':mBST' for _id in loaded_identifier] + [_id+':mCMB' for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"best\":\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            label_data = torch.cat([label_data_left[config.reg_state+'_all'][:44], label_data_right[config.reg_state+'_all'][:63]], dim=0)\n",
    "            postfix = 'mBST'\n",
    "            loaded_identifier = [_id+':'+postfix for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"combined\":\n",
    "            domain = 'source'\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            label_data = torch.cat([label_data_left[config.reg_state+'_all'][:44], label_data_right[config.reg_state+'_all'][:63]], dim=0)\n",
    "            postfix = 'mCMB'\n",
    "            loaded_identifier = [_id+':'+postfix for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"acummulate_convex_adam_FT2_MT1\":\n",
    "            domain = 'target'\n",
    "            bare_data = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220114_crossmoda_multiple_registrations/crossmoda_convex_registered.pth\")\n",
    "            label_data = []\n",
    "            loaded_identifier = []\n",
    "            for fixed_id, moving_dict in bare_data.items():\n",
    "                for moving_id, moving_sample in moving_dict.items():\n",
    "                    label_data.append(moving_sample['warped_label'])\n",
    "                    loaded_identifier.append(f\"{fixed_id}:m{moving_id}\")\n",
    "\n",
    "        elif config.reg_state == \"acummulate_every_third_deeds_FT2_MT1\":\n",
    "            domain = 'target'\n",
    "            bare_data = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220114_crossmoda_multiple_registrations/crossmoda_deeds_registered.pth\")\n",
    "            label_data = []\n",
    "            loaded_identifier = []\n",
    "            for fixed_id, moving_dict in bare_data.items():\n",
    "                sorted_moving_dict = OrderedDict(moving_dict)\n",
    "                for idx_mov, (moving_id, moving_sample) in enumerate(sorted_moving_dict.items()):\n",
    "                    # Only use every third warped sample\n",
    "                    if idx_mov % 3 == 0:\n",
    "                        label_data.append(moving_sample['warped_label'].cpu())\n",
    "                        loaded_identifier.append(f\"{fixed_id}:m{moving_id}\")\n",
    "\n",
    "        elif config.reg_state == \"acummulate_every_deeds_FT2_MT1\":\n",
    "            domain = 'target'\n",
    "            bare_data = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220114_crossmoda_multiple_registrations/crossmoda_deeds_registered.pth\")\n",
    "            label_data = []\n",
    "            loaded_identifier = []\n",
    "            for fixed_id, moving_dict in bare_data.items():\n",
    "                sorted_moving_dict = OrderedDict(moving_dict)\n",
    "                for idx_mov, (moving_id, moving_sample) in enumerate(sorted_moving_dict.items()):\n",
    "                    label_data.append(moving_sample['warped_label'].cpu())\n",
    "                    loaded_identifier.append(f\"{fixed_id}:m{moving_id}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        modified_3d_label_override = {}\n",
    "        for idx, identifier in enumerate(loaded_identifier):\n",
    "            # Find sth. like 100r:mBST or 100r:m001l\n",
    "            nl_id, lr_id, m_id = re.findall(r'(\\d{1,3})([lr]):m([A-Z0-9a-z]{3,4})$', identifier)[0]\n",
    "            nl_id = int(nl_id)\n",
    "            crossmoda_var_id = f\"{nl_id:03d}{lr_id}:m{m_id}\"\n",
    "            modified_3d_label_override[crossmoda_var_id] = label_data[idx]\n",
    "\n",
    "        prevent_disturbance = True\n",
    "\n",
    "    else:\n",
    "        domain = 'source'\n",
    "        modified_3d_label_override = None\n",
    "        prevent_disturbance = False\n",
    "\n",
    "    if config.dataset == 'crossmoda':\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.5\n",
    "        clsre = get_crossmoda_data_load_closure(\n",
    "            base_dir=\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "            domain=domain, state='l4', use_additional_data=False,\n",
    "            size=(128,128,128), resample=True, normalize=True, crop_3d_w_dim_range=config.crop_3d_w_dim_range,\n",
    "            ensure_labeled_pairs=True, modified_3d_label_override=modified_3d_label_override,\n",
    "            debug=config.debug\n",
    "        )\n",
    "        training_dataset = CrossmodaHybridIdLoader(\n",
    "            clsre,\n",
    "            size=(128,128,128), resample=True, normalize=True, crop_3d_w_dim_range=config.crop_3d_w_dim_range,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor,\n",
    "            fixed_weight_file=config.fixed_weight_file, fixed_weight_min_quantile=config.fixed_weight_min_quantile, fixed_weight_min_value=config.fixed_weight_min_value,\n",
    "        )\n",
    "\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "    if config.dataset == 'ixi':\n",
    "        raise NotImplementedError()\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.\n",
    "        clsre = get_ixi_data_load_closure()\n",
    "        training_dataset = IXIHybridIdLoader(\n",
    "            clsre,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(f\"Nonzero slices: \" \\\n",
    "            f\"{sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)*100}%\"\n",
    "        )\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "\n",
    "    elif config['dataset'] == 'organmnist3d':\n",
    "        training_dataset = WrapperOrganMNIST3D(\n",
    "            split='train', root='./data/medmnist', download=True, normalize=True,\n",
    "            max_load_num=300, crop_3d_w_dim_range=None,\n",
    "            disturbed_idxs=None, use_2d_normal_to='W'\n",
    "        )\n",
    "        print(training_dataset.mnist_set.info)\n",
    "        print(\"Classes: \", training_dataset.label_tags)\n",
    "        print(\"Samples: \", len(training_dataset))\n",
    "\n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fbc5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    _, all_labels, _ = training_dataset.get_data(use_2d_override=False)\n",
    "    print(all_labels.shape)\n",
    "    sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "    plt.xlabel(\"W\")\n",
    "    plt.ylabel(\"ground truth>0\")\n",
    "    plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0adbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    # Print bare 2D data\n",
    "    # print(\"Displaying 2D bare sample\")\n",
    "    # for img, label in zip(training_dataset.img_data_2d.values(),\n",
    "    #                       training_dataset.label_data_2d.values()):\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=img.unsqueeze(0),\n",
    "    #                 ground_truth=label,\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "    # Print transformed 2D data\n",
    "    training_dataset.train(use_modified=False, augment=True)\n",
    "    print(training_dataset.disturbed_idxs)\n",
    "\n",
    "    print(\"Displaying 2D training sample\")\n",
    "    for dist_stren in [0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 5.0]:\n",
    "        print(dist_stren)\n",
    "        training_dataset.disturb_idxs(list(range(0,20,2)),\n",
    "            disturbance_mode=LabelDisturbanceMode.AFFINE,\n",
    "            disturbance_strength=dist_stren\n",
    "        )\n",
    "        img_stack = []\n",
    "        label_stack = []\n",
    "        mod_label_stack = []\n",
    "\n",
    "        for sample in (training_dataset[idx] for idx in range(20)):\n",
    "            img_stack.append(sample['image'])\n",
    "            label_stack.append(sample['label'])\n",
    "            mod_label_stack.append(sample['modified_label'])\n",
    "\n",
    "        # Change label num == hue shift for display\n",
    "        img_stack = torch.stack(img_stack).unsqueeze(1)\n",
    "        label_stack = torch.stack(label_stack)\n",
    "        mod_label_stack = torch.stack(mod_label_stack)\n",
    "\n",
    "        mod_label_stack*=4\n",
    "\n",
    "        visualize_seg(in_type=\"batch_2D\",\n",
    "            img=img_stack,\n",
    "            # ground_truth=label_stack,\n",
    "            seg=(mod_label_stack-label_stack).abs(),\n",
    "            # crop_to_non_zero_gt=True,\n",
    "            crop_to_non_zero_seg=True,\n",
    "            alpha_seg = .6,\n",
    "            file_path=f'out{dist_stren}.png'\n",
    "        )\n",
    "\n",
    "    # Print transformed 3D data\n",
    "    # training_dataset.train()\n",
    "    # print(\"Displaying 3D training sample\")\n",
    "    # leng = 1# training_dataset.__len__(use_2d_override=False)\n",
    "    # for sample in (training_dataset.get_3d_item(idx) for idx in range(leng)):\n",
    "    #     # training_dataset.set_dilate_kernel_size(1)\n",
    "    #     visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "    #                 img=sample['image'].unsqueeze(0),\n",
    "    #                 ground_truth=sample['label'],\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "#         # training_dataset.set_dilate_kernel_size(7)\n",
    "#         display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "#                     img=sample['image'].unsqueeze(0),\n",
    "#                     ground_truth=sample['modified_label'],\n",
    "#                     crop_to_non_zero_gt=True,\n",
    "#                     alpha_gt = .3)\n",
    "\n",
    "    for sidx in [0,]:\n",
    "        print(f\"Sample {sidx}:\")\n",
    "\n",
    "        training_dataset.eval()\n",
    "        sample_eval = training_dataset.get_3d_item(sidx)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "\n",
    "        training_dataset.train()\n",
    "        print(\"Train sample with ground-truth overlay\")\n",
    "        sample_train = training_dataset.get_3d_item(sidx)\n",
    "        print(sample_train['label'].unique())\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_train['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_train['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt=.3)\n",
    "\n",
    "        print(\"Eval/train diff with diff overlay\")\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=(sample_eval['image'] - sample_train['image']).unsqueeze(0),\n",
    "                    ground_truth=(sample_eval['label'] - sample_train['label']).clamp(min=0),\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "    train_plotset = (training_dataset.get_3d_item(idx) for idx in (55, 81, 63))\n",
    "    for sample in train_plotset:\n",
    "        print(f\"Sample {sample['dataset_idx']}:\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .6)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d52956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use module.named_modules() to retrieve valid keychains for layers.\n",
    "       e.g.\n",
    "       first_keychain = list(module.keys())[0]\n",
    "       new_first_replacee = torch.nn.Conv1d(1,2,3)\n",
    "       set_module(first_keychain, torch.nn.Conv1d(1,2,3))\n",
    "    \"\"\"\n",
    "\n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "335f4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(_path, **statefuls):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "    _path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for name, stful in statefuls.items():\n",
    "        if stful != None:\n",
    "            torch.save(stful.state_dict(), _path.joinpath(name+'.pth'))\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config, dataset_len, num_classes, THIS_SCRIPT_DIR, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        # Use vanilla torch model\n",
    "        lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(\n",
    "            pretrained=False, progress=True, num_classes=num_classes\n",
    "        )\n",
    "        set_module(lraspp, 'backbone.0.0',\n",
    "            torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2),\n",
    "                            padding=(1, 1), bias=False)\n",
    "        )\n",
    "    else:\n",
    "        # Use custom 3d model\n",
    "        lraspp = MobileNet_LRASPP_3D(\n",
    "            in_num=in_channels, num_classes=num_classes,\n",
    "            use_checkpointing=True\n",
    "        )\n",
    "\n",
    "    # lraspp.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "    lraspp.to(device)\n",
    "    print(f\"Param count lraspp: {sum(p.numel() for p in lraspp.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(lraspp.parameters(), lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # Add data paramters embedding and optimizer\n",
    "    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len, 1, sparse=True)\n",
    "        # p_offset = torch.zeros(1, layout=torch.strided, requires_grad=True)\n",
    "        # p_offset.grad = torch.sparse_coo_tensor([[0]], 1., size=(1,))\n",
    "\n",
    "        # embedding.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "        # embedding.sigmoid_offset.register_hook(lambda grad: torch.sparse_coo_tensor([[0]], grad, size=(1,)))\n",
    "        embedding = embedding.to(device)\n",
    "\n",
    "    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len*config.grid_size_y*config.grid_size_x, 1, sparse=True).to(device)\n",
    "    else:\n",
    "        embedding = None\n",
    "\n",
    "\n",
    "    if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "        optimizer_dp = torch.optim.SparseAdam(\n",
    "            embedding.parameters(), lr=config.lr_inst_param,\n",
    "            betas=(0.9, 0.999), eps=1e-08)\n",
    "        torch.nn.init.normal_(embedding.weight.data, mean=config.init_inst_param, std=0.00)\n",
    "\n",
    "        if _path and _path.is_dir():\n",
    "            print(f\"Loading embedding and dp_optimizer from {_path}\")\n",
    "            optimizer_dp.load_state_dict(torch.load(_path.joinpath('optimizer_dp.pth'), map_location=device))\n",
    "            embedding.load_state_dict(torch.load(_path.joinpath('embedding.pth'), map_location=device))\n",
    "\n",
    "        print(f\"Param count embedding: {sum(p.numel() for p in embedding.parameters())}\")\n",
    "\n",
    "    else:\n",
    "        optimizer_dp = None\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading lr-aspp model, optimizers and grad scalers from {_path}\")\n",
    "        lraspp.load_state_dict(torch.load(_path.joinpath('lraspp.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "\n",
    "    return (lraspp, optimizer, optimizer_dp, embedding, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "490f1722",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "\n",
    "\n",
    "\n",
    "def save_parameter_figure(_path, title, text, parameters, reweighted_parameters, dices):\n",
    "    # Show weights and weights with compensation\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12, 4), dpi=80)\n",
    "    sc1 = axs[0].scatter(\n",
    "        range(len(parameters)),\n",
    "        parameters.cpu().detach(), c=dices,s=1, cmap='plasma', vmin=0., vmax=1.)\n",
    "    sc2 = axs[1].scatter(\n",
    "        range(len(reweighted_parameters)),\n",
    "        reweighted_parameters.cpu().detach(), s=1,c=dices, cmap='plasma', vmin=0., vmax=1.)\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.text(0, 0, text)\n",
    "    axs[0].set_title('Bare parameters')\n",
    "    axs[1].set_title('Reweighted parameters')\n",
    "    axs[0].set_ylim(-10, 10)\n",
    "    axs[1].set_ylim(-3, 1)\n",
    "    plt.colorbar(sc2)\n",
    "    plt.savefig(_path)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "\n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "\n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "\n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "\n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio\n",
    "\n",
    "def log_data_parameter_stats(log_path, epx, data_parameters):\n",
    "    \"\"\"Log stats for data parameters on wandb.\"\"\"\n",
    "    wandb.log({f'{log_path}/highest': torch.max(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/lowest': torch.min(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/mean': torch.mean(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/std': torch.std(data_parameters).item()}, step=epx)\n",
    "\n",
    "\n",
    "\n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_class_dices(log_prefix, log_postfix, class_dices, log_idx):\n",
    "    if not class_dices:\n",
    "        return\n",
    "\n",
    "    for cls_name in class_dices[0].keys():\n",
    "        log_path = f\"{log_prefix}{cls_name}{log_postfix}\"\n",
    "\n",
    "        cls_dices = list(map(lambda dct: dct[cls_name], class_dices))\n",
    "        mean_per_class =np.nanmean(cls_dices)\n",
    "        print(log_path, f\"{mean_per_class*100:.2f}%\")\n",
    "        wandb.log({log_path: mean_per_class}, step=log_idx)\n",
    "\n",
    "def CELoss(logits, targets, bin_weight=None):\n",
    "    # -logits[0,targets[0,0,0],0,0]+torch.log(logits[0,:,0,0].exp().sum())\n",
    "    targets = torch.nn.functional.one_hot(targets).permute(0,3,1,2)\n",
    "    loss = -targets*F.log_softmax(logits, 1)\n",
    "\n",
    "    if bin_weight is not None:\n",
    "        brdcast_shape = torch.Size((loss.shape[0], loss.shape[1], *((loss.dim()-2)*[1])))\n",
    "        inv_weight = (bin_weight+np.exp(1)).log()+np.exp(1)\n",
    "        inv_weight = inv_weight/inv_weight.mean()\n",
    "        loss = inv_weight.view(brdcast_shape)*loss\n",
    "\n",
    "    return loss.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "779c09b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def map_embedding_idxs(idxs, grid_size_y, grid_size_x):\n",
    "    with torch.no_grad():\n",
    "        t_sz = grid_size_y * grid_size_x\n",
    "        return ((idxs*t_sz).long().repeat(t_sz).view(t_sz, idxs.numel())+torch.tensor(range(t_sz)).to(idxs).view(t_sz,1)).permute(1,0).reshape(-1)\n",
    "\n",
    "def inference_wrap(lraspp, img, use_2d, use_mind):\n",
    "    with torch.inference_mode():\n",
    "        b_img = img.unsqueeze(0).unsqueeze(0).float()\n",
    "        if use_2d and use_mind:\n",
    "            # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "            b_img = mindssc(b_img.unsqueeze(0)).squeeze(2)\n",
    "        elif not use_2d and use_mind:\n",
    "            # MIND 3D in Bx1xDxHxW out BxMINDxDxHxW\n",
    "            b_img = mindssc(b_img)\n",
    "        elif use_2d or not use_2d:\n",
    "            # 2D Bx1xHxW\n",
    "            # 3D out Bx1xDxHxW\n",
    "            pass\n",
    "\n",
    "        b_out = lraspp(b_img)['out']\n",
    "        b_out = b_out.argmax(1)\n",
    "        return b_out\n",
    "\n",
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    # kf.get_n_splits(training_dataset.__len__(use_2d_override=False))\n",
    "    fold_iter = enumerate(kf.split(range(training_dataset.__len__(use_2d_override=False))))\n",
    "\n",
    "    if config.get('fold_override', None):\n",
    "        selected_fold = config.get('fold_override', 0)\n",
    "        fold_iter = list(fold_iter)[selected_fold:selected_fold+1]\n",
    "    elif config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "\n",
    "    if config.wandb_mode != 'disabled':\n",
    "        warnings.warn(\"Logging of dataset file paths is disabled.\")\n",
    "        # # Log dataset info\n",
    "        # training_dataset.eval()\n",
    "        # dataset_info = [[smp['dataset_idx'], smp['id'], smp['image_path'], smp['label_path']] \\\n",
    "        #                 for smp in training_dataset]\n",
    "        # wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', 'id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        n_dims = (-2,-1)\n",
    "    else:\n",
    "        n_dims = (-3,-2,-1)\n",
    "\n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        all_3d_ids = training_dataset.get_3d_ids()\n",
    "\n",
    "        # Override fold idxs #TODO automate\n",
    "        if config.debug:\n",
    "            NUM_VAL_IMAGES = 2\n",
    "            NUM_REGISTRATIONS_PER_IMG = 1\n",
    "        else:\n",
    "            NUM_VAL_IMAGES = 0\n",
    "            NUM_REGISTRATIONS_PER_IMG = 30 #TODO automate\n",
    "\n",
    "        if config.use_2d_normal_to is not None:\n",
    "            # Override idxs\n",
    "            all_3d_ids = training_dataset.get_3d_ids()\n",
    "\n",
    "            val_3d_idxs = torch.tensor(list(range(0, NUM_VAL_IMAGES*NUM_REGISTRATIONS_PER_IMG, NUM_REGISTRATIONS_PER_IMG)))\n",
    "            val_3d_ids = training_dataset.switch_3d_identifiers(val_3d_idxs)\n",
    "\n",
    "            train_3d_idxs = list(range(NUM_VAL_IMAGES*NUM_REGISTRATIONS_PER_IMG, len(all_3d_ids)))\n",
    "\n",
    "            # Get corresponding 2D idxs\n",
    "            train_2d_ids = []\n",
    "            dcts = training_dataset.get_id_dicts()\n",
    "            for id_dict in dcts:\n",
    "                _2d_id = id_dict['2d_id']\n",
    "                _3d_idx = id_dict['3d_dataset_idx']\n",
    "                if _2d_id in training_dataset.label_data_2d.keys() and _3d_idx in train_3d_idxs:\n",
    "                    train_2d_ids.append(_2d_id)\n",
    "\n",
    "            train_2d_idxs = training_dataset.switch_2d_identifiers(train_2d_ids)\n",
    "            train_idxs = torch.tensor(train_2d_idxs)\n",
    "\n",
    "        else:\n",
    "            val_3d_idxs = torch.tensor(list(range(0, NUM_VAL_IMAGES*NUM_REGISTRATIONS_PER_IMG, NUM_REGISTRATIONS_PER_IMG)))\n",
    "            val_3d_ids = training_dataset.switch_3d_identifiers(val_3d_idxs)\n",
    "\n",
    "            train_3d_idxs = list(range(NUM_VAL_IMAGES*NUM_REGISTRATIONS_PER_IMG, len(all_3d_ids)))\n",
    "            train_idxs = torch.tensor(train_3d_idxs)\n",
    "\n",
    "        print(f\"Will run validation with these 3D samples (#{len(val_3d_ids)}):\", sorted(val_3d_ids))\n",
    "\n",
    "        _, _, all_modified_segs = training_dataset.get_data()\n",
    "\n",
    "        if config.disturbed_percentage > 0.:\n",
    "            with torch.no_grad():\n",
    "                non_empty_train_idxs = [(all_modified_segs[train_idxs].sum(dim=n_dims) > 0)]\n",
    "\n",
    "            ### Disturb dataset (only non-emtpy idxs)###\n",
    "            proposed_disturbed_idxs = np.random.choice(non_empty_train_idxs, size=int(len(non_empty_train_idxs)*config.disturbed_percentage), replace=False)\n",
    "            proposed_disturbed_idxs = torch.tensor(proposed_disturbed_idxs)\n",
    "            training_dataset.disturb_idxs(proposed_disturbed_idxs,\n",
    "                disturbance_mode=config.disturbance_mode,\n",
    "                disturbance_strength=config.disturbance_strength\n",
    "            )\n",
    "            disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "            disturbed_bool_vect[training_dataset.disturbed_idxs] = 1.\n",
    "        else:\n",
    "            disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "\n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, training_dataset.disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(training_dataset.disturbed_idxs))\n",
    "\n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in training_dataset.disturbed_idxs])},\n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "\n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels = 12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "\n",
    "\n",
    "\n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "            sampler=train_subsampler, pin_memory=False, drop_last=False,\n",
    "            # collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "        )\n",
    "\n",
    "        # training_dataset.set_augment_at_collate(True)\n",
    "\n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        epx_start = config.get('checkpoint_epx', 0)\n",
    "\n",
    "        if config.checkpoint_name:\n",
    "            # Load from checkpoint\n",
    "            _path = f\"{config.mdl_save_prefix}/{config.checkpoint_name}_fold{fold_idx}_epx{epx_start}\"\n",
    "        else:\n",
    "            _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx_start}\"\n",
    "\n",
    "        (lraspp, optimizer, optimizer_dp, embedding, scaler) = get_model(config, len(training_dataset), len(training_dataset.label_tags),\n",
    "            THIS_SCRIPT_DIR=THIS_SCRIPT_DIR, _path=_path, device='cuda')\n",
    "\n",
    "        if config.override_embedding_weights:\n",
    "            fixed_weightdata = torch.load(config.fixed_weight_file)\n",
    "            fixed_weights = fixed_weightdata['data_parameters']\n",
    "            fixed_d_ids = fixed_weightdata['d_ids']\n",
    "            if config.use_2d_normal_to is not None:\n",
    "                corresp_dataset_idxs = [training_dataset.get_2d_ids().index(_id) for _id in fixed_d_ids]\n",
    "            else:\n",
    "                corresp_dataset_idxs = [training_dataset.get_3d_ids().index(_id) for _id in fixed_d_ids]\n",
    "            embedding_weight_tensor = torch.zeros_like(embedding.weight)\n",
    "            embedding_weight_tensor[corresp_dataset_idxs] = fixed_weights.view(-1,1).cuda()\n",
    "            embedding = nn.Embedding(len(training_dataset), 1, sparse=True, _weight=embedding_weight_tensor)\n",
    "\n",
    "\n",
    "\n",
    "        dp_scaler = amp.GradScaler()\n",
    "\n",
    "        if config.use_2d_normal_to is not None:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=10, T_mult=2)\n",
    "        else:\n",
    "            # Use ExponentialLR in 3D\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.99)\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        dice_func = dice2d if config.use_2d_normal_to is not None else dice3d\n",
    "\n",
    "        bn_count = torch.zeros([len(training_dataset.label_tags)], device=all_modified_segs.device)\n",
    "        wise_dice = torch.zeros([len(training_dataset), len(training_dataset.label_tags)])\n",
    "        gt_num = torch.zeros([len(training_dataset)])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(\"Fetching training metrics for samples.\")\n",
    "            # _, wise_lbls, mod_lbls = training_dataset.get_data()\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            for sample in tqdm((training_dataset[idx] for idx in train_idxs), desc=\"metric:\", total=len(train_idxs)):\n",
    "                d_idxs = sample['dataset_idx']\n",
    "                wise_label, mod_label = sample['label'], sample['modified_label']\n",
    "                mod_label = mod_label.cuda()\n",
    "                wise_label = wise_label.cuda()\n",
    "                mod_label, _ = ensure_dense(mod_label)\n",
    "\n",
    "                dsc = dice_func(\n",
    "                    torch.nn.functional.one_hot(wise_label.unsqueeze(0), len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(mod_label.unsqueeze(0), len(training_dataset.label_tags)),\n",
    "                    one_hot_torch_style=True, nan_for_unlabeled_target=False\n",
    "                )\n",
    "                bn_count += torch.bincount(mod_label.reshape(-1).long(), minlength=len(training_dataset.label_tags)).cpu()\n",
    "                wise_dice[d_idxs] = dsc.cpu()\n",
    "                gt_num[d_idxs] = (mod_label > 0).sum(dim=n_dims).float().cpu()\n",
    "\n",
    "            class_weights = 1/(bn_count).float().pow(.35)\n",
    "            class_weights /= class_weights.mean()\n",
    "\n",
    "            t_metric = (gt_num+np.exp(1)).log()+np.exp(1)\n",
    "\n",
    "        if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "            union_wise_mod_label = torch.logical_or(wise_labels, mod_labels)\n",
    "            union_wise_mod_label = union_wise_mod_label.cuda()\n",
    "\n",
    "        class_weights = class_weights.cuda()\n",
    "        gt_num = gt_num.cuda()\n",
    "        t_metric = t_metric.cuda()\n",
    "\n",
    "        for epx in range(epx_start, config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "\n",
    "            lraspp.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            training_dataset.train(use_modified=(epx >= config.start_disturbing_after_ep))\n",
    "            wandb.log({\"use_modified\": float(training_dataset.use_modified)}, step=global_idx)\n",
    "\n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            class_dices = []\n",
    "\n",
    "            # Load data\n",
    "            for batch_idx, batch in tqdm(enumerate(train_dataloader), desc=\"batch:\", total=len(train_dataloader)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if optimizer_dp:\n",
    "                    optimizer_dp.zero_grad()\n",
    "\n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "                b_spat_aug_grid = batch['spat_augment_grid']\n",
    "\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "                b_img = b_img.float()\n",
    "\n",
    "                b_img = b_img.cuda()\n",
    "                b_seg_modified = b_seg_modified.cuda()\n",
    "                b_idxs_dataset = b_idxs_dataset.cuda()\n",
    "                b_seg = b_seg.cuda()\n",
    "                b_spat_aug_grid = b_spat_aug_grid.cuda()\n",
    "\n",
    "                if training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "                elif not training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 3D\n",
    "                    b_img = mindssc(b_img.unsqueeze(1))\n",
    "                else:\n",
    "                    b_img = b_img.unsqueeze(1)\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=True):\n",
    "                    assert b_img.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input image for model must be {len(n_dims)+2}D: BxCxSPATIAL but is {b_img.shape}\"\n",
    "                    for param in lraspp.parameters():\n",
    "                        param.requires_grad = True\n",
    "\n",
    "                    lraspp.use_checkpointing = True\n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    ### Calculate loss ###\n",
    "                    assert logits.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input shape for loss must be BxNUM_CLASSESxSPATIAL but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == len(n_dims)+1, \\\n",
    "                        f\"Target shape for loss must be BxSPATIAL but is {b_seg_modified.shape}\"\n",
    "\n",
    "                    ce_loss = nn.CrossEntropyLoss(class_weights)(logits, b_seg_modified)\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.DISABLED) or config.use_parallel_dp_loss:\n",
    "                        scaler.scale(ce_loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                        if config.use_parallel_dp_loss:\n",
    "                            # Run second consecutive forward pass\n",
    "                            for param in lraspp.parameters():\n",
    "                                param.requires_grad = False\n",
    "                            lraspp.use_checkpointing = False\n",
    "                            dp_logits = lraspp(b_img)['out']\n",
    "                        else:\n",
    "                            # Do not run a second forward pass\n",
    "                            for param in lraspp.parameters():\n",
    "                                param.requires_grad = True\n",
    "                            lraspp.use_checkpointing = True\n",
    "                            dp_logits = logits\n",
    "\n",
    "                        dp_loss = nn.CrossEntropyLoss(reduction='none')(dp_logits, b_seg_modified)\n",
    "                        dp_loss = dp_loss.mean(n_dims)\n",
    "\n",
    "                        bare_weight = embedding(b_idxs_dataset).squeeze()\n",
    "\n",
    "                        weight = torch.sigmoid(bare_weight)\n",
    "                        weight = weight/weight.mean()\n",
    "\n",
    "                        # This improves scores significantly: Reweight with log(gt_numel)\n",
    "                        weight = weight/t_metric[b_idxs_dataset]\n",
    "\n",
    "                        if config.use_risk_regularization:\n",
    "                            p_pred_num = (dp_logits.argmax(1) > 0).sum(dim=n_dims).detach()\n",
    "                            if config.use_2d_normal_to is not None:\n",
    "                                risk_regularization = -weight*p_pred_num/(dp_logits.shape[-2]*dp_logits.shape[-1])\n",
    "                            else:\n",
    "                                risk_regularization = -weight*p_pred_num/(dp_logits.shape[-3]*dp_logits.shape[-2]*dp_logits.shape[-1])\n",
    "\n",
    "                            dp_loss = (dp_loss*weight).sum() + risk_regularization.sum()\n",
    "                        else:\n",
    "                            dp_loss = (dp_loss*weight).sum()\n",
    "\n",
    "                    # elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                    #     dp_loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified)\n",
    "                    #     m_dp_idxs = map_embedding_idxs(b_idxs_dataset, config.grid_size_y, config.grid_size_x)\n",
    "                    #     weight = embedding(m_dp_idxs)\n",
    "                    #     weight = weight.reshape(-1, config.grid_size_y, config.grid_size_x)\n",
    "                    #     weight = weight.unsqueeze(1)\n",
    "                    #     weight = torch.nn.functional.interpolate(\n",
    "                    #         weight,\n",
    "                    #         size=(b_seg_modified.shape[-2:]),\n",
    "                    #         mode='bilinear',\n",
    "                    #         align_corners=True\n",
    "                    #     )\n",
    "                    #     weight = weight/weight.mean()\n",
    "                    #     weight = F.grid_sample(weight, b_spat_aug_grid,\n",
    "                    #         padding_mode='border', align_corners=False)\n",
    "                    #     dp_loss = (dp_loss.unsqueeze(1)*weight).sum()\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                    dp_scaler.scale(dp_loss).backward()\n",
    "\n",
    "                    if config.use_parallel_dp_loss:\n",
    "                        # LRASPP already stepped.\n",
    "                        if not config.override_embedding_weights:\n",
    "                            dp_scaler.step(optimizer_dp)\n",
    "                            dp_scaler.update()\n",
    "                    else:\n",
    "                        dp_scaler.step(optimizer)\n",
    "                        if not config.override_embedding_weights:\n",
    "                            dp_scaler.step(optimizer_dp)\n",
    "                        dp_scaler.update()\n",
    "\n",
    "                    epx_losses.append(dp_loss.item())\n",
    "                else:\n",
    "                    epx_losses.append(ce_loss.item())\n",
    "\n",
    "                logits_for_score = logits.argmax(1)\n",
    "\n",
    "                # Calculate dice score\n",
    "                b_dice = dice_func(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(b_seg, len(training_dataset.label_tags)), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_over_all(\n",
    "                    b_dice, exclude_bg=True))\n",
    "                class_dices.append(get_batch_dice_per_class(\n",
    "                    b_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                ###  Scheduler management ###\n",
    "                if config.use_scheduling and epx % NUM_REGISTRATIONS_PER_IMG == 0:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED) and batch_idx % 10 == 0 and config.save_dp_figures:\n",
    "                    # Output data parameter figure\n",
    "                    train_params = embedding.weight[train_idxs].squeeze()\n",
    "                    # order = np.argsort(train_params.cpu().detach()) # Order by DP value\n",
    "                    order = torch.arange(len(train_params))\n",
    "                    wise_corr_coeff = np.corrcoef(train_params.cpu().detach(), wise_dice[train_idxs][:,1].cpu().detach())[0,1]\n",
    "                    dp_figure_path = Path(f\"data/output_figures/{wandb.run.name}_fold{fold_idx}/dp_figure_epx{epx:03d}_batch{batch_idx:03d}.png\")\n",
    "                    dp_figure_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    save_parameter_figure(dp_figure_path, wandb.run.name, f\"corr. coeff. DP vs. dice(expert label, train gt): {wise_corr_coeff:4f}\",\n",
    "                        train_params[order], train_params[order]/t_metric[train_idxs][order], dices=wise_dice[train_idxs][:,1][order])\n",
    "\n",
    "                if config.debug:\n",
    "                    break\n",
    "\n",
    "            ### Logging ###\n",
    "            print(f\"### Log epoch {epx} @ {time.time()-t0:.2f}s\")\n",
    "            print(\"### Training\")\n",
    "            ### Log wandb data ###\n",
    "            # Log the epoch idx per fold - so we can recover the diagram by setting\n",
    "            # ref_epoch_idx as x-axis in wandb interface\n",
    "            wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "\n",
    "            mean_loss = torch.tensor(epx_losses).mean()\n",
    "            wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "\n",
    "            mean_dice = np.nanmean(dices)\n",
    "            print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "            wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "\n",
    "            log_class_dices(\"scores/dice_mean_\", f\"_fold{fold_idx}\", class_dices, global_idx)\n",
    "\n",
    "            # Log data parameters of disturbed samples\n",
    "            if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                # Calculate dice score corr coeff (unknown to network)\n",
    "                train_params = embedding.weight[train_idxs].squeeze()\n",
    "                order = np.argsort(train_params.cpu().detach())\n",
    "                wise_corr_coeff = np.corrcoef(train_params[order].cpu().detach(), wise_dice[train_idxs][:,1][order].cpu().detach())[0,1]\n",
    "                spearman_corr_coeff, spearman_p = scipy.stats.spearmanr(train_params[order].cpu().detach(), wise_dice[train_idxs][:,1][order].cpu().detach())\n",
    "\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/wise_corr_coeff_fold{fold_idx}': wise_corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/spearman_corr_coeff_fold{fold_idx}': spearman_corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/spearman_p_fold{fold_idx}': spearman_p},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                print(f'data_parameters/wise_corr_coeff_fold{fold_idx}', f\"{wise_corr_coeff:.2f}\")\n",
    "                print(f'data_parameters/spearman_corr_coeff_fold{fold_idx}', f\"{spearman_corr_coeff:.2f}\")\n",
    "                print(f'data_parameters/spearman_p_fold{fold_idx}', f\"{spearman_p:.5f}\")\n",
    "\n",
    "                # Log stats of data parameters and figure\n",
    "                log_data_parameter_stats(f'data_parameters/iter_stats_fold{fold_idx}', global_idx, embedding.weight.data)\n",
    "\n",
    "                # Map gridded instance parameters\n",
    "                if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                    m_tr_idxs = map_embedding_idxs(train_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                    ).cuda()\n",
    "                    masks = union_wise_mod_label[train_idxs].float()\n",
    "                    masked_weights = torch.nn.functional.interpolate(\n",
    "                        embedding(m_tr_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                        size=(masks.shape[-2:]),\n",
    "                        mode='bilinear',\n",
    "                        align_corners=True\n",
    "                    ).squeeze(1) * masks\n",
    "                    masked_weights[masked_weights==0.] = float('nan')\n",
    "\n",
    "            if (epx % config.save_every == 0 and epx != 0) \\\n",
    "                or (epx+1 == config.epochs):\n",
    "                _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx}\"\n",
    "                save_model(\n",
    "                    _path,\n",
    "                    lraspp=lraspp,\n",
    "                    optimizer=optimizer, optimizer_dp=optimizer_dp,\n",
    "                    scheduler=scheduler,\n",
    "                    # scheduler_dp=scheduler_dp,\n",
    "                    embedding=embedding,\n",
    "                    scaler=scaler)\n",
    "                (lraspp, optimizer, optimizer_dp, embedding, scaler) = \\\n",
    "                    get_model(\n",
    "                        config, len(training_dataset),\n",
    "                        len(training_dataset.label_tags),\n",
    "                        THIS_SCRIPT_DIR=THIS_SCRIPT_DIR,\n",
    "                        _path=_path, device='cuda')\n",
    "\n",
    "            print()\n",
    "            print(\"### Validation\")\n",
    "            lraspp.eval()\n",
    "            training_dataset.eval()\n",
    "\n",
    "            val_dices = []\n",
    "            val_class_dices = []\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    for val_idx in val_3d_idxs:\n",
    "                        val_sample = training_dataset.get_3d_item(val_idx)\n",
    "                        stack_dim = training_dataset.use_2d_normal_to\n",
    "                        # Create batch out of single val sample\n",
    "                        b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                        b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "\n",
    "                        B = b_val_img.shape[0]\n",
    "\n",
    "                        b_val_img = b_val_img.unsqueeze(1).float().cuda()\n",
    "                        b_val_seg = b_val_seg.cuda()\n",
    "\n",
    "                        if training_dataset.use_2d():\n",
    "                            b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=training_dataset.use_2d_normal_to)\n",
    "\n",
    "                            if config.use_mind:\n",
    "                                # MIND 2D model, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                                b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                            output_val = lraspp(b_val_img_2d)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "                            # Prepare logits for scoring\n",
    "                            # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                            val_logits_for_score = make_3d_from_2d_stack(\n",
    "                                val_logits_for_score.unsqueeze(1), stack_dim, B\n",
    "                            ).squeeze(1)\n",
    "\n",
    "                        else:\n",
    "                            if config.use_mind:\n",
    "                                # MIND 3D model shape BxMINDxDxHxW\n",
    "                                b_val_img = mindssc(b_val_img)\n",
    "                            else:\n",
    "                                # 3D model shape Bx1xDxHxW\n",
    "                                pass\n",
    "\n",
    "                            output_val = lraspp(b_val_img)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "\n",
    "                        b_val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(val_logits_for_score, len(training_dataset.label_tags)),\n",
    "                            torch.nn.functional.one_hot(b_val_seg, len(training_dataset.label_tags)),\n",
    "                            one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        # Get mean score over batch\n",
    "                        val_dices.append(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=True))\n",
    "\n",
    "                        val_class_dices.append(get_batch_dice_per_class(\n",
    "                            b_val_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                        if config.do_plot:\n",
    "                            print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                            print(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=False))\n",
    "                            # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                            display_seg(in_type=\"single_3D\",\n",
    "                                reduce_dim=\"W\",\n",
    "                                img=val_sample['image'].unsqueeze(0).cpu(),\n",
    "                                seg=val_logits_for_score_3d.squeeze(0).cpu(), # CHECK TODO\n",
    "                                ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                crop_to_non_zero_seg=True,\n",
    "                                crop_to_non_zero_gt=True,\n",
    "                                alpha_seg=.3,\n",
    "                                alpha_gt=.0\n",
    "                            )\n",
    "\n",
    "                    mean_val_dice = np.nanmean(val_dices)\n",
    "                    print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                    wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                    log_class_dices(\"scores/val_dice_mean_\", f\"_fold{fold_idx}\", val_class_dices, global_idx)\n",
    "\n",
    "            print()\n",
    "            # End of training loop\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "            # Write sample data\n",
    "\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            all_idxs = torch.tensor(range(len(training_dataset))).cuda()\n",
    "            train_label_snapshot_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/train_label_snapshot.pth\")\n",
    "            seg_viz_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weighted_samples.png\")\n",
    "\n",
    "            train_label_snapshot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if str(config.data_param_mode) == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                dp_weights = embedding(all_idxs)\n",
    "                save_data = []\n",
    "                data_generator = zip(\n",
    "                    dp_weights[train_idxs], \\\n",
    "                    disturbed_bool_vect[train_idxs],\n",
    "                    torch.utils.data.Subset(training_dataset, train_idxs)\n",
    "                )\n",
    "\n",
    "                for dp_weight, disturb_flg, sample in data_generator:\n",
    "                    data_tuple = ( \\\n",
    "                        dp_weight,\n",
    "                        bool(disturb_flg.item()),\n",
    "                        sample['id'],\n",
    "                        sample['dataset_idx'],\n",
    "                        # sample['image'],\n",
    "                        sample['label'].to_sparse(),\n",
    "                        sample['modified_label'].to_sparse(),\n",
    "                        inference_wrap(lraspp, sample['image'].cuda(), use_2d=training_dataset.use_2d(), use_mind=config.use_mind).to_sparse()\n",
    "                    )\n",
    "                    save_data.append(data_tuple)\n",
    "\n",
    "                save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "                (dp_weight, disturb_flags,\n",
    "                 d_ids, dataset_idxs,\n",
    "                #  _imgs,\n",
    "                 _labels, _modified_labels, _predictions) = zip(*save_data)\n",
    "\n",
    "                dp_weight = torch.stack(dp_weight)\n",
    "                dataset_idxs = torch.stack(dataset_idxs)\n",
    "                # _imgs = torch.stack(_imgs)\n",
    "                _labels = torch.stack(_labels)\n",
    "                _modified_labels = torch.stack(_modified_labels)\n",
    "                _predictions = torch.stack(_predictions)\n",
    "\n",
    "                print(\"Saving data parameters.\")\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'data_parameters': dp_weight.cpu(),\n",
    "                        'disturb_flags': disturb_flags,\n",
    "                        'd_ids': d_ids,\n",
    "                        'dataset_idxs': dataset_idxs.cpu(),\n",
    "                        'labels': _labels.cpu(),\n",
    "                        'modified_labels': _modified_labels.cpu(),\n",
    "                        'train_predictions': _predictions.cpu(),\n",
    "                    },\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "            elif str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                # Log histogram of clean and disturbed parameters\n",
    "                if not training_dataset.use_2d():\n",
    "                    raise NotImplementedError(\"Script does not support 3D model and GRIDDED_INSTANCE_PARAMS yet.\")\n",
    "\n",
    "                m_all_idxs = map_embedding_idxs(all_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                ).cuda()\n",
    "                masks = union_wise_mod_label[all_idxs].float()\n",
    "                all_weights = torch.nn.functional.interpolate(\n",
    "                    embedding(m_all_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                    size=(masks.shape[-2:]),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=True\n",
    "                ).squeeze(1)\n",
    "\n",
    "                masked_weights = all_weights * masks\n",
    "                masked_weights[masked_weights==0.] = float('nan')\n",
    "                dp_weights = np.nanmean(masked_weights.detach().cpu(), axis=n_dims)\n",
    "\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weightmap.png\")\n",
    "\n",
    "                save_data = []\n",
    "                data_generator = zip(\n",
    "                    dp_weights[train_idxs],\n",
    "                    all_weights[train_idxs],\n",
    "                    disturbed_bool_vect[train_idxs],\n",
    "                    torch.utils.data.Subset(training_dataset,train_idxs)\n",
    "                )\n",
    "\n",
    "                for dp_weight, weightmap, disturb_flg, sample in data_generator:\n",
    "                    data_tuple = (\n",
    "                        dp_weight,\n",
    "                        weightmap,\n",
    "                        bool(disturb_flg.item()),\n",
    "                        sample['id'],\n",
    "                        sample['dataset_idx'],\n",
    "                        sample['image'],\n",
    "                        sample['label'],\n",
    "                        sample['modified_label']\n",
    "                    )\n",
    "                    save_data.append(data_tuple)\n",
    "\n",
    "                save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "\n",
    "                (dp_weight, dp_weightmap, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _2d_imgs,\n",
    "                 _2d_labels, _2d_modified_labels) = zip(*save_data)\n",
    "\n",
    "                dp_weight = torch.stack(dp_weight)\n",
    "                dp_weightmap = torch.stack(dp_weightmap)\n",
    "                dataset_idxs = torch.stack(dataset_idxs)\n",
    "                _2d_imgs = torch.stack(_2d_imgs)\n",
    "                _2d_labels = torch.stack(_2d_labels)\n",
    "                _2d_modified_labels = torch.stack(_2d_modified_labels)\n",
    "                _2d_predictions = torch.stack(_2d_predictions)\n",
    "\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'data_parameters': dp_weight.cpu(),\n",
    "                        'data_parameter_weightmaps': dp_weightmap.cpu(),\n",
    "                        'disturb_flags': disturb_flags,\n",
    "                        'd_ids': d_ids,\n",
    "                        'dataset_idxs': dataset_idxs.cpu(),\n",
    "                        'labels': _2d_labels.cpu().to_sparse(),\n",
    "                        'modified_labels': _2d_modified_labels.cpu().to_sparse(),\n",
    "                        'train_predictions': _2d_predictions.cpu().to_sparse(),\n",
    "                    },\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "                print(\"Writing weight map image.\")\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}_data_parameter_weightmap.png\")\n",
    "                visualize_seg(in_type=\"batch_2D\",\n",
    "                    img=torch.stack(dp_weightmap).unsqueeze(1),\n",
    "                    seg=torch.stack(_2d_modified_labels),\n",
    "                    alpha_seg = 0.,\n",
    "                    n_per_row=70,\n",
    "                    overlay_text=overlay_text_list,\n",
    "                    annotate_color=(0,255,255),\n",
    "                    frame_elements=disturb_flags,\n",
    "                    file_path=weightmap_out_path,\n",
    "                )\n",
    "\n",
    "            if len(training_dataset.disturbed_idxs) > 0:\n",
    "                # Log histogram\n",
    "                separated_params = list(zip(dp_weights[clean_idxs], dp_weights[training_dataset.disturbed_idxs]))\n",
    "                s_table = wandb.Table(columns=['clean_idxs', 'disturbed_idxs'], data=separated_params)\n",
    "                fields = {\"primary_bins\" : \"clean_idxs\", \"secondary_bins\" : \"disturbed_idxs\", \"title\" : \"Data parameter composite histogram\"}\n",
    "                composite_histogram = wandb.plot_table(vega_spec_name=\"rap1ide/composite_histogram\", data_table=s_table, fields=fields)\n",
    "                wandb.log({f\"data_parameters/separated_params_fold_{fold_idx}\": composite_histogram})\n",
    "\n",
    "            # Write out data of modified and un-modified labels and an overview image\n",
    "\n",
    "            if training_dataset.use_2d():\n",
    "                reduce_dim = None\n",
    "                in_type = \"batch_2D\"\n",
    "                skip_writeout = len(training_dataset) > 3000\n",
    "            else:\n",
    "                reduce_dim = \"W\"\n",
    "                in_type = \"batch_3D\"\n",
    "                skip_writeout = len(training_dataset) > 150\n",
    "            skip_writeout = True\n",
    "            if not skip_writeout:\n",
    "                print(\"Writing train sample image.\")\n",
    "                # overlay text example: d_idx=0, dp_i=1.00, dist? False\n",
    "                overlay_text_list = [f\"id:{d_id} dp:{instance_p.item():.2f}\" \\\n",
    "                    for d_id, instance_p, disturb_flg in zip(d_ids, dp_weight, disturb_flags)]\n",
    "\n",
    "                use_2d = training_dataset.use_2d()\n",
    "                scf = 1/training_dataset.pre_interpolation_factor\n",
    "\n",
    "                show_img = interpolate_sample(b_label=_labels.to_dense(), scale_factor=scf, use_2d=use_2d)[1].unsqueeze(1)\n",
    "                show_seg = interpolate_sample(b_label=_predictions.to_dense().squeeze(1), scale_factor=scf, use_2d=use_2d)[1]\n",
    "                show_gt = interpolate_sample(b_label=_modified_labels.to_dense(), scale_factor=scf, use_2d=use_2d)[1]\n",
    "\n",
    "                visualize_seg(in_type=in_type, reduce_dim=reduce_dim,\n",
    "                    img=show_img, # Expert label in BW\n",
    "                    seg=4*show_seg, # Prediction in blue\n",
    "                    ground_truth=show_gt, # Modified label in red\n",
    "                    crop_to_non_zero_seg=False,\n",
    "                    alpha_seg = .5,\n",
    "                    alpha_gt = .5,\n",
    "                    n_per_row=70,\n",
    "                    overlay_text=overlay_text_list,\n",
    "                    annotate_color=(0,255,255),\n",
    "                    frame_elements=disturb_flags,\n",
    "                    file_path=seg_viz_out_path,\n",
    "                )\n",
    "\n",
    "        # End of fold loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "450fc80a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Config overrides\n",
    "# config_dict['wandb_mode'] = 'disabled'\n",
    "# config_dict['debug'] = True\n",
    "# Model loading\n",
    "# config_dict['checkpoint_name'] = 'ethereal-serenity-1138'\n",
    "# config_dict['fold_override'] = 0\n",
    "# config_dict['checkpoint_epx'] = 39\n",
    "\n",
    "# Define sweep override dict\n",
    "sweep_config_dict = dict(\n",
    "    method='grid',\n",
    "    metric=dict(goal='maximize', name='scores/val_dice_mean_tumour_fold0'),\n",
    "    parameters=dict(\n",
    "        # disturbance_mode=dict(\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        # disturbance_strength=dict(\n",
    "        #     values=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "        # ),\n",
    "        # disturbed_percentage=dict(\n",
    "        #     values=[0.3, 0.6]\n",
    "        # ),\n",
    "        # data_param_mode=dict(\n",
    "        #     values=[\n",
    "        #         DataParamMode.INSTANCE_PARAMS,\n",
    "        #         DataParamMode.DISABLED,\n",
    "        #     ]\n",
    "        # ),\n",
    "        # use_risk_regularization=dict(\n",
    "        #     values=[False, True]\n",
    "        # ),\n",
    "        # fixed_weight_min_quantile=dict(\n",
    "        #     values=[0.9, 0.8, 0.6, 0.4, 0.2, 0.0]\n",
    "        # ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549637cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_run():\n",
    "    with wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        ) as run:\n",
    "\n",
    "        run_name = run.name\n",
    "        print(\"Running\", run_name)\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init() as run:\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "\n",
    "        run_name = run.name\n",
    "        print(\"Running\", run_name)\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "\n",
    "if config_dict['do_sweep']:\n",
    "    # Integrate all config_dict entries into sweep_dict.parameters -> sweep overrides config_dict\n",
    "    cp_config_dict = copy.deepcopy(config_dict)\n",
    "    # cp_config_dict.update(copy.deepcopy(sweep_config_dict['parameters']))\n",
    "    for del_key in sweep_config_dict['parameters'].keys():\n",
    "        if del_key in cp_config_dict:\n",
    "            del cp_config_dict[del_key]\n",
    "    merged_sweep_config_dict = copy.deepcopy(sweep_config_dict)\n",
    "    # merged_sweep_config_dict.update(cp_config_dict)\n",
    "    for key, value in cp_config_dict.items():\n",
    "        merged_sweep_config_dict['parameters'][key] = dict(value=value)\n",
    "    # Convert enum values in parameters to string. They will be identified by their numerical index otherwise\n",
    "    for key, param_dict in merged_sweep_config_dict['parameters'].items():\n",
    "        if 'value' in param_dict and isinstance(param_dict['value'], Enum):\n",
    "            param_dict['value'] = str(param_dict['value'])\n",
    "        if 'values' in param_dict:\n",
    "            param_dict['values'] = [str(elem) if isinstance(elem, Enum) else elem for elem in param_dict['values']]\n",
    "\n",
    "        merged_sweep_config_dict['parameters'][key] = param_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(merged_sweep_config_dict, project=\"curriculum_deeplab\")\n",
    "    wandb.agent(sweep_id, function=sweep_run)\n",
    "\n",
    "else:\n",
    "    normal_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2244b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading registered data.\n",
      "Loading CrossMoDa hrT2 images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60 images, 60 labels: 100%|██████████| 120/120 [00:10<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding label data with modified_3d_label_override from 60 to 1800 labels\n",
      "Postprocessing 3D volumes\n",
      "Removed 0 3D images in postprocessing\n",
      "Equal image and label numbers: True (1800)\n",
      "Image shape: torch.Size([1800, 128, 128, 50]), mean.: 0.00, std.: 1.00\n",
      "Label shape: torch.Size([1800, 128, 128, 50]), max.: 1\n",
      "Data import finished.\n",
      "CrossMoDa loader will yield 3D samples\n"
     ]
    }
   ],
   "source": [
    "if not in_notebook():\n",
    "    sys.exit(0)\n",
    "\n",
    "d_set = prepare_data(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a04ac42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run validation with these 3D samples (#0): []\n"
     ]
    }
   ],
   "source": [
    "config = config_dict\n",
    "training_dataset = d_set\n",
    "all_3d_ids = training_dataset.get_3d_ids()\n",
    "if config.debug:\n",
    "    NUM_VAL_IMAGES = 2\n",
    "    NUM_REGISTRATIONS_PER_IMG = 1\n",
    "else:\n",
    "    NUM_VAL_IMAGES = 00\n",
    "    NUM_REGISTRATIONS_PER_IMG = 30 #TODO automate\n",
    "\n",
    "if config.use_2d_normal_to is not None:\n",
    "    # Override idxs\n",
    "    all_3d_ids = training_dataset.get_3d_ids()\n",
    "\n",
    "    val_3d_idxs = torch.tensor(list(range(0, NUM_VAL_IMAGES*NUM_REGISTRATIONS_PER_IMG, NUM_REGISTRATIONS_PER_IMG)))\n",
    "    val_3d_ids = training_dataset.switch_3d_identifiers(val_3d_idxs)\n",
    "\n",
    "    train_3d_idxs = list(range(NUM_VAL_IMAGES*NUM_REGISTRATIONS_PER_IMG, len(all_3d_ids)))\n",
    "\n",
    "    # Get corresponding 2D idxs\n",
    "    train_2d_ids = []\n",
    "    dcts = training_dataset.get_id_dicts()\n",
    "    for id_dict in dcts:\n",
    "        _2d_id = id_dict['2d_id']\n",
    "        _3d_idx = id_dict['3d_dataset_idx']\n",
    "        if _2d_id in training_dataset.label_data_2d.keys() and _3d_idx in train_3d_idxs:\n",
    "            train_2d_ids.append(_2d_id)\n",
    "\n",
    "    train_2d_idxs = training_dataset.switch_2d_identifiers(train_2d_ids)\n",
    "    train_idxs = torch.tensor(train_2d_idxs)\n",
    "\n",
    "else:\n",
    "    val_3d_idxs = torch.tensor(list(range(0, NUM_VAL_IMAGES*NUM_REGISTRATIONS_PER_IMG, NUM_REGISTRATIONS_PER_IMG)))\n",
    "    val_3d_ids = training_dataset.switch_3d_identifiers(val_3d_idxs)\n",
    "\n",
    "    train_3d_idxs = list(range(NUM_VAL_IMAGES*NUM_REGISTRATIONS_PER_IMG, len(all_3d_ids)))\n",
    "    train_idxs = torch.tensor(train_3d_idxs)\n",
    "\n",
    "print(f\"Will run validation with these 3D samples (#{len(val_3d_ids)}):\", sorted(val_3d_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99f77cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 60 60\n"
     ]
    }
   ],
   "source": [
    "train_3d_ids = training_dataset.switch_3d_identifiers(train_3d_idxs)\n",
    "val_label_paths = set([training_dataset.img_paths[_id] for _id in val_3d_ids])\n",
    "val_image_paths = set([training_dataset.img_paths[_id] for _id in val_3d_ids])\n",
    "train_label_paths = set([training_dataset.label_paths[_id] for _id in train_3d_ids])\n",
    "train_image_paths = set([training_dataset.img_paths[_id] for _id in train_3d_ids])\n",
    "\n",
    "\n",
    "path_dict = {}\n",
    "path_dict['val_label_paths'] = list(val_label_paths)\n",
    "path_dict['val_image_paths'] = list(val_image_paths)\n",
    "path_dict['train_label_paths'] = list(train_label_paths)\n",
    "path_dict['train_image_paths'] = list(train_image_paths)\n",
    "print(len(val_label_paths), len(val_image_paths), len(train_label_paths), len(train_image_paths))\n",
    "torch.save(path_dict, f'network_dataset_path_dict_train_{len(train_labels)}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2bb163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
