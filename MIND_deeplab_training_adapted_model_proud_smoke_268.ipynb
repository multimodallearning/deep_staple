{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3175666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name                     Util    Mem free  Cuda             User(s)\n",
      "----  --------------------------  ------  ----------  ---------------  ---------\n",
      "   1  NVIDIA GeForce RTX 2080 Ti     0 %   11018 MiB  11.5(495.29.05)\n",
      "   2  NVIDIA GeForce RTX 2080 Ti     0 %   11018 MiB  11.5(495.29.05)\n",
      "   3  NVIDIA GeForce RTX 2080 Ti     0 %   11018 MiB  11.5(495.29.05)\n",
      "   0  NVIDIA GeForce RTX 2080 Ti     0 %    4565 MiB  11.5(495.29.05)  weihsbach\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   1  NVIDIA GeForce RTX 2080 Ti  ->  cuda:0\n",
      "1.9.0a0+gitdfbd030\n",
      "8204\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -4\"))\n",
    "import pickle\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import KFold\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import visualize_seg\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de18b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in: /share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab\n"
     ]
    }
   ],
   "source": [
    "def in_notebook():\n",
    "    try:\n",
    "        get_ipython().__class__.__name__\n",
    "        return True\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "if in_notebook:\n",
    "    THIS_SCRIPT_DIR = os.path.abspath('')\n",
    "else:\n",
    "    THIS_SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "print(f\"Running in: {THIS_SCRIPT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fbc5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sample(b_image=None, b_label=None, scale_factor=1.,\n",
    "                       yield_2d=False):\n",
    "    if yield_2d:\n",
    "        scale = [scale_factor]*2\n",
    "        im_mode = 'bilinear'\n",
    "    else:\n",
    "        scale = [scale_factor]*3\n",
    "        im_mode = 'trilinear'\n",
    "\n",
    "    if b_image is not None:\n",
    "        b_image = F.interpolate(\n",
    "            b_image.unsqueeze(1), scale_factor=scale, mode=im_mode, align_corners=True,\n",
    "            recompute_scale_factor=False\n",
    "        )\n",
    "        b_image = b_image.squeeze(1)\n",
    "\n",
    "    if b_label is not None:\n",
    "        b_label = F.interpolate(\n",
    "            b_label.unsqueeze(1).float(), scale_factor=scale, mode='nearest',\n",
    "            recompute_scale_factor=False\n",
    "        ).long()\n",
    "        b_label = b_label.squeeze(1)\n",
    "\n",
    "    return b_image, b_label\n",
    "\n",
    "\n",
    "\n",
    "def dilate_label_class(b_label, class_max_idx, class_dilate_idx,\n",
    "                       yield_2d, kernel_sz=3):\n",
    "\n",
    "    if kernel_sz < 2:\n",
    "        return b_label\n",
    "\n",
    "    b_dilated_label = b_label\n",
    "\n",
    "    b_onehot = torch.nn.functional.one_hot(b_label.long(), class_max_idx+1)\n",
    "    class_slice = b_onehot[...,class_dilate_idx]\n",
    "\n",
    "    if yield_2d:\n",
    "        B, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz]).long()\n",
    "        kernel = kernel.view(1,1,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv2d(\n",
    "            class_slice.view(B,1,H,W), kernel, padding='same')\n",
    "\n",
    "    else:\n",
    "        B, D, H, W = class_slice.shape\n",
    "        kernel = torch.ones([kernel_sz,kernel_sz,kernel_sz])\n",
    "        kernel = kernel.long().view(1,1,kernel_sz,kernel_sz,kernel_sz)\n",
    "        class_slice = torch.nn.functional.conv3d(\n",
    "            class_slice.view(B,1,D,H,W), kernel, padding='same')\n",
    "\n",
    "    dilated_class_slice = torch.clamp(class_slice.squeeze(0), 0, 1)\n",
    "    b_dilated_label[dilated_class_slice.bool()] = class_dilate_idx\n",
    "\n",
    "    return b_dilated_label\n",
    "\n",
    "\n",
    "def get_batch_dice_per_class(b_dice, class_tags, exclude_bg=True) -> dict:\n",
    "    score_dict = {}\n",
    "    for cls_idx, cls_tag in enumerate(class_tags):\n",
    "        if exclude_bg and cls_idx == 0:\n",
    "            continue\n",
    "\n",
    "        if torch.all(torch.isnan(b_dice[:,cls_idx])):\n",
    "            score = float('nan')\n",
    "        else:\n",
    "            score = np.nanmean(b_dice[:,cls_idx]).item()\n",
    "\n",
    "        score_dict[cls_tag] = score\n",
    "\n",
    "    return score_dict\n",
    "\n",
    "def get_batch_dice_over_all(b_dice, exclude_bg=True) -> float:\n",
    "\n",
    "    start_idx = 1 if exclude_bg else 0\n",
    "    if torch.all(torch.isnan(b_dice[:,start_idx:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,start_idx:]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0adbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_augment(b_image=None, b_label=None,\n",
    "    bspline_num_ctl_points=6, bspline_strength=0.005, bspline_probability=.9,\n",
    "    affine_strength=0.08, add_affine_translation=0., affine_probability=.45,\n",
    "    pre_interpolation_factor=None,\n",
    "    yield_2d=False,\n",
    "    b_grid_override=None):\n",
    "\n",
    "    \"\"\"\n",
    "    2D/3D b-spline augmentation on image and segmentation mini-batch on GPU.\n",
    "    :input: b_image batch (torch.cuda.FloatTensor), b_label batch (torch.cuda.LongTensor)\n",
    "    :return: augmented Bx(D)xHxW image batch (torch.cuda.FloatTensor),\n",
    "    augmented Bx(D)xHxW seg batch (torch.cuda.LongTensor)\n",
    "    \"\"\"\n",
    "\n",
    "    do_bspline = (np.random.rand() < bspline_probability)\n",
    "    do_affine = (np.random.rand() < affine_probability)\n",
    "\n",
    "    if pre_interpolation_factor:\n",
    "        b_image, b_label = interpolate_sample(b_image, b_label, pre_interpolation_factor, yield_2d)\n",
    "\n",
    "    KERNEL_SIZE = 3\n",
    "\n",
    "    if b_image is None:\n",
    "        common_shape = b_label.shape\n",
    "        common_device = b_label.device\n",
    "\n",
    "    elif b_label is None:\n",
    "        common_shape = b_image.shape\n",
    "        common_device = b_image.device\n",
    "    else:\n",
    "        assert b_image.shape == b_label.shape, \\\n",
    "            f\"Image and label shapes must match but are {b_image.shape} and {b_label.shape}.\"\n",
    "        common_shape = b_image.shape\n",
    "        common_device = b_image.device\n",
    "\n",
    "    if b_grid_override is None:\n",
    "        if yield_2d:\n",
    "            assert len(common_shape) == 3, \\\n",
    "                f\"Augmenting 2D. Input batch \" \\\n",
    "                f\"should be BxHxW but is {common_shape}.\"\n",
    "            B,H,W = common_shape\n",
    "\n",
    "            identity = torch.eye(2,3).expand(B,2,3).to(common_device)\n",
    "            id_grid = F.affine_grid(identity, torch.Size((B,2,H,W)),\n",
    "                align_corners=False)\n",
    "\n",
    "            grid = id_grid\n",
    "\n",
    "            if do_bspline:\n",
    "                bspline = torch.nn.Sequential(\n",
    "                    nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "                    nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "                    nn.AvgPool2d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2))\n",
    "                ).to(common_device)\n",
    "                # Add an extra *.5 factor to dim strength to make strength fit 3D case\n",
    "                dim_strength = (torch.tensor([H,W]).float()*bspline_strength*.5).to(common_device)\n",
    "                rand_control_points = dim_strength.view(1,2,1,1) * \\\n",
    "                    (\n",
    "                        torch.randn(B, 2, bspline_num_ctl_points, bspline_num_ctl_points)\n",
    "                    ).to(common_device)\n",
    "\n",
    "                bspline_disp = bspline(rand_control_points)\n",
    "                bspline_disp = torch.nn.functional.interpolate(\n",
    "                    bspline_disp, size=(H,W), mode='bilinear', align_corners=True\n",
    "                ).permute(0,2,3,1)\n",
    "\n",
    "                grid += bspline_disp\n",
    "\n",
    "            if do_affine:\n",
    "                affine_matrix = (torch.eye(2,3).unsqueeze(0) + \\\n",
    "                    affine_strength * torch.randn(B,2,3)).to(common_device)\n",
    "                # Add additional x,y offset\n",
    "                alpha = np.random.rand() * 2 * np.pi\n",
    "                offset_dir =  torch.tensor([np.cos(alpha), np.sin(alpha)])\n",
    "                affine_matrix[:,:,-1] = add_affine_translation * offset_dir\n",
    "                affine_disp = F.affine_grid(affine_matrix, torch.Size((B,1,H,W)),\n",
    "                                        align_corners=False)\n",
    "                grid += (affine_disp-id_grid)\n",
    "\n",
    "        else:\n",
    "            assert len(common_shape) == 4, \\\n",
    "                f\"Augmenting 3D. Input batch \" \\\n",
    "                f\"should be BxDxHxW but is {common_shape}.\"\n",
    "            B,D,H,W = common_shape\n",
    "\n",
    "            identity = torch.eye(3,4).expand(B,3,4).to(common_device)\n",
    "            id_grid = F.affine_grid(identity, torch.Size((B,3,D,H,W)),\n",
    "                align_corners=False)\n",
    "\n",
    "            grid = id_grid\n",
    "\n",
    "            if do_bspline:\n",
    "                bspline = torch.nn.Sequential(\n",
    "                    nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "                    nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2)),\n",
    "                    nn.AvgPool3d(KERNEL_SIZE,stride=1,padding=int(KERNEL_SIZE//2))\n",
    "                ).to(b_image.device)\n",
    "                dim_strength = (torch.tensor([D,H,W]).float()*bspline_strength).to(common_device)\n",
    "\n",
    "                rand_control_points = dim_strength.view(1,3,1,1,1)  * \\\n",
    "                    (\n",
    "                        torch.randn(B, 3, bspline_num_ctl_points, bspline_num_ctl_points, bspline_num_ctl_points)\n",
    "                    ).to(b_image.device)\n",
    "\n",
    "                bspline_disp = bspline(rand_control_points)\n",
    "\n",
    "                bspline_disp = torch.nn.functional.interpolate(\n",
    "                    bspline_disp, size=(D,H,W), mode='trilinear', align_corners=True\n",
    "                ).permute(0,2,3,4,1)\n",
    "\n",
    "                grid += bspline_disp\n",
    "\n",
    "            if do_affine:\n",
    "                affine_matrix = (torch.eye(3,4).unsqueeze(0) + \\\n",
    "                    affine_strength * torch.randn(B,3,4)).to(common_device)\n",
    "\n",
    "                # Add additional x,y,z offset\n",
    "                theta = np.random.rand() * 2 * np.pi\n",
    "                phi = np.random.rand() * 2 * np.pi\n",
    "                offset_dir =  torch.tensor([\n",
    "                    np.cos(phi)*np.sin(theta),\n",
    "                    np.sin(phi)*np.sin(theta),\n",
    "                    np.cos(theta)])\n",
    "                affine_matrix[:,:,-1] = add_affine_translation * offset_dir\n",
    "\n",
    "                affine_disp = F.affine_grid(affine_matrix,torch.Size((B,1,D,H,W)),\n",
    "                                        align_corners=False)\n",
    "\n",
    "                grid += (affine_disp-id_grid)\n",
    "    else:\n",
    "        # Override grid with external value\n",
    "        grid = b_grid_override\n",
    "\n",
    "    if b_image is not None:\n",
    "        b_image_out = F.grid_sample(\n",
    "            b_image.unsqueeze(1).float(), grid,\n",
    "            padding_mode='border', align_corners=False)\n",
    "        b_image_out = b_image_out.squeeze(1)\n",
    "    else:\n",
    "        b_image_out = None\n",
    "\n",
    "    if b_label is not None:\n",
    "        b_label_out = F.grid_sample(\n",
    "            b_label.unsqueeze(1).float(), grid,\n",
    "            mode='nearest', align_corners=False)\n",
    "        b_label_out = b_label_out.squeeze(1).long()\n",
    "    else:\n",
    "        b_label_out = None\n",
    "\n",
    "    b_out_grid = grid\n",
    "\n",
    "    if pre_interpolation_factor and False:\n",
    "        b_image_out, b_label_out = interpolate_sample(\n",
    "            b_image_out, b_label_out,\n",
    "            1/pre_interpolation_factor, yield_2d\n",
    "        )\n",
    "\n",
    "        b_out_grid = F.interpolate(b_out_grid.permute(0,3,1,2),\n",
    "            scale_factor=1/pre_interpolation_factor, mode='bilinear',\n",
    "            align_corners=True, recompute_scale_factor=False).permute(0,2,3,1)\n",
    "\n",
    "    return b_image_out, b_label_out, b_out_grid\n",
    "\n",
    "\n",
    "\n",
    "def augmentNoise(b_image, strength=0.05):\n",
    "    return b_image + strength*torch.randn_like(b_image)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def torch_manual_seeded(seed):\n",
    "    saved_state = torch.get_rng_state()\n",
    "    yield\n",
    "    torch.set_rng_state(saved_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d52956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossMoDa_Data(Dataset):\n",
    "    def __init__(self,\n",
    "        base_dir, domain, state,\n",
    "        ensure_labeled_pairs=True, use_additional_data=False, resample=True,\n",
    "        size:tuple=(96,96,60), normalize:bool=True,\n",
    "        max_load_num=None, crop_3d_w_dim_range=None, crop_2d_slices_gt_num_threshold=None,\n",
    "        modified_3d_label_override=None, prevent_disturbance=False,\n",
    "        yield_2d_normal_to=None, flip_r_samples=True,\n",
    "        debug=False\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Function to create Dataset structure with crossMoDa data.\n",
    "        The function allows to use different preproccessing steps of the crossMoDa data set\n",
    "        and using additinal data from TCIA database.\n",
    "        The data can also be resampled to a desired size and normalized to mean=0 and std=1.\n",
    "\n",
    "        Parameters:\n",
    "                base_dir (os.Pathlike): provide the directory which contains \"L1...\" to \"L4...\" directories\n",
    "                domain (str): choose which domain to load. Can be set to \"source\", \"target\" or \"validation\". Source are ceT1, target and validation hrT2 images.\n",
    "\n",
    "                state (str): state of preprocessing:    \"l1\" = original data,\n",
    "                                                        \"l2\" = resampled data @ 0.5mm,\n",
    "                                                        \"l3\" = center-cropped data,\n",
    "                                                        \"l4\" = image specific crops for desired anatomy\n",
    "\n",
    "                ensure_labeled_pairs (bool): Only images with corresponding labels will be loaded (default: True)\n",
    "\n",
    "                use_additional_data (bool): set to True to use additional data from TCIA (default: False)\n",
    "\n",
    "                resample (bool): set to False to disable resampling to desired size (default: True)\n",
    "\n",
    "                size (tuple): 3d-tuple(int) to which the data is resampled. Unused if resample=False. (default: (96,96,60)).\n",
    "                    WARNING: choosing large sizes or not resampling can lead to excess memory usage\n",
    "\n",
    "                normalize (bool): set to False to disable normalization to mean=0, std=1 for each image (default: True)\n",
    "                max_load_num (int): maximum number of pairs to load (uses first max_load_num samples for either images and labels found)\n",
    "                crop_3d_w_dim_range (tuple): Tuple of ints defining the range to which dimension W of (D,H,W) is cropped\n",
    "                yield_2d_normal_to (bool):\n",
    "\n",
    "        Returns:\n",
    "                torch.utils.data.Dataset containing CrossMoDa data\n",
    "\n",
    "        Useful Links:\n",
    "        CrossMoDa challenge:\n",
    "        https://crossmoda.grand-challenge.org/\n",
    "\n",
    "        ToDos:\n",
    "            extend to other preprocessing states\n",
    "\n",
    "        Example:\n",
    "            dataset = CrossMoDa_source('original')\n",
    "\n",
    "            data = dataset.get_data()\n",
    "\n",
    "        \"\"\"\n",
    "        self.label_tags = ['background', 'tumour']\n",
    "        self.yield_2d_normal_to = yield_2d_normal_to\n",
    "        self.crop_2d_slices_gt_num_threshold = crop_2d_slices_gt_num_threshold\n",
    "        self.prevent_disturbance = prevent_disturbance\n",
    "        self.do_augment = False\n",
    "        self.use_modified = False\n",
    "        self.disturbed_idxs = []\n",
    "        self.augment_at_collate = False\n",
    "\n",
    "        #define finished preprocessing states here with subpath and default size\n",
    "        states = {\n",
    "            'l1':('L1_original/', (512,512,160)),\n",
    "            'l2':('L2_resampled_05mm/', (420,420,360)),\n",
    "            'l3':('L3_coarse_fixed_crop/', (128,128,192)),\n",
    "            'l4':('L4_fine_localized_crop/', (128,128,128))\n",
    "        }\n",
    "        t0 = time.time()\n",
    "        #choose directory with data according to chosen preprocessing state\n",
    "        if state not in states: raise Exception(\"Unknown state. Choose one of: \"+str(states.keys))\n",
    "\n",
    "        state_dir = states[state.lower()][0] #get sub directory\n",
    "\n",
    "        if not resample: size = states[state.lower()][1] #set size to default defined at top of file\n",
    "\n",
    "        path = base_dir + state_dir\n",
    "\n",
    "        #get file list\n",
    "        if domain.lower() ==\"ceT1\" or domain.lower() ==\"source\":\n",
    "            directory = \"source_training_labeled/\"\n",
    "            add_directory = \"__additional_data_source_domain__\"\n",
    "            domain = \"ceT1\"\n",
    "\n",
    "        elif domain.lower() ==\"hrT2\" or domain.lower() ==\"target\":\n",
    "            directory = \"target_training_unlabeled/\"\n",
    "            add_directory = \"__additional_data_target_domain__\"\n",
    "            domain = \"hrT2\"\n",
    "\n",
    "        elif domain.lower() ==\"validation\":\n",
    "            directory = \"target_validation_unlabeled/\"\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Unknown domain. Choose either 'source', 'target' or 'validation'\")\n",
    "        path = \"/share/data_supergrover1/hansen/temp/crossMoDa/preprocessed_new/resampled/localised_crop/target_training/\"\n",
    "        directory = \"\"\n",
    "        files = sorted(glob.glob(os.path.join(path+directory , \"*.nii.gz\")))\n",
    "\n",
    "        if domain == \"hrT2\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_training__\" , \"*.nii.gz\")))\n",
    "\n",
    "        if domain.lower() == \"validation\":\n",
    "            files = files+sorted(glob.glob(os.path.join(path+\"__omitted_labels_target_validation__\" , \"*.nii.gz\")))\n",
    "\n",
    "        if use_additional_data and domain.lower() != \"validation\": #add additional data to file list\n",
    "            files = files+sorted(glob.glob(os.path.join(path+add_directory , \"*.nii.gz\")))\n",
    "            files = [i for i in files if \"additionalLabel\" not in i] #remove additional label files\n",
    "\n",
    "\n",
    "        # First read filepaths\n",
    "        self.img_paths = {}\n",
    "        self.label_paths = {}\n",
    "\n",
    "        if debug:\n",
    "            files = files[:4]\n",
    "\n",
    "\n",
    "        for _path in files:\n",
    "\n",
    "            numeric_id = int(re.findall(r'\\d+', os.path.basename(_path))[0])\n",
    "            if \"_l.nii.gz\" in _path or \"_l_Label.nii.gz\" in _path:\n",
    "                lr_id = 'l'\n",
    "            elif \"_r.nii.gz\" in _path or \"_r_Label.nii.gz\" in _path:\n",
    "                lr_id = 'r'\n",
    "            else:\n",
    "                lr_id = \"\"\n",
    "\n",
    "            # Generate crossmoda id like 004r\n",
    "            crossmoda_id = f\"{numeric_id:03d}{lr_id}\"\n",
    "\n",
    "            # Skip file if we do not have modified labels for it when external labels were provided\n",
    "            if modified_3d_label_override:\n",
    "                if crossmoda_id not in modified_3d_label_override.keys():\n",
    "                    continue\n",
    "\n",
    "            if \"Label\" in _path:\n",
    "                self.label_paths[crossmoda_id] = _path\n",
    "\n",
    "            elif domain in _path:\n",
    "                self.img_paths[crossmoda_id] = _path\n",
    "\n",
    "        if ensure_labeled_pairs:\n",
    "            pair_idxs = set(self.img_paths).intersection(set(self.label_paths))\n",
    "            self.label_paths = {_id: _path for _id, _path in self.label_paths.items() if _id in pair_idxs}\n",
    "            self.img_paths = {_id: _path for _id, _path in self.img_paths.items() if _id in pair_idxs}\n",
    "\n",
    "\n",
    "        # Populate data\n",
    "        self.img_data_3d = {}\n",
    "        self.label_data_3d = {}\n",
    "        self.modified_label_data_3d = {}\n",
    "\n",
    "        self.img_data_2d = {}\n",
    "        self.label_data_2d = {}\n",
    "        self.modified_label_data_2d = {}\n",
    "\n",
    "        #load data\n",
    "\n",
    "        print(\"Loading CrossMoDa {} images and labels...\".format(domain))\n",
    "        id_paths_to_load = list(self.label_paths.items()) + list(self.img_paths.items())\n",
    "\n",
    "        description = f\"{len(self.img_paths)} images, {len(self.label_paths)} labels\"\n",
    "\n",
    "        for _3d_id, _file in tqdm(id_paths_to_load, desc=description):\n",
    "            # tqdm.write(f\"Loading {f}\")\n",
    "            if \"Label\" in _file:\n",
    "                tmp = torch.from_numpy(nib.load(_file).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "\n",
    "                if crop_3d_w_dim_range:\n",
    "                    tmp = tmp[..., crop_3d_w_dim_range[0]:crop_3d_w_dim_range[1]]\n",
    "\n",
    "                # Only use tumour class, remove TODO\n",
    "                tmp[tmp==2] = 0\n",
    "                self.label_data_3d[_3d_id] = tmp.long()\n",
    "\n",
    "            elif domain in _file:\n",
    "                tmp = torch.from_numpy(nib.load(_file).get_fdata())\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='trilinear',align_corners=False).squeeze()\n",
    "\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "\n",
    "                if crop_3d_w_dim_range:\n",
    "                    tmp = tmp[..., crop_3d_w_dim_range[0]:crop_3d_w_dim_range[1]]\n",
    "\n",
    "                if normalize: #normalize image to zero mean and unit std\n",
    "                    tmp = (tmp - tmp.mean()) / tmp.std()\n",
    "\n",
    "                self.img_data_3d[_3d_id] = tmp\n",
    "\n",
    "        # Initialize 3d modified labels as unmodified labels\n",
    "        for label_id in self.label_data_3d.keys():\n",
    "            self.modified_label_data_3d[label_id] = self.label_data_3d[label_id]\n",
    "\n",
    "        # Now inject externally overriden labels (if any)\n",
    "        if modified_3d_label_override:\n",
    "            for _3d_id, modified_label in modified_3d_label_override.items():\n",
    "                tmp = modified_label\n",
    "                if resample: #resample image to specified size\n",
    "                    tmp = F.interpolate(tmp.unsqueeze(0).unsqueeze(0), size=size,mode='nearest').squeeze()\n",
    "\n",
    "                if tmp.shape != size: #for size missmatch use symmetric padding with 0\n",
    "                    difs = [size[0]-tmp.size(0),size[1]-tmp.size(1),size[2]-tmp.size(2)]\n",
    "                    pad = (difs[-1]//2,difs[-1]-difs[-1]//2,difs[-2]//2,difs[-2]-difs[-2]//2,difs[-3]//2,difs[-3]-difs[-3]//2)\n",
    "                    tmp = F.pad(tmp,pad)\n",
    "\n",
    "                if crop_3d_w_dim_range:\n",
    "                    tmp = tmp[..., crop_3d_w_dim_range[0]:crop_3d_w_dim_range[1]]\n",
    "\n",
    "                # Only use tumour class, remove TODO\n",
    "                tmp[tmp==2] = 0\n",
    "                self.modified_label_data_3d[_3d_id] = tmp.long()\n",
    "\n",
    "        # Postprocessing of 3d volumes\n",
    "        for _3d_id in list(self.label_data_3d.keys()):\n",
    "            if self.label_data_3d[_3d_id].unique().numel() != 2: #TODO use 3 classes again\n",
    "                del self.img_data_3d[_3d_id]\n",
    "                del self.label_data_3d[_3d_id]\n",
    "                del self.modified_label_data_3d[_3d_id]\n",
    "            elif \"r\" in _3d_id:\n",
    "                self.img_data_3d[_3d_id] = self.img_data_3d[_3d_id].flip(dims=(1,))\n",
    "                self.label_data_3d[_3d_id] = self.label_data_3d[_3d_id].flip(dims=(1,))\n",
    "                self.modified_label_data_3d[_3d_id] = self.modified_label_data_3d[_3d_id].flip(dims=(1,))\n",
    "\n",
    "        if ensure_labeled_pairs:\n",
    "            labelled_keys = set(self.label_data_3d.keys())\n",
    "            unlabelled_imgs = set(self.img_data_3d.keys()) - labelled_keys\n",
    "            unlabelled_modified_labels = set(self.modified_label_data_3d.keys()) - labelled_keys\n",
    "\n",
    "            for del_key in unlabelled_imgs:\n",
    "                del self.img_data_3d[del_key]\n",
    "            for del_key in unlabelled_modified_labels:\n",
    "                del self.modified_label_data_3d[del_key]\n",
    "\n",
    "        if max_load_num:\n",
    "            for del_key in sorted(list(self.img_data_3d.keys()))[max_load_num:]:\n",
    "                del self.img_data_3d[del_key]\n",
    "            for del_key in sorted(list(self.label_data_3d.keys()))[max_load_num:]:\n",
    "                del self.label_data_3d[del_key]\n",
    "            for del_key in sorted(list(self.modified_label_data_3d.keys()))[max_load_num:]:\n",
    "                del self.modified_label_data_3d[del_key]\n",
    "\n",
    "        #check for consistency\n",
    "        print(f\"Equal image and label numbers: {set(self.img_data_3d)==set(self.label_data_3d)==set(self.modified_label_data_3d)} ({len(self.img_data_3d)})\")\n",
    "\n",
    "        img_stack = torch.stack(list(self.img_data_3d.values()), dim=0)\n",
    "        img_mean, img_std = img_stack.mean(), img_stack.std()\n",
    "\n",
    "        label_stack = torch.stack(list(self.label_data_3d.values()), dim=0)\n",
    "\n",
    "        print(\"Image shape: {}, mean.: {:.2f}, std.: {:.2f}\".format(img_stack.shape, img_mean, img_std))\n",
    "        print(\"Label shape: {}, max.: {}\".format(label_stack.shape,torch.max(label_stack)))\n",
    "\n",
    "        if yield_2d_normal_to:\n",
    "            if yield_2d_normal_to == \"D\":\n",
    "                slice_dim = -3\n",
    "            if yield_2d_normal_to == \"H\":\n",
    "                slice_dim = -2\n",
    "            if yield_2d_normal_to == \"W\":\n",
    "                slice_dim = -1\n",
    "\n",
    "            for _3d_id, image in self.img_data_3d.items():\n",
    "                for idx, img_slc in [(slice_idx, image.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(image.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.img_data_2d[f\"{_3d_id}{yield_2d_normal_to}{idx:03d}\"] = img_slc\n",
    "\n",
    "            for _3d_id, label in self.label_data_3d.items():\n",
    "                for idx, lbl_slc in [(slice_idx, label.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(label.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.label_data_2d[f\"{_3d_id}{yield_2d_normal_to}{idx:03d}\"] = lbl_slc\n",
    "\n",
    "            for _3d_id, label in self.modified_label_data_3d.items():\n",
    "                for idx, lbl_slc in [(slice_idx, label.select(slice_dim, slice_idx)) \\\n",
    "                                     for slice_idx in range(label.shape[slice_dim])]:\n",
    "                    # Set data view for crossmoda id like \"003rW100\"\n",
    "                    self.modified_label_data_2d[f\"{_3d_id}{yield_2d_normal_to}{idx:03d}\"] = lbl_slc\n",
    "\n",
    "        # Postprocessing of 2d slices\n",
    "\n",
    "        for key, label in list(self.label_data_2d.items()):\n",
    "            uniq_vals = label.unique()\n",
    "\n",
    "            if uniq_vals.max() == 0 and False:\n",
    "                # Delete empty 2D slices (but keep 3d data)\n",
    "                del self.img_data_2d[key]\n",
    "                del self.label_data_2d[key]\n",
    "                del self.modified_label_data_2d[key]\n",
    "\n",
    "            elif sum(label[label > 0]) < self.crop_2d_slices_gt_num_threshold:\n",
    "                # Delete 2D slices with less than n gt-pixels (but keep 3d data)\n",
    "                del self.img_data_2d[key]\n",
    "                del self.label_data_2d[key]\n",
    "                del self.modified_label_data_2d[key]\n",
    "\n",
    "        print(\"Data import finished.\")\n",
    "        print(f\"CrossMoDa loader will yield {'2D' if self.yield_2d_normal_to else '3D'} samples\")\n",
    "\n",
    "    def get_3d_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data_3d.keys())\n",
    "            .union(set(self.label_data_3d.keys()))\n",
    "        ))\n",
    "\n",
    "    def get_2d_ids(self):\n",
    "        return sorted(list(\n",
    "            set(self.img_data_2d.keys())\n",
    "            .union(set(self.label_data_2d.keys()))\n",
    "        ))\n",
    "\n",
    "    def get_id_dicts(self):\n",
    "\n",
    "        all_3d_ids = self.get_3d_ids()\n",
    "        id_dicts = []\n",
    "\n",
    "        for _2d_dataset_idx, _2d_id in enumerate(self.get_2d_ids()):\n",
    "            _3d_id = _2d_id[:-4]\n",
    "            id_dicts.append(\n",
    "                {\n",
    "                    '2d_id': _2d_id,\n",
    "                    '2d_dataset_idx': _2d_dataset_idx,\n",
    "                    '3d_id': _3d_id,\n",
    "                    '3d_dataset_idx': all_3d_ids.index(_3d_id),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return id_dicts\n",
    "\n",
    "    def __len__(self, yield_2d_override=None):\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data length\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "\n",
    "        if yield_2d:\n",
    "            return len(self.img_data_2d)\n",
    "\n",
    "        return len(self.img_data_3d)\n",
    "\n",
    "    def __getitem__(self, dataset_idx, yield_2d_override=None):\n",
    "\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data can be overridden\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "\n",
    "        if yield_2d:\n",
    "            all_ids = self.get_2d_ids()\n",
    "            _id = all_ids[dataset_idx]\n",
    "            image = self.img_data_2d.get(_id, torch.tensor([]))\n",
    "            label = self.label_data_2d.get(_id, torch.tensor([]))\n",
    "\n",
    "            # For 2D crossmoda id cut last 4 \"003rW100\"\n",
    "            image_path = self.img_paths[_id[:-4]]\n",
    "            label_path = self.label_paths[_id[:-4]]\n",
    "\n",
    "        else:\n",
    "            all_ids = self.get_3d_ids()\n",
    "            _id = all_ids[dataset_idx]\n",
    "            image = self.img_data_3d.get(_id, torch.tensor([]))\n",
    "            label = self.label_data_3d.get(_id, torch.tensor([]))\n",
    "\n",
    "            image_path = self.img_paths[_id]\n",
    "            label_path = self.label_paths[_id]\n",
    "\n",
    "        spat_augment_grid = []\n",
    "\n",
    "        if self.use_modified:\n",
    "            if yield_2d:\n",
    "                modified_label = self.modified_label_data_2d.get(_id, label.detach().clone())\n",
    "            else:\n",
    "                modified_label = self.modified_label_data_3d.get(_id, label.detach().clone())\n",
    "        else:\n",
    "            modified_label = label.detach().clone()\n",
    "\n",
    "        if self.do_augment and not self.augment_at_collate:\n",
    "            b_image = image.unsqueeze(0).cuda()\n",
    "            b_label = label.unsqueeze(0).cuda()\n",
    "            b_modified_label = modified_label.unsqueeze(0).cuda()\n",
    "\n",
    "            b_image, b_label, b_spat_augment_grid = self.augment(\n",
    "                b_image, b_label, yield_2d\n",
    "            )\n",
    "            _, b_modified_label, _ = spatial_augment(\n",
    "                b_label=b_modified_label, yield_2d=yield_2d, b_grid_override=b_spat_augment_grid\n",
    "            )\n",
    "\n",
    "            image = b_image.squeeze(0).cpu()\n",
    "            label = b_label.squeeze(0).cpu()\n",
    "            modified_label = b_modified_label.squeeze(0).cpu()\n",
    "            spat_augment_grid = b_spat_augment_grid.squeeze(0).detach().cpu().clone()\n",
    "\n",
    "        if yield_2d:\n",
    "            assert image.dim() == label.dim() == 2\n",
    "        else:\n",
    "            assert image.dim() == label.dim() == 3\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'modified_label': modified_label,\n",
    "            # if disturbance is off, modified label is equals label\n",
    "            'dataset_idx': dataset_idx,\n",
    "            'id': _id,\n",
    "            'image_path': image_path,\n",
    "            'label_path': label_path,\n",
    "            'spat_augment_grid': spat_augment_grid\n",
    "        }\n",
    "\n",
    "    def get_3d_item(self, _3d_dataset_idx):\n",
    "        return self.__getitem__(_3d_dataset_idx, yield_2d_override=False)\n",
    "\n",
    "    def get_data(self, yield_2d_override=None):\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data can be overridden\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "\n",
    "        if yield_2d:\n",
    "            img_stack = torch.stack(list(self.img_data_2d.values()), dim=0)\n",
    "            label_stack = torch.stack(list(self.label_data_2d.values()), dim=0)\n",
    "        else:\n",
    "            img_stack = torch.stack(list(self.img_data_3d.values()), dim=0)\n",
    "            label_stack = torch.stack(list(self.label_data_3d.values()), dim=0)\n",
    "\n",
    "        return img_stack, label_stack\n",
    "\n",
    "    def disturb_idxs(self, all_idxs, disturbance_mode, disturbance_strength=1., yield_2d_override=None):\n",
    "        if self.prevent_disturbance:\n",
    "            warnings.warn(\"Disturbed idxs shall be set but disturbance is prevented for dataset.\")\n",
    "            return\n",
    "\n",
    "        if yield_2d_override == None:\n",
    "            # Here getting 2D or 3D data can be overridden\n",
    "            yield_2d = True if self.yield_2d_normal_to else False\n",
    "        else:\n",
    "            yield_2d = yield_2d_override\n",
    "\n",
    "        if all_idxs is not None:\n",
    "            if isinstance(all_idxs, (np.ndarray, torch.Tensor)):\n",
    "                all_idxs = all_idxs.tolist()\n",
    "\n",
    "            self.disturbed_idxs = all_idxs\n",
    "        else:\n",
    "            self.disturbed_idxs = []\n",
    "\n",
    "        # Reset modified data\n",
    "        for idx in range(self.__len__(yield_2d_override=yield_2d)):\n",
    "            if yield_2d:\n",
    "                label_id = self.get_2d_ids()[idx]\n",
    "                self.modified_label_data_2d[label_id] = self.label_data_2d[label_id]\n",
    "            else:\n",
    "                label_id = self.get_3d_ids()[idx]\n",
    "                self.modified_label_data_3d[label_id] = self.label_data_3d[label_id]\n",
    "\n",
    "            # Now apply disturbance\n",
    "            if idx in self.disturbed_idxs:\n",
    "                label = self.modified_label_data_2d[label_id].detach().clone()\n",
    "\n",
    "                with torch_manual_seeded(idx):\n",
    "                    if str(disturbance_mode)==str(LabelDisturbanceMode.FLIP_ROLL):\n",
    "                        roll_strength = 10*disturbance_strength\n",
    "                        if yield_2d:\n",
    "                            modified_label = \\\n",
    "                                torch.roll(\n",
    "                                    label.transpose(-2,-1),\n",
    "                                    (\n",
    "                                        int(torch.randn(1)*roll_strength),\n",
    "                                        int(torch.randn(1)*roll_strength)\n",
    "                                    ),(-2,-1)\n",
    "                                )\n",
    "                        else:\n",
    "                            modified_label = \\\n",
    "                                torch.roll(\n",
    "                                    label.permute(1,2,0),\n",
    "                                    (\n",
    "                                        int(torch.randn(1)*roll_strength),\n",
    "                                        int(torch.randn(1)*roll_strength),\n",
    "                                        int(torch.randn(1)*roll_strength)\n",
    "                                    ),(-3,-2,-1)\n",
    "                                )\n",
    "\n",
    "                    elif str(disturbance_mode)==str(LabelDisturbanceMode.AFFINE):\n",
    "                        b_modified_label = label.unsqueeze(0).cuda()\n",
    "                        _, b_modified_label, _ = spatial_augment(b_label=b_modified_label, yield_2d=yield_2d,\n",
    "                            bspline_num_ctl_points=6, bspline_strength=0., bspline_probability=0.,\n",
    "                            affine_strength=0.09*disturbance_strength,\n",
    "                            add_affine_translation=0.18*disturbance_strength, affine_probability=1.)\n",
    "                        modified_label = b_modified_label.squeeze(0).cpu()\n",
    "\n",
    "                    else:\n",
    "                        raise ValueError(f\"Disturbance mode {disturbance_mode} is not implemented.\")\n",
    "\n",
    "                    if yield_2d:\n",
    "                        self.modified_label_data_2d[label_id] = modified_label\n",
    "                    else:\n",
    "                        self.modified_label_data_2d[label_id] = modified_label\n",
    "\n",
    "\n",
    "    def train(self, augment=True, use_modified=True):\n",
    "        self.do_augment = augment\n",
    "        self.use_modified = use_modified\n",
    "\n",
    "    def eval(self, augment=False, use_modified=False):\n",
    "        self.train(augment, use_modified)\n",
    "\n",
    "    def set_augment_at_collate(self, augment_at_collate=True):\n",
    "        self.augment_at_collate = augment_at_collate\n",
    "\n",
    "    def get_efficient_augmentation_collate_fn(self):\n",
    "        yield_2d = True if self.yield_2d_normal_to else False\n",
    "\n",
    "        def collate_closure(batch):\n",
    "            batch = torch.utils.data._utils.collate.default_collate(batch)\n",
    "            if self.augment_at_collate:\n",
    "                # Augment the whole batch not just one sample\n",
    "                b_image = batch['image'].cuda()\n",
    "                b_label = batch['label'].cuda()\n",
    "                b_image, b_label = self.augment(b_image, b_label, yield_2d)\n",
    "                batch['image'], batch['label'] = b_image.cpu(), b_label.cpu()\n",
    "\n",
    "            return batch\n",
    "\n",
    "        return collate_closure\n",
    "\n",
    "    def augment(self, b_image, b_label, yield_2d,\n",
    "        noise_strength=0.05,\n",
    "        bspline_num_ctl_points=6, bspline_strength=0.004, bspline_probability=.95,\n",
    "        affine_strength=0.07, affine_probability=.45,\n",
    "        pre_interpolation_factor=2.):\n",
    "\n",
    "        if yield_2d:\n",
    "            assert b_image.dim() == b_label.dim() == 3, \\\n",
    "                f\"Augmenting 2D. Input batch of image and \" \\\n",
    "                f\"label should be BxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "        else:\n",
    "            assert b_image.dim() == b_label.dim() == 4, \\\n",
    "                f\"Augmenting 3D. Input batch of image and \" \\\n",
    "                f\"label should be BxDxHxW but are {b_image.shape} and {b_label.shape}\"\n",
    "\n",
    "        b_image = augmentNoise(b_image, strength=noise_strength)\n",
    "        b_image, b_label, b_spat_augment_grid = spatial_augment(\n",
    "            b_image, b_label,\n",
    "            bspline_num_ctl_points=bspline_num_ctl_points, bspline_strength=bspline_strength, bspline_probability=bspline_probability,\n",
    "            affine_strength=affine_strength, affine_probability=affine_probability,\n",
    "            pre_interpolation_factor=2., yield_2d=yield_2d)\n",
    "\n",
    "        b_label = b_label.long()\n",
    "\n",
    "        return b_image, b_label, b_spat_augment_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "335f4df3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "from enum import Enum, auto\n",
    "class LabelDisturbanceMode(Enum):\n",
    "    FLIP_ROLL = auto()\n",
    "    AFFINE = auto()\n",
    "\n",
    "class DataParamMode(Enum):\n",
    "    INSTANCE_PARAMS = auto()\n",
    "    GRIDDED_INSTANCE_PARAMS = auto()\n",
    "    DISABLED = auto()\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "        See https://stackoverflow.com/questions/49901590/python-using-copy-deepcopy-on-dotdict\n",
    "    \"\"\"\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        try:\n",
    "            return self[item]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError from e\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "\n",
    "    'use_mind': False,\n",
    "    'epochs': 80,\n",
    "\n",
    "    'batch_size': 32,\n",
    "    'val_batch_size': 1,\n",
    "\n",
    "    'dataset': 'crossmoda',\n",
    "    'reg_state': None,\n",
    "    'train_set_max_len': None,\n",
    "    'crop_3d_w_dim_range': (35, 105),\n",
    "    'crop_2d_slices_gt_num_threshold': 0,\n",
    "    'yield_2d_normal_to': \"W\",\n",
    "\n",
    "    'lr': 0.001,\n",
    "    'use_cosine_annealing': True,\n",
    "\n",
    "    # Data parameter config\n",
    "    'data_param_mode': DataParamMode.DISABLED,\n",
    "        # init_class_param=0.01,\n",
    "        # lr_class_param=0.1,\n",
    "    'init_inst_param': 1.0,\n",
    "    'lr_inst_param': 0.1,\n",
    "    # 'wd_inst_param': 0.01,\n",
    "        # wd_class_param=0.0,\n",
    "        # skip_clamp_data_param=False,\n",
    "    # 'clamp_sigma_min': 0.,\n",
    "    # 'clamp_sigma_max': 1.,\n",
    "        # optim_algorithm=DataParamOptim.ADAM,\n",
    "        # optim_options=dict(\n",
    "        #     betas=(0.9, 0.999)\n",
    "        # )\n",
    "    'grid_size_y': 64,\n",
    "    'grid_size_x': 64,\n",
    "    # ),\n",
    "\n",
    "    'save_every': 200,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'do_plot': False,\n",
    "    'debug': False,\n",
    "    'wandb_mode': \"disabled\",\n",
    "    'checkpoint_name': None,\n",
    "    'do_sweep': False,\n",
    "\n",
    "    'disturbance_mode': None,\n",
    "    'disturbance_strength': 0.,\n",
    "    'disturbed_percentage': .0,\n",
    "    'start_disturbing_after_ep': 0,\n",
    "\n",
    "    'start_dilate_kernel_sz': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0eec4ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CrossMoDa ceT1 images and labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209 images, 209 labels: 100%|█████████████████| 418/418 [00:45<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal image and label numbers: True\n",
      "Image shape: torch.Size([100, 128, 128, 86]), mean.: 0.00, std.: 1.00\n",
      "Label shape: torch.Size([100, 128, 128, 86]), max.: 1\n",
      "Data import finished.\n",
      "CrossMoDa loader will yield 2D samples\n",
      "Nonzero slice ratio:  1.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(config):\n",
    "    if config.reg_state:\n",
    "        REG_STATES = [\"combined\", \"best\"]\n",
    "        if config.reg_state in REG_STATES:\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(f\"Unknown registration version. Choose one of {REG_STATES}\")\n",
    "\n",
    "        label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "        label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "\n",
    "        loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "        label_data = torch.cat([label_data_left[config.reg_state+'_all'][:44], label_data_right[config.reg_state+'_all'][:63]], dim=0)\n",
    "\n",
    "        modified_3d_label_override = {}\n",
    "        for idx, identifier in enumerate(loaded_identifier):\n",
    "            nl_id = int(re.findall(r'\\d+', identifier)[0])\n",
    "            lr_id = identifier[-8]\n",
    "            crossmoda_id = f\"{nl_id:03d}{lr_id}\"\n",
    "            loaded_identifier[idx] = crossmoda_id\n",
    "            modified_3d_label_override[crossmoda_id] = label_data[idx]\n",
    "\n",
    "        prevent_disturbance = True\n",
    "\n",
    "    else:\n",
    "        modified_3d_label_override = None\n",
    "        prevent_disturbance = False\n",
    "\n",
    "    if config.dataset == 'crossmoda':\n",
    "        training_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "            domain=\"target\", state=\"l4\", size=(128, 128, 128),\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_num=config['train_set_max_len'],\n",
    "            crop_3d_w_dim_range=config['crop_3d_w_dim_range'], crop_2d_slices_gt_num_threshold=config['crop_2d_slices_gt_num_threshold'],\n",
    "            yield_2d_normal_to=config['yield_2d_normal_to'],\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            debug=config['debug'],\n",
    "            # inject_data_path = '/share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/healing_polished-river.xpth'\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(\"Nonzero slice ratio: \",\n",
    "            sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)\n",
    "        )\n",
    "        # validation_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossMoDa_Data(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "    elif config['dataset'] == 'organmnist3d':\n",
    "        training_dataset = WrapperOrganMNIST3D(\n",
    "            split='train', root='./data/medmnist', download=True, normalize=True,\n",
    "            max_load_num=300, crop_3d_w_dim_range=None,\n",
    "            disturbed_idxs=None, yield_2d_normal_to='W'\n",
    "        )\n",
    "        print(training_dataset.mnist_set.info)\n",
    "        print(\"Classes: \", training_dataset.label_tags)\n",
    "        print(\"Samples: \", len(training_dataset))\n",
    "\n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ad43e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128, 128, 86])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    _, all_labels = training_dataset.get_data(yield_2d_override=False)\n",
    "    print(all_labels.shape)\n",
    "    sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "    plt.xlabel(\"W\")\n",
    "    plt.ylabel(\"ground truth>0\")\n",
    "    plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdc76c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    # Print bare 2D data\n",
    "    # print(\"Displaying 2D bare sample\")\n",
    "    # for img, label in zip(training_dataset.img_data_2d.values(),\n",
    "    #                       training_dataset.label_data_2d.values()):\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=img.unsqueeze(0),\n",
    "    #                 ground_truth=label,\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "    # Print transformed 2D data\n",
    "    training_dataset.train(use_modified=False, augment=True)\n",
    "    print(training_dataset.disturbed_idxs)\n",
    "\n",
    "    print(\"Displaying 2D training sample\")\n",
    "    for dist_stren in [0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 5.0]:\n",
    "        print(dist_stren)\n",
    "        training_dataset.disturb_idxs(list(range(0,20,2)),\n",
    "            disturbance_mode=LabelDisturbanceMode.AFFINE,\n",
    "            disturbance_strength=dist_stren\n",
    "        )\n",
    "        img_stack = []\n",
    "        label_stack = []\n",
    "        mod_label_stack = []\n",
    "\n",
    "        for sample in (training_dataset[idx] for idx in range(20)):\n",
    "            img_stack.append(sample['image'])\n",
    "            label_stack.append(sample['label'])\n",
    "            mod_label_stack.append(sample['modified_label'])\n",
    "\n",
    "        # Change label num == hue shift for display\n",
    "        img_stack = torch.stack(img_stack).unsqueeze(1)\n",
    "        label_stack = torch.stack(label_stack)\n",
    "        mod_label_stack = torch.stack(mod_label_stack)\n",
    "\n",
    "        mod_label_stack*=4\n",
    "\n",
    "        visualize_seg(in_type=\"batch_2D\",\n",
    "            img=img_stack,\n",
    "            # ground_truth=label_stack,\n",
    "            seg=(mod_label_stack-label_stack).abs(),\n",
    "            # crop_to_non_zero_gt=True,\n",
    "            crop_to_non_zero_seg=True,\n",
    "            alpha_seg = .6,\n",
    "            file_path=f'out{dist_stren}.png'\n",
    "        )\n",
    "\n",
    "    # Print transformed 3D data\n",
    "    # training_dataset.train()\n",
    "    # print(\"Displaying 3D training sample\")\n",
    "    # leng = 1# training_dataset.__len__(yield_2d_override=False)\n",
    "    # for sample in (training_dataset.get_3d_item(idx) for idx in range(leng)):\n",
    "    #     # training_dataset.set_dilate_kernel_size(1)\n",
    "    #     visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "    #                 img=sample['image'].unsqueeze(0),\n",
    "    #                 ground_truth=sample['label'],\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "#         # training_dataset.set_dilate_kernel_size(7)\n",
    "#         display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "#                     img=sample['image'].unsqueeze(0),\n",
    "#                     ground_truth=sample['modified_label'],\n",
    "#                     crop_to_non_zero_gt=True,\n",
    "#                     alpha_gt = .3)\n",
    "\n",
    "    for sidx in [0,]:\n",
    "        print(f\"Sample {sidx}:\")\n",
    "\n",
    "        training_dataset.eval()\n",
    "        sample_eval = training_dataset.get_3d_item(sidx)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "\n",
    "        training_dataset.train()\n",
    "        print(\"Train sample with ground-truth overlay\")\n",
    "        sample_train = training_dataset.get_3d_item(sidx)\n",
    "        print(sample_train['label'].unique())\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_train['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_train['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt=.3)\n",
    "\n",
    "        print(\"Eval/train diff with diff overlay\")\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=(sample_eval['image'] - sample_train['image']).unsqueeze(0),\n",
    "                    ground_truth=(sample_eval['label'] - sample_train['label']).clamp(min=0),\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "    train_plotset = (training_dataset.get_3d_item(idx) for idx in (55, 81, 63))\n",
    "    for sample in train_plotset:\n",
    "        print(f\"Sample {sample['dataset_idx']}:\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .6)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1efa2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "\n",
    "import functools\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use get_named_layers_leaves(module) to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4950d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(_path, **statefuls):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "    _path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for name, stful in statefuls.items():\n",
    "        if stful != None:\n",
    "            torch.save(stful.state_dict(), _path.joinpath(name+'.pth'))\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config, dataset_len, num_classes, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(\n",
    "        pretrained=False, progress=True, num_classes=num_classes\n",
    "    )\n",
    "    set_module(lraspp, 'backbone.0.0',\n",
    "        torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2),\n",
    "                        padding=(1, 1), bias=False)\n",
    "    )\n",
    "    # set_module(lraspp, 'classifier.scale.2',\n",
    "    #     torch.nn.Identity()\n",
    "    # )\n",
    "    lraspp.to(device)\n",
    "    print(f\"Param count lraspp: {sum(p.numel() for p in lraspp.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(lraspp.parameters(), lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # Add data paramters embedding and optimizer\n",
    "    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len, 1, sparse=True).to(device)\n",
    "\n",
    "    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len*config.grid_size_y*config.grid_size_x, 1, sparse=True).to(device)\n",
    "    else:\n",
    "        embedding = None\n",
    "\n",
    "\n",
    "    if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "        optimizer_dp = torch.optim.SparseAdam(\n",
    "            embedding.parameters(), lr=config.lr_inst_param,\n",
    "            betas=(0.9, 0.999), eps=1e-08)\n",
    "        torch.nn.init.normal_(embedding.weight.data, mean=config.init_inst_param, std=0.01)\n",
    "\n",
    "        if _path and _path.is_dir():\n",
    "            print(f\"Loading embedding and dp_optimizer from {_path}\")\n",
    "            optimizer_dp.load_state_dict(torch.load(_path.joinpath('optimizer_dp.pth'), map_location=device))\n",
    "            embedding.load_state_dict(torch.load(_path.joinpath('embedding.pth'), map_location=device))\n",
    "\n",
    "        print(f\"Param count embedding: {sum(p.numel() for p in embedding.parameters())}\")\n",
    "\n",
    "    else:\n",
    "        optimizer_dp = None\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading lr-aspp model, optimizers and grad scalers from {_path}\")\n",
    "        lraspp.load_state_dict(torch.load(_path.joinpath('lraspp.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "\n",
    "    return (lraspp, optimizer, optimizer_dp, embedding, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "490f1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "\n",
    "\n",
    "\n",
    "def log_data_parameters(log_path, parameter_idxs, parameters):\n",
    "    data = [[idx, param] for (idx, param) in \\\n",
    "        zip(parameter_idxs, torch.exp(parameters).tolist())]\n",
    "\n",
    "    table = wandb.Table(data=data, columns = [\"parameter_idx\", \"value\"])\n",
    "    wandb.log({log_path:wandb.plot.bar(table, \"parameter_idx\", \"value\", title=log_path)})\n",
    "\n",
    "\n",
    "\n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "\n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "\n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "\n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "\n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio\n",
    "\n",
    "def log_data_parameter_stats(log_path, epx, data_parameters):\n",
    "    \"\"\"Log stats for data parameters on wandb.\"\"\"\n",
    "    data_parameters = data_parameters.exp()\n",
    "    wandb.log({f'{log_path}/highest': torch.max(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/lowest': torch.min(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/mean': torch.mean(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/std': torch.std(data_parameters).item()}, step=epx)\n",
    "\n",
    "\n",
    "\n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_class_dices(log_prefix, log_postfix, class_dices, log_idx):\n",
    "    if not class_dices:\n",
    "        return\n",
    "\n",
    "    for cls_name in class_dices[0].keys():\n",
    "        log_path = f\"{log_prefix}{cls_name}{log_postfix}\"\n",
    "\n",
    "        cls_dices = list(map(lambda dct: dct[cls_name], class_dices))\n",
    "        mean_per_class =np.nanmean(cls_dices)\n",
    "        print(log_path, f\"{mean_per_class*100:.2f}%\")\n",
    "        wandb.log({log_path: mean_per_class}, step=log_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "779c09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embedding_idxs(idxs, grid_size_y, grid_size_x):\n",
    "    with torch.no_grad():\n",
    "        t_sz = grid_size_y * grid_size_x\n",
    "        return ((idxs*t_sz).long().repeat(t_sz).view(t_sz, idxs.numel())+torch.tensor(range(t_sz)).to(idxs).view(t_sz,1)).permute(1,0).reshape(-1)\n",
    "\n",
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "\n",
    "    fold_iter = enumerate(kf.split(training_dataset))\n",
    "\n",
    "    if config.get('fold_override', None):\n",
    "        selected_fold = config.get('fold_override', 0)\n",
    "        fold_iter = list(fold_iter)[selected_fold:selected_fold+1]\n",
    "    elif config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "\n",
    "    if config.wandb_mode != 'disabled':\n",
    "        # Log dataset info\n",
    "        training_dataset.eval()\n",
    "        dataset_info = [[smp['dataset_idx'], smp['id'], smp['image_path'], smp['label_path']] \\\n",
    "                        for smp in training_dataset]\n",
    "        wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', 'id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    "\n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        SET_LEN = training_dataset.__len__(yield_2d_override=False)\n",
    "        SHAPE_LEN = training_dataset.__getitem__(0, yield_2d_override=False)['image'].shape[-1]\n",
    "        train_idxs = list(range(SHAPE_LEN*(SET_LEN-20)))\n",
    "        val_idxs = list(range(SHAPE_LEN*(SET_LEN-20), SET_LEN*SHAPE_LEN))\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        # Training happens in 2D, validation happens in 3D:\n",
    "        # Read 2D dataset idxs which are used for training,\n",
    "        # get their 3D super-ids by 3d dataset length\n",
    "        # and substract these from all 3D ids to get val_3d_idxs\n",
    "        trained_3d_dataset_idxs = {dct['3d_dataset_idx'] \\\n",
    "             for dct in training_dataset.get_id_dicts() if dct['2d_dataset_idx'] in train_idxs.tolist()}\n",
    "        val_3d_idxs = set(range(training_dataset.__len__(yield_2d_override=False))) - trained_3d_dataset_idxs\n",
    "        print(\"Will run validation with these 3D samples:\", val_3d_idxs)\n",
    "\n",
    "        ### Disturb dataset ###\n",
    "        proposed_disturbed_idxs = np.random.choice(train_idxs, size=int(len(train_idxs)*config.disturbed_percentage), replace=False)\n",
    "        proposed_disturbed_idxs = torch.tensor(proposed_disturbed_idxs)\n",
    "        training_dataset.disturb_idxs(proposed_disturbed_idxs,\n",
    "            disturbance_mode=config.disturbance_mode,\n",
    "            disturbance_strength=config.disturbance_strength\n",
    "        )\n",
    "\n",
    "        disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "        disturbed_bool_vect[training_dataset.disturbed_idxs] = 1.\n",
    "\n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, training_dataset.disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(training_dataset.disturbed_idxs))\n",
    "\n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in training_dataset.disturbed_idxs])},\n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "\n",
    "        ### Visualization ###\n",
    "        if config.do_plot:\n",
    "            print(\"Disturbed samples:\")\n",
    "            for d_idx in training_dataset.disturbed_idxs:\n",
    "                display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=training_dataset[d_idx][0],\n",
    "                    ground_truth=disturb_seg(training_dataset[d_idx][1]),\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "\n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels =12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "\n",
    "        _, all_segs = training_dataset.get_data()\n",
    "\n",
    "        class_weights = 1/(torch.bincount(all_segs.reshape(-1).long())).float().pow(.35)\n",
    "        class_weights /= class_weights.mean()\n",
    "\n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "            sampler=train_subsampler, pin_memory=True, drop_last=False,\n",
    "            # collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "        )\n",
    "\n",
    "        training_dataset.set_augment_at_collate(False)\n",
    "\n",
    "#         val_dataloader = DataLoader(training_dataset, batch_size=config.val_batch_size,\n",
    "#                                     sampler=val_subsampler, pin_memory=True, drop_last=False)\n",
    "\n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        epx_start = config.get('checkpoint_epx', 0)\n",
    "\n",
    "        if config.checkpoint_name:\n",
    "            # Load from checkpoint\n",
    "            _path = f\"{config.mdl_save_prefix}/{config.checkpoint_name}_fold{fold_idx}_epx{epx_start}\"\n",
    "        else:\n",
    "            _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx_start}\"\n",
    "\n",
    "        (lraspp, optimizer, optimizer_dp, embedding, scaler) = get_model(config, len(training_dataset), len(training_dataset.label_tags), _path=_path, device='cuda')\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=500, T_mult=2)\n",
    "\n",
    "        if optimizer_dp:\n",
    "            scheduler_dp = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer_dp, T_0=500, T_mult=2)\n",
    "        else:\n",
    "            scheduler_dp = None\n",
    "\n",
    "        _, all_segs = training_dataset.get_data()\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare corr coefficient scoring\n",
    "        training_dataset.eval(use_modified=True)\n",
    "        norm_label, mod_label = list(zip(*[(sample['label'], sample['modified_label']) \\\n",
    "            for sample in training_dataset]))\n",
    "        union_norm_mod_label = torch.logical_or(torch.stack(norm_label), torch.stack(mod_label))\n",
    "        union_norm_mod_label = union_norm_mod_label.cuda()\n",
    "\n",
    "        for epx in range(epx_start, config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "\n",
    "            lraspp.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            training_dataset.train(use_modified=(epx >= config.start_disturbing_after_ep))\n",
    "            wandb.log({\"use_modified\": float(training_dataset.use_modified)}, step=global_idx)\n",
    "\n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            class_dices = []\n",
    "\n",
    "            # Load data\n",
    "            for batch in train_dataloader:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if optimizer_dp:\n",
    "                    optimizer_dp.zero_grad()\n",
    "\n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "                b_spat_aug_grid = batch['spat_augment_grid']\n",
    "\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "                b_img = b_img.float()\n",
    "\n",
    "                b_img = b_img.cuda()\n",
    "                b_seg_modified = b_seg_modified.cuda()\n",
    "                b_idxs_dataset = b_idxs_dataset.cuda()\n",
    "                b_seg = b_seg.cuda()\n",
    "                b_spat_aug_grid = b_spat_aug_grid.cuda()\n",
    "\n",
    "                if config.use_mind:\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "                else:\n",
    "                    b_img = b_img.unsqueeze(1)\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=True):\n",
    "                    assert b_img.dim() == 4, \\\n",
    "                        f\"Input image for model must be 4D: BxCxHxW but is {b_img.shape}\"\n",
    "\n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    ### Calculate loss ###\n",
    "                    assert logits.dim() == 4, \\\n",
    "                        f\"Input shape for loss must be BxNUM_CLASSESxHxW but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == 3, \\\n",
    "                        f\"Target shape for loss must be BxHxW but is {b_seg_modified.shape}\"\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "\n",
    "                        loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified).mean((-1,-2))\n",
    "                        weight = torch.sigmoid(embedding(b_idxs_dataset)).squeeze()\n",
    "                        weight = weight/weight.mean()\n",
    "                        # weight = weight/instance_pixel_weight[b_idxs_dataset] TODO removce\n",
    "                        loss = (loss*weight).sum()\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = (logits*weight.view(-1,1,1,1)).argmax(1)\n",
    "\n",
    "                    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                        loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified)\n",
    "                        m_dp_idxs = map_embedding_idxs(b_idxs_dataset, config.grid_size_y, config.grid_size_x)\n",
    "                        weight = embedding(m_dp_idxs)\n",
    "                        weight = weight.reshape(-1, config.grid_size_y, config.grid_size_x)\n",
    "                        weight = weight.unsqueeze(1)\n",
    "                        weight = torch.nn.functional.interpolate(\n",
    "                            weight,\n",
    "                            size=(b_seg_modified.shape[-2:]),\n",
    "                            mode='bilinear',\n",
    "                            align_corners=True\n",
    "                        )\n",
    "                        weight = torch.sigmoid(weight)\n",
    "                        weight = weight/weight.mean()\n",
    "                        weight = F.grid_sample(weight, b_spat_aug_grid,\n",
    "                            padding_mode='border', align_corners=False)\n",
    "                        loss = (loss.unsqueeze(1)*weight).sum()\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = (logits*weight).argmax(1)\n",
    "\n",
    "                    else:\n",
    "                        loss = nn.CrossEntropyLoss(class_weights.cuda())(logits, b_seg_modified)\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = logits.argmax(1)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED) and epx > 10:\n",
    "                    scaler.step(optimizer_dp)\n",
    "\n",
    "                scaler.update()\n",
    "\n",
    "                epx_losses.append(loss.item())\n",
    "\n",
    "                # Calculate dice score\n",
    "                b_dice = dice2d(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(b_seg, len(training_dataset.label_tags)), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_over_all(\n",
    "                    b_dice, exclude_bg=True))\n",
    "                class_dices.append(get_batch_dice_per_class(\n",
    "                    b_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                ###  Scheduler management ###\n",
    "                if config.use_cosine_annealing:\n",
    "                    scheduler.step()\n",
    "                    if epx == config.epochs//2:\n",
    "                        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                            optimizer, T_0=500, T_mult=2)\n",
    "                if config.debug:\n",
    "                    break\n",
    "\n",
    "            ### Logging ###\n",
    "            print(f\"### Log epoch {epx} @ {time.time()-t0:.2f}s\")\n",
    "            print(\"### Training\")\n",
    "            ### Log wandb data ###\n",
    "            # Log the epoch idx per fold - so we can recover the diagram by setting\n",
    "            # ref_epoch_idx as x-axis in wandb interface\n",
    "            wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "\n",
    "            mean_loss = torch.tensor(epx_losses).mean()\n",
    "            wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "\n",
    "            mean_dice = np.nanmean(dices)\n",
    "            print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "            wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "\n",
    "            log_class_dices(\"scores/dice_mean_\", f\"_fold{fold_idx}\", class_dices, global_idx)\n",
    "\n",
    "            # Log data parameters of disturbed samples\n",
    "            if len(training_dataset.disturbed_idxs) > 0 and str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                    m_tr_idxs = map_embedding_idxs(train_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                    ).cuda()\n",
    "                    masks = union_norm_mod_label[train_idxs].float()\n",
    "                    masked_weights = torch.nn.functional.interpolate(\n",
    "                        embedding(m_tr_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                        size=(masks.shape[-2:]),\n",
    "                        mode='bilinear',\n",
    "                        align_corners=True\n",
    "                    ).squeeze(1) * masks\n",
    "                    masked_weights[masked_weights==0.] = float('nan')\n",
    "\n",
    "                    corr_coeff = np.corrcoef(\n",
    "                        np.nanmean(masked_weights.detach().cpu(), axis=(-2,-1)),\n",
    "                        disturbed_bool_vect[train_idxs].cpu().numpy()\n",
    "                    )[0,1]\n",
    "\n",
    "                elif str(config.data_param_mode) == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                    corr_coeff = np.corrcoef(\n",
    "                        embedding(train_idxs.cuda()).detach().cpu().view(train_idxs.numel()).numpy(),\n",
    "                        disturbed_bool_vect[train_idxs].cpu().numpy()\n",
    "                    )[0,1]\n",
    "\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/corr_coeff_fold{fold_idx}': corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                print(f'data_parameters/corr_coeff_fold{fold_idx}', f\"{corr_coeff:.2f}\")\n",
    "\n",
    "            if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                log_data_parameter_stats(f'data_parameters/iter_stats_fold{fold_idx}', global_idx, embedding.weight.data)\n",
    "\n",
    "            if (epx % config.save_every == 0 and epx != 0) \\\n",
    "                or (epx+1 == config.epochs):\n",
    "                _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx}\"\n",
    "                save_model(\n",
    "                    _path,\n",
    "                    lraspp=lraspp,\n",
    "                    optimizer=optimizer, optimizer_dp=optimizer_dp,\n",
    "                    scheduler=scheduler, sheduler_dp=scheduler_dp,\n",
    "                    embedding=embedding,\n",
    "                    scaler=scaler)\n",
    "                (lraspp, optimizer, optimizer_dp, embedding, scaler) = get_model(config, len(training_dataset), len(training_dataset.label_tags), _path=_path, device='cuda')\n",
    "\n",
    "            print()\n",
    "            print(\"### Validation\")\n",
    "            lraspp.eval()\n",
    "            training_dataset.eval()\n",
    "\n",
    "            val_dices = []\n",
    "            val_class_dices = []\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    for val_idx in val_3d_idxs:\n",
    "                        val_sample = training_dataset.get_3d_item(val_idx)\n",
    "                        stack_dim = training_dataset.yield_2d_normal_to\n",
    "                        # Create batch out of single val sample\n",
    "                        b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                        b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "\n",
    "                        B = b_val_img.shape[0]\n",
    "\n",
    "                        b_val_img = b_val_img.unsqueeze(1).float().cuda()\n",
    "                        b_val_seg = b_val_seg.cuda()\n",
    "                        b_val_seg = interpolate_sample(b_label=b_val_seg.permute(0,3,1,2).squeeze(0), scale_factor=2, yield_2d=True)[1].permute(1,2,0).unsqueeze(0)\n",
    "\n",
    "                        b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=stack_dim)\n",
    "\n",
    "                        if config.use_mind:\n",
    "                            b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "                        b_val_img_2d= interpolate_sample(b_image=b_val_img_2d.squeeze(1), scale_factor=2, yield_2d=True)[0].unsqueeze(1)\n",
    "                        output_val = lraspp(b_val_img_2d)['out']\n",
    "                        # features = lraspp.backbone(F.interpolate(b_val_img_2d,scale_factor=2,mode='bilinear'))\n",
    "                        # output_val = F.interpolate(lraspp.classifier(features),scale_factor=2,mode='bilinear',align_corners=False)\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                        val_logits_for_score = output_val.argmax(1)\n",
    "                        val_logits_for_score_3d = make_3d_from_2d_stack(\n",
    "                            val_logits_for_score.unsqueeze(1), stack_dim, B\n",
    "                        ).squeeze(1)\n",
    "\n",
    "                        b_val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(val_logits_for_score_3d, len(training_dataset.label_tags)),\n",
    "                            torch.nn.functional.one_hot(b_val_seg, len(training_dataset.label_tags)),\n",
    "                            one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        # Get mean score over batch\n",
    "                        val_dices.append(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=True))\n",
    "\n",
    "                        val_class_dices.append(get_batch_dice_per_class(\n",
    "                            b_val_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                        if config.do_plot:\n",
    "                            print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                            print(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=False))\n",
    "                            # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                            display_seg(in_type=\"single_3D\",\n",
    "                                reduce_dim=\"W\",\n",
    "                                img=val_sample['image'].unsqueeze(0).cpu(),\n",
    "                                seg=val_logits_for_score_3d.squeeze(0).cpu(), # CHECK TODO\n",
    "                                ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                crop_to_non_zero_seg=True,\n",
    "                                crop_to_non_zero_gt=True,\n",
    "                                alpha_seg=.3,\n",
    "                                alpha_gt=.0\n",
    "                            )\n",
    "\n",
    "                    mean_val_dice = np.nanmean(val_dices)\n",
    "                    print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                    wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                    log_class_dices(\"scores/val_dice_mean_\", f\"_fold{fold_idx}\", val_class_dices, global_idx)\n",
    "\n",
    "            print()\n",
    "            # End of training loop\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            all_idxs = torch.tensor(range(len(training_dataset))).cuda()\n",
    "            train_label_snapshot_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/train_label_snapshot.pth\")\n",
    "            seg_viz_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weighted_samples.png\")\n",
    "\n",
    "            train_label_snapshot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if str(config.data_param_mode) == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                dp_weights = embedding(all_idxs)\n",
    "\n",
    "                data = [(\n",
    "                    dp_weight,\n",
    "                    bool(disturb_flg.item()),\n",
    "                    sample['id'],\n",
    "                    sample['dataset_idx'],\n",
    "                    sample['image'],\n",
    "                    sample['label'],\n",
    "                    sample['modified_label']) for dp_weight, disturb_flg, sample in zip(dp_weights[train_idxs], disturbed_bool_vect[train_idxs], torch.utils.data.Subset(training_dataset,train_idxs))\n",
    "                ]\n",
    "\n",
    "                samples_sorted = sorted(data, key=lambda tpl: tpl[0])\n",
    "                (dp_weight, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _2d_imgs,\n",
    "                 _2d_labels, _2d_modified_labels) = zip(*samples_sorted)\n",
    "\n",
    "                torch.save(\n",
    "                    [\n",
    "                        dp_weight, disturb_flags,\n",
    "                        d_ids, torch.stack(dataset_idxs), torch.stack(_2d_labels),\n",
    "                        torch.stack(_2d_modified_labels)\n",
    "                    ],\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "            elif str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                # Log histogram of clean and disturbed parameters\n",
    "                m_all_idxs = map_embedding_idxs(all_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                ).cuda()\n",
    "                masks = union_norm_mod_label[all_idxs].float()\n",
    "                all_weights = torch.nn.functional.interpolate(\n",
    "                    embedding(m_all_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                    size=(masks.shape[-2:]),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=True\n",
    "                ).squeeze(1)\n",
    "\n",
    "                masked_weights = all_weights * masks\n",
    "                masked_weights[masked_weights==0.] = float('nan')\n",
    "                dp_weights = np.nanmean(masked_weights.detach().cpu(), axis=(-2,-1))\n",
    "\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weightmap.png\")\n",
    "\n",
    "                data = [(\n",
    "                    dp_weight,\n",
    "                    weightmap,\n",
    "                    bool(disturb_flg.item()),\n",
    "                    sample['id'],\n",
    "                    sample['dataset_idx'],\n",
    "                    sample['image'],\n",
    "                    sample['label'],\n",
    "                    sample['modified_label']) for dp_weight, weightmap, disturb_flg, sample in zip(dp_weights[train_idxs], all_weights[train_idxs], disturbed_bool_vect[train_idxs], torch.utils.data.Subset(training_dataset,train_idxs))\n",
    "                ]\n",
    "\n",
    "                samples_sorted = sorted(data, key=lambda tpl: tpl[0])\n",
    "                (dp_weight, dp_weightmap, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _2d_imgs,\n",
    "                 _2d_labels, _2d_modified_labels) = zip(*samples_sorted)\n",
    "\n",
    "                torch.save(\n",
    "                    [\n",
    "                        dp_weight, torch.stack(dp_weightmap), disturb_flags,\n",
    "                        d_ids, torch.stack(dataset_idxs), torch.stack(_2d_labels),\n",
    "                        torch.stack(_2d_modified_labels)\n",
    "                    ],\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "                print(\"Writing weight map.\")\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}_data_parameter_weightmap.png\")\n",
    "                visualize_seg(in_type=\"batch_2D\",\n",
    "                    img=torch.stack(dp_weightmap).unsqueeze(1),\n",
    "                    seg=torch.stack(_2d_modified_labels),\n",
    "                    alpha_seg = 0.,\n",
    "                    n_per_row=70,\n",
    "                    overlay_text=overlay_text_list,\n",
    "                    annotate_color=(0,255,255),\n",
    "                    frame_elements=disturb_flags,\n",
    "                    file_path=weightmap_out_path,\n",
    "                )\n",
    "\n",
    "            if len(training_dataset.disturbed_idxs) > 0:\n",
    "                # Log histogram\n",
    "                separated_params = list(zip(dp_weights[clean_idxs], dp_weights[training_dataset.disturbed_idxs]))\n",
    "                s_table = wandb.Table(columns=['clean_idxs', 'disturbed_idxs'], data=separated_params)\n",
    "                fields = {\"primary_bins\" : \"clean_idxs\", \"secondary_bins\" : \"disturbed_idxs\", \"title\" : \"Data parameter composite histogram\"}\n",
    "                composite_histogram = wandb.plot_table(vega_spec_name=\"rap1ide/composite_histogram\", data_table=s_table, fields=fields)\n",
    "                wandb.log({f\"data_parameters/separated_params_fold_{fold_idx}\": composite_histogram})\n",
    "\n",
    "            # Write out data of modified and un-modified labels and an overview image\n",
    "            print(\"Writing out sample image.\")\n",
    "            # overlay text example: d_idx=0, dp_i=1.00, dist? False\n",
    "            overlay_text_list = [f\"id:{d_id} dp:{instance_p.item():.2f}\" \\\n",
    "                for d_id, instance_p, disturb_flg in zip(d_ids, dp_weight, disturb_flags)]\n",
    "\n",
    "            visualize_seg(in_type=\"batch_2D\",\n",
    "                img=torch.stack(_2d_imgs).unsqueeze(1),\n",
    "                seg=4*torch.stack(_2d_modified_labels),\n",
    "                ground_truth=torch.stack(_2d_labels),\n",
    "                crop_to_non_zero_seg=False,\n",
    "                alpha_seg = .3,\n",
    "                alpha_gt = .5,\n",
    "                n_per_row=70,\n",
    "                overlay_text=overlay_text_list,\n",
    "                annotate_color=(0,255,255),\n",
    "                frame_elements=disturb_flags,\n",
    "                file_path=seg_viz_out_path,\n",
    "            )\n",
    "        # End of fold loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "450fc80a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Config overrides\n",
    "# config_dict['wandb_mode'] = 'disabled'\n",
    "# config_dict['debug'] = True\n",
    "# Model loading\n",
    "# config_dict['checkpoint_name'] = 'treasured-water-717'\n",
    "# # config_dict['fold_override'] = 0\n",
    "# config_dict['checkpoint_epx'] = 39\n",
    "\n",
    "# Define sweep override dict\n",
    "sweep_config_dict = dict(\n",
    "    method='grid',\n",
    "    metric=dict(goal='maximize', name='scores/val_dice_mean_tumour_fold0'),\n",
    "    parameters=dict(\n",
    "        # disturbance_mode=dict(\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        reg_state=dict(\n",
    "            values=['best','combined']\n",
    "        ),\n",
    "        # disturbance_strength=dict(\n",
    "        #     values=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "        # ),\n",
    "        # disturbed_percentage=dict(\n",
    "        #     values=[0.3, 0.6]\n",
    "        # ),\n",
    "        data_param_mode=dict(\n",
    "            values=[\n",
    "                DataParamMode.INSTANCE_PARAMS,\n",
    "                DataParamMode.DISABLED,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549637cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rap1ide/curriculum_deeplab/runs/2rmix1hh\" target=\"_blank\">fiery-armadillo-635</a></strong> to <a href=\"https://wandb.ai/rap1ide/curriculum_deeplab\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run validation with these 3D samples: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 40}\n",
      "Disturbed indexes: [987, 988, 990, 991, 992, 995, 1000, 1003, 1004, 1005, 1008, 1013, 1016, 1017, 1020, 1023, 1025, 1031, 1033, 1034, 1038, 1039, 1040, 1041, 1044, 1047, 1052, 1062, 1063, 1066, 1068, 1071, 1075, 1078, 1093, 1094, 1099, 1104, 1108, 1110, 1119, 1121, 1127, 1128, 1130, 1131, 1134, 1138, 1140, 1142, 1145, 1147, 1156, 1157, 1161, 1163, 1168, 1170, 1173, 1176, 1177, 1178, 1182, 1184, 1186, 1188, 1190, 1192, 1196, 1200, 1201, 1203, 1205, 1206, 1209, 1213, 1215, 1219, 1226, 1235, 1237, 1239, 1240, 1245, 1246, 1248, 1250, 1254, 1256, 1257, 1262, 1264, 1265, 1275, 1280, 1284, 1285, 1288, 1289, 1294, 1296, 1301, 1303, 1304, 1305, 1308, 1312, 1319, 1328, 1333, 1337, 1339, 1346, 1347, 1358, 1362, 1364, 1365, 1367, 1370, 1371, 1372, 1376, 1379, 1385, 1388, 1391, 1394, 1398, 1400, 1411, 1413, 1420, 1422, 1424, 1426, 1428, 1429, 1432, 1438, 1439, 1443, 1444, 1445, 1447, 1451, 1453, 1459, 1461, 1464, 1465, 1467, 1468, 1471, 1473, 1475, 1477, 1487, 1489, 1492, 1497, 1498, 1502, 1503, 1504, 1505, 1507, 1508, 1512, 1515, 1516, 1518, 1522, 1524, 1525, 1532, 1539, 1543, 1544, 1549, 1550, 1551, 1554, 1558, 1560, 1561, 1562, 1564, 1565, 1568, 1573, 1582, 1583, 1588, 1595, 1596, 1601, 1602, 1604, 1605, 1606, 1607, 1618, 1620, 1624, 1627, 1634, 1637, 1640, 1641, 1644, 1651, 1652, 1653, 1658, 1660, 1662, 1668, 1672, 1674, 1675, 1678, 1690, 1692, 1694, 1697, 1698, 1699, 1702, 1707, 1709, 1712, 1717, 1719, 1723, 1726, 1729, 1730, 1731, 1734, 1745, 1746, 1748, 1750, 1755, 1759, 1761, 1762, 1767, 1778, 1780, 1782, 1785, 1787, 1791, 1796, 1797, 1799, 1803, 1806, 1811, 1814, 1815, 1817, 1824, 1825, 1827, 1847, 1849, 1852, 1861, 1864, 1866, 1869, 1873, 1878, 1885, 1886, 1899, 1902, 1904, 1907, 1910, 1913, 1916, 1923, 1924, 1929, 1930, 1932, 1934, 1943, 1946, 1947, 1948, 1949, 1952, 1956, 1960, 1963, 1967, 1968, 1969, 1973, 1975, 1978, 1980, 1982, 1986, 1987, 1988, 1989, 2004, 2007, 2009, 2011, 2012, 2013, 2017, 2018, 2022, 2024, 2025, 2028, 2031, 2033, 2035, 2036, 2038, 2041, 2043, 2044, 2049, 2050, 2059, 2062, 2064, 2065, 2067, 2073, 2074, 2077, 2083, 2085, 2089, 2091, 2096, 2100, 2102, 2107, 2111, 2114, 2118, 2122, 2133, 2134, 2136, 2150, 2151, 2154, 2156, 2160, 2161, 2164, 2166, 2169, 2174, 2175, 2177, 2179, 2182, 2189, 2199, 2208, 2212, 2220, 2222, 2223, 2232, 2240, 2241, 2242, 2243, 2245, 2246, 2247, 2250, 2260, 2261, 2262, 2263, 2265, 2266, 2267, 2268, 2272, 2274, 2277, 2285, 2287, 2288, 2295, 2296, 2301, 2303, 2304, 2305, 2306, 2309, 2312, 2316, 2318, 2321, 2326, 2330, 2335, 2337, 2342, 2345, 2346, 2353, 2354, 2355, 2362, 2371, 2372, 2373, 2374, 2377, 2382, 2387, 2388, 2392, 2398, 2401, 2408, 2412, 2413, 2416, 2417, 2418, 2422, 2423, 2425, 2426, 2430, 2437, 2439, 2440, 2442, 2444, 2446, 2447, 2453, 2467, 2468, 2472, 2476, 2480, 2481, 2484, 2488, 2490, 2492, 2494, 2495, 2499, 2501, 2504, 2508, 2515, 2516, 2521, 2524, 2529, 2531, 2534, 2537, 2540, 2543, 2545, 2548, 2554, 2566, 2569, 2571, 2572, 2574, 2576, 2578, 2582, 2583, 2586, 2588, 2598, 2600, 2601, 2608, 2612, 2623, 2629, 2636, 2642, 2646, 2649, 2653, 2659, 2660, 2663, 2666, 2667, 2668, 2669, 2673, 2679, 2682, 2683, 2688, 2689, 2691, 2694, 2696, 2698, 2701, 2705, 2707, 2712, 2723, 2729, 2730, 2735, 2742, 2745, 2747, 2748, 2749, 2757, 2760, 2762, 2766, 2768, 2771, 2772, 2775, 2782, 2791, 2796, 2797, 2799, 2800, 2803, 2804, 2805, 2807, 2816, 2818, 2820, 2821, 2825, 2829, 2835, 2840, 2841, 2844, 2848, 2850, 2858, 2865, 2866, 2870, 2877, 2878, 2881, 2882, 2885, 2886, 2888, 2896, 2905, 2913, 2915, 2918, 2929, 2932, 2934, 2938, 2942, 2948, 2950, 2952, 2956]\n",
      "Loading lr-aspp model, optimizers, embedding and grad scalers from /share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab/data/models/fiery-armadillo-635_fold0_epx79\n",
      "Param count lraspp: 3219604\n",
      "Param count embedding: 756992\n",
      "### Log epoch 79 @ 20.96s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 73.94%\n",
      "scores/dice_mean_tumour_fold0 73.94%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 54.97%\n",
      "scores/val_dice_mean_tumour_fold0 54.97%\n",
      "\n",
      "### Log epoch 80 @ 37.37s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 73.98%\n",
      "scores/dice_mean_tumour_fold0 73.98%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 55.51%\n",
      "scores/val_dice_mean_tumour_fold0 55.51%\n",
      "\n",
      "### Log epoch 81 @ 53.66s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 74.67%\n",
      "scores/dice_mean_tumour_fold0 74.67%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 54.68%\n",
      "scores/val_dice_mean_tumour_fold0 54.68%\n",
      "\n",
      "### Log epoch 82 @ 69.87s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 74.14%\n",
      "scores/dice_mean_tumour_fold0 74.14%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 55.31%\n",
      "scores/val_dice_mean_tumour_fold0 55.31%\n",
      "\n",
      "### Log epoch 83 @ 85.55s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 74.07%\n",
      "scores/dice_mean_tumour_fold0 74.07%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 55.56%\n",
      "scores/val_dice_mean_tumour_fold0 55.56%\n",
      "\n",
      "### Log epoch 84 @ 101.63s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 74.72%\n",
      "scores/dice_mean_tumour_fold0 74.72%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 55.43%\n",
      "scores/val_dice_mean_tumour_fold0 55.43%\n",
      "\n",
      "### Log epoch 85 @ 117.76s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 74.38%\n",
      "scores/dice_mean_tumour_fold0 74.38%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 55.72%\n",
      "scores/val_dice_mean_tumour_fold0 55.72%\n",
      "\n",
      "### Log epoch 86 @ 133.94s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 73.74%\n",
      "scores/dice_mean_tumour_fold0 73.74%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 55.24%\n",
      "scores/val_dice_mean_tumour_fold0 55.24%\n",
      "\n",
      "### Log epoch 87 @ 150.19s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 74.10%\n",
      "scores/dice_mean_tumour_fold0 74.10%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 54.69%\n",
      "scores/val_dice_mean_tumour_fold0 54.69%\n",
      "\n",
      "### Log epoch 88 @ 166.37s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 73.76%\n",
      "scores/dice_mean_tumour_fold0 73.76%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 55.34%\n",
      "scores/val_dice_mean_tumour_fold0 55.34%\n",
      "\n",
      "### Log epoch 89 @ 182.41s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 73.85%\n",
      "scores/dice_mean_tumour_fold0 73.85%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 55.13%\n",
      "scores/val_dice_mean_tumour_fold0 55.13%\n",
      "\n",
      "### Log epoch 90 @ 198.67s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 73.93%\n",
      "scores/dice_mean_tumour_fold0 73.93%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 54.49%\n",
      "scores/val_dice_mean_tumour_fold0 54.49%\n",
      "\n",
      "### Log epoch 91 @ 214.86s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 74.12%\n",
      "scores/dice_mean_tumour_fold0 74.12%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 54.04%\n",
      "scores/val_dice_mean_tumour_fold0 54.04%\n",
      "\n",
      "### Log epoch 92 @ 231.09s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 73.66%\n",
      "scores/dice_mean_tumour_fold0 73.66%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 53.83%\n",
      "scores/val_dice_mean_tumour_fold0 53.83%\n",
      "\n",
      "### Log epoch 93 @ 247.32s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 74.14%\n",
      "scores/dice_mean_tumour_fold0 74.14%\n",
      "data_parameters/corr_coeff_fold0 0.01\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 54.61%\n",
      "scores/val_dice_mean_tumour_fold0 54.61%\n",
      "\n",
      "### Log epoch 94 @ 263.56s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 73.70%\n",
      "scores/dice_mean_tumour_fold0 73.70%\n",
      "data_parameters/corr_coeff_fold0 0.00\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 54.48%\n",
      "scores/val_dice_mean_tumour_fold0 54.48%\n",
      "\n",
      "### Log epoch 95 @ 279.65s\n",
      "### Training\n",
      "dice_mean_wo_bg_fold0 73.60%\n",
      "scores/dice_mean_tumour_fold0 73.60%\n",
      "data_parameters/corr_coeff_fold0 0.00\n",
      "\n",
      "### Validation\n",
      "val_dice_mean_wo_bg_fold0 54.76%\n",
      "scores/val_dice_mean_tumour_fold0 54.76%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normal_run():\n",
    "    with wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        ) as run:\n",
    "\n",
    "        run_name = run.name\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init() as run:\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "\n",
    "        run_name = run.name\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "\n",
    "if config_dict['do_sweep']:\n",
    "    # Integrate all config_dict entries into sweep_dict.parameters -> sweep overrides config_dict\n",
    "    cp_config_dict = copy.deepcopy(config_dict)\n",
    "    # cp_config_dict.update(copy.deepcopy(sweep_config_dict['parameters']))\n",
    "    for del_key in sweep_config_dict['parameters'].keys():\n",
    "        if del_key in cp_config_dict:\n",
    "            del cp_config_dict[del_key]\n",
    "    merged_sweep_config_dict = copy.deepcopy(sweep_config_dict)\n",
    "    # merged_sweep_config_dict.update(cp_config_dict)\n",
    "    for key, value in cp_config_dict.items():\n",
    "        merged_sweep_config_dict['parameters'][key] = dict(value=value)\n",
    "    # Convert enum values in parameters to string. They will be identified by their numerical index otherwise\n",
    "    for key, param_dict in merged_sweep_config_dict['parameters'].items():\n",
    "        if 'value' in param_dict and isinstance(param_dict['value'], Enum):\n",
    "            param_dict['value'] = str(param_dict['value'])\n",
    "        if 'values' in param_dict:\n",
    "            param_dict['values'] = [str(elem) if isinstance(elem, Enum) else elem for elem in param_dict['values']]\n",
    "\n",
    "        merged_sweep_config_dict['parameters'][key] = param_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(merged_sweep_config_dict, project=\"curriculum_deeplab\")\n",
    "    wandb.agent(sweep_id, function=sweep_run)\n",
    "\n",
    "else:\n",
    "    normal_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7469925",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# def inference_DL(run_name, config, inf_dataset):\n",
    "\n",
    "#     score_dicts = []\n",
    "\n",
    "#     fold_iter = range(config.num_folds)\n",
    "#     if config_dict['only_first_fold']:\n",
    "#         fold_iter = fold_iter[0:1]\n",
    "\n",
    "#     for fold_idx in fold_iter:\n",
    "#         lraspp, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "#         lraspp.eval()\n",
    "#         inf_dataset.eval()\n",
    "#         stack_dim = config.yield_2d_normal_to\n",
    "\n",
    "#         inf_dices = []\n",
    "#         inf_dices_tumour = []\n",
    "#         inf_dices_cochlea = []\n",
    "\n",
    "#         for inf_sample in inf_dataset:\n",
    "#             global_idx = get_global_idx(fold_idx, sample_idx, config.epochs)\n",
    "#             crossmoda_id = sample['crossmoda_id']\n",
    "#             with amp.autocast(enabled=True):\n",
    "#                 with torch.no_grad():\n",
    "\n",
    "#                     # Create batch out of single val sample\n",
    "#                     b_inf_img = inf_sample['image'].unsqueeze(0)\n",
    "#                     b_inf_seg = inf_sample['label'].unsqueeze(0)\n",
    "\n",
    "#                     B = b_inf_img.shape[0]\n",
    "\n",
    "#                     b_inf_img = b_inf_img.unsqueeze(1).float().cuda()\n",
    "#                     b_inf_seg = b_inf_seg.cuda()\n",
    "#                     b_inf_img_2d = make_2d_stack_from_3d(b_inf_img, stack_dim=stack_dim)\n",
    "\n",
    "#                     if config.use_mind:\n",
    "#                         b_inf_img_2d = mindssc(b_inf_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "#                     output_inf = lraspp(b_inf_img_2d)['out']\n",
    "\n",
    "#                     # Prepare logits for scoring\n",
    "#                     # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "#                     inf_logits_for_score = make_3d_from_2d_stack(output_inf, stack_dim, B)\n",
    "#                     inf_logits_for_score = inf_logits_for_score.argmax(1)\n",
    "\n",
    "#                     inf_dice = dice3d(\n",
    "#                         torch.nn.functional.one_hot(inf_logits_for_score, 3),\n",
    "#                         torch.nn.functional.one_hot(b_inf_seg, 3),\n",
    "#                         one_hot_torch_style=True\n",
    "#                     )\n",
    "#                     inf_dices.append(get_batch_dice_wo_bg(inf_dice))\n",
    "#                     inf_dices_tumour.append(get_batch_dice_tumour(inf_dice))\n",
    "#                     inf_dices_cochlea.append(get_batch_dice_cochlea(inf_dice))\n",
    "\n",
    "#                     if config.do_plot:\n",
    "#                         print(\"Inference 3D image label/ground-truth\")\n",
    "#                         print(inf_dice)\n",
    "#                         # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "#                         display_seg(in_type=\"single_3D\",\n",
    "#                             reduce_dim=\"W\",\n",
    "#                             img=inf_sample['image'].unsqueeze(0).cpu(),\n",
    "#                             seg=inf_logits_for_score.squeeze(0).cpu(),\n",
    "#                             ground_truth=b_inf_seg.squeeze(0).cpu(),\n",
    "#                             crop_to_non_zero_seg=True,\n",
    "#                             crop_to_non_zero_gt=True,\n",
    "#                             alpha_seg=.4,\n",
    "#                             alpha_gt=.2\n",
    "#                         )\n",
    "\n",
    "#             if config.debug:\n",
    "#                 break\n",
    "\n",
    "#         mean_inf_dice = np.nanmean(inf_dices)\n",
    "#         mean_inf_dice_tumour = np.nanmean(inf_dices_tumour)\n",
    "#         mean_inf_dice_cochlea = np.nanmean(inf_dices_cochlea)\n",
    "\n",
    "#         print(f'inf_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_inf_dice*100:.2f}%\")\n",
    "#         print(f'inf_dice_mean_tumour_fold{fold_idx}', f\"{mean_inf_dice_tumour*100:.2f}%\")\n",
    "#         print(f'inf_dice_mean_cochlea_fold{fold_idx}', f\"{mean_inf_dice_cochlea*100:.2f}%\")\n",
    "#         wandb.log({f'scores/inf_dice_mean_wo_bg_fold{fold_idx}': mean_inf_dice}, step=global_idx)\n",
    "#         wandb.log({f'scores/inf_dice_mean_tumour_fold{fold_idx}': mean_inf_dice_tumour}, step=global_idx)\n",
    "#         wandb.log({f'scores/inf_dice_mean_cochlea_fold{fold_idx}': mean_inf_dice_cochlea}, step=global_idx)\n",
    "\n",
    "#         # Store data for inter-fold scoring\n",
    "#         class_dice_list = inf_dices.tolist()[0]\n",
    "#         for class_idx, class_dice in enumerate(class_dice_list):\n",
    "#             score_dicts.append(\n",
    "#                 {\n",
    "#                     'fold_idx': fold_idx,\n",
    "#                     'crossmoda_id': crossmoda_id,\n",
    "#                     'class_idx': class_idx,\n",
    "#                     'class_dice': class_dice,\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     mean_oa_inf_dice = np.nanmean(torch.tensor([score['class_dice'] for score in score_dicts]))\n",
    "#     print(f\"Mean dice over all folds, classes and samples: {mean_oa_inf_dice*100:.2f}%\")\n",
    "#     wandb.log({'scores/mean_dice_all_folds_samples_classes': mean_oa_inf_dice}, step=global_idx)\n",
    "\n",
    "#     return score_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a608620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds_scores = []\n",
    "# run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"testing\", job_type=\"test\",\n",
    "#         config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "#         mode=config_dict['wandb_mode']\n",
    "# )\n",
    "# config = wandb.config\n",
    "# score_dicts = inference_DL(run_name, config, validation_dataset)\n",
    "# folds_scores.append(score_dicts)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984e8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
