{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3175666",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name                     Util    Mem free  Cuda             User(s)\n",
      "----  --------------------------  ------  ----------  ---------------  -------------\n",
      "   2  NVIDIA GeForce RTX 2080 Ti     0 %   11016 MiB  11.5(495.29.05)\n",
      "   3  NVIDIA GeForce RTX 2080 Ti     0 %    8834 MiB  11.5(495.29.05)  andresen\n",
      "   0  NVIDIA GeForce RTX 2080 Ti     0 %    2575 MiB  11.5(495.29.05)  schneider\n",
      "   1  NVIDIA GeForce RTX 2080 Ti     0 %      55 MiB  11.5(495.29.05)  grossbroehmer\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   2  NVIDIA GeForce RTX 2080 Ti  ->  cuda:0\n",
      "1.9.0a0+gitdfbd030\n",
      "8204\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Running in: /share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import glob\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -4\"))\n",
    "import pickle\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import functools\n",
    "from enum import Enum, auto\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import visualize_seg\n",
    "\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "from curriculum_deeplab.utils import interpolate_sample, in_notebook, dilate_label_class, LabelDisturbanceMode\n",
    "from curriculum_deeplab.CrossmodaHybridIdLoader import CrossmodaHybridIdLoader, get_crossmoda_data_load_closure\n",
    "from curriculum_deeplab.MobileNet_LR_ASPP_3D import MobileNet_LRASPP_3D\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "if in_notebook:\n",
    "    THIS_SCRIPT_DIR = os.path.abspath('')\n",
    "else:\n",
    "    THIS_SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "print(f\"Running in: {THIS_SCRIPT_DIR}\")\n",
    "\n",
    "def get_batch_dice_per_class(b_dice, class_tags, exclude_bg=True) -> dict:\n",
    "    score_dict = {}\n",
    "    for cls_idx, cls_tag in enumerate(class_tags):\n",
    "        if exclude_bg and cls_idx == 0:\n",
    "            continue\n",
    "\n",
    "        if torch.all(torch.isnan(b_dice[:,cls_idx])):\n",
    "            score = float('nan')\n",
    "        else:\n",
    "            score = np.nanmean(b_dice[:,cls_idx]).item()\n",
    "\n",
    "        score_dict[cls_tag] = score\n",
    "\n",
    "    return score_dict\n",
    "\n",
    "def get_batch_dice_over_all(b_dice, exclude_bg=True) -> float:\n",
    "\n",
    "    start_idx = 1 if exclude_bg else 0\n",
    "    if torch.all(torch.isnan(b_dice[:,start_idx:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,start_idx:]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5750fa2f-e706-4c65-ac09-c3ab8050b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParamMode(Enum):\n",
    "    INSTANCE_PARAMS = auto()\n",
    "    GRIDDED_INSTANCE_PARAMS = auto()\n",
    "    DISABLED = auto()\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "        See https://stackoverflow.com/questions/49901590/python-using-copy-deepcopy-on-dotdict\n",
    "    \"\"\"\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        try:\n",
    "            return self[item]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError from e\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "\n",
    "    'use_mind': False,\n",
    "    'epochs': 40,\n",
    "\n",
    "    'batch_size': 16,\n",
    "    'val_batch_size': 1,\n",
    "    'use_2d_normal_to': None,\n",
    "    'train_patchwise': True,\n",
    "\n",
    "    'dataset': 'crossmoda',\n",
    "    'reg_state': \"acummulate_convex_adam_FT2_MT1\",\n",
    "    'train_set_max_len': None,\n",
    "    'crop_3d_w_dim_range': (45, 95),\n",
    "    'crop_2d_slices_gt_num_threshold': 0,\n",
    "\n",
    "    'lr': 0.0005,\n",
    "    'use_cosine_annealing': True,\n",
    "\n",
    "    # Data parameter config\n",
    "    'data_param_mode': DataParamMode.INSTANCE_PARAMS,\n",
    "    'init_inst_param': 0.0,\n",
    "    'lr_inst_param': 0.1,\n",
    "    'use_risk_regularization': True,\n",
    "\n",
    "    'grid_size_y': 64,\n",
    "    'grid_size_x': 64,\n",
    "    # ),\n",
    "\n",
    "    'save_every': 200,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'do_plot': False,\n",
    "    'save_dp_figures': False,\n",
    "    'debug': True,\n",
    "    'wandb_mode': 'disabled', # e.g. online, disabled\n",
    "    'checkpoint_name': None,\n",
    "    'do_sweep': False,\n",
    "\n",
    "    'disturbance_mode': LabelDisturbanceMode.AFFINE,\n",
    "    'disturbance_strength': 2.,\n",
    "    'disturbed_percentage': .3,\n",
    "    'start_disturbing_after_ep': 0,\n",
    "\n",
    "    'start_dilate_kernel_sz': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de18b0cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2457023498.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_653929/2457023498.py\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    combined_label_data =\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(config):\n",
    "    if config.reg_state:\n",
    "        print(\"Loading registered data.\")\n",
    "\n",
    "        REG_STATES = [\n",
    "            \"combined\", \"best_1\", \"best_n\",\n",
    "            \"multiple\", \"mix_combined_best\",\n",
    "            \"best\", \"cummulate_combined_best\"]\n",
    "\n",
    "        # assert config.reg_state in REG_STATES, f\"Unknown registration version. Choose one of {REG_STATES}\"\n",
    "\n",
    "        if config.reg_state == \"mix_combined_best\":\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "\n",
    "            perm = np.random.permutation(len(loaded_identifier))\n",
    "            _clen = int(.5*len(loaded_identifier))\n",
    "            best_choice = perm[:_clen]\n",
    "            combined_choice = perm[_clen:]\n",
    "\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)[best_choice]\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)[combined_choice]\n",
    "            label_data = torch.zeros([107,128,128,128])\n",
    "            label_data[best_choice] = best_label_data\n",
    "            label_data[combined_choice] = combined_label_data\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"cummulate_combined_best\":\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)\n",
    "            label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier] + [_id+':var001' for _id in loaded_identifier]\n",
    "        \n",
    "        elif config.reg_state == \"acummulate_convex_adam_FT2_MT1\":\n",
    "            \n",
    "            label_data = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220113_crossmoda_convex/crossmoda_convex.pth\")\n",
    "            combined_label_data = \n",
    "            # best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)\n",
    "            # combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)\n",
    "            # label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            # loaded_identifier = [_id+':var000' for _id in loaded_identifier] + [_id+':var001' for _id in loaded_identifier]\n",
    "\n",
    "        else:\n",
    "            label_data = torch.cat([label_data_left[config.reg_state+'_all'][:44], label_data_right[config.reg_state+'_all'][:63]], dim=0)\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier]\n",
    "        raise(False)\n",
    "        modified_3d_label_override = {}\n",
    "        for idx, identifier in enumerate(loaded_identifier):\n",
    "            nl_id = int(re.findall(r'\\d+', identifier)[0])\n",
    "            var_id = int(re.findall(r':var(\\d+)$', identifier)[0])\n",
    "            lr_id = re.findall(r'([lr])\\.nii\\.gz', identifier)[0]\n",
    "\n",
    "            crossmoda_var_id = f\"{nl_id:03d}{lr_id}:var{var_id:03d}\"\n",
    "\n",
    "            modified_3d_label_override[crossmoda_var_id] = label_data[idx]\n",
    "\n",
    "        prevent_disturbance = True\n",
    "\n",
    "    else:\n",
    "        modified_3d_label_override = None\n",
    "        prevent_disturbance = False\n",
    "                  \n",
    "    if config.dataset == 'crossmoda':\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.\n",
    "        clsre = get_crossmoda_data_load_closure(\n",
    "            base_dir=\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "            domain='source', state='l4', use_additional_data=False,\n",
    "            size=(128,128,128), resample=True, normalize=True, crop_3d_w_dim_range=config.crop_3d_w_dim_range,\n",
    "            ensure_labeled_pairs=True,\n",
    "            debug=config.debug\n",
    "        )\n",
    "        training_dataset = CrossmodaHybridIdLoader(\n",
    "            clsre,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(f\"Nonzero slices: \" \\\n",
    "            f\"{sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)*100}%\"\n",
    "        )\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "    if config.dataset == 'ixi':\n",
    "        raise NotImplementedError()\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.\n",
    "        clsre = get_ixi_data_load_closure()\n",
    "        training_dataset = IXIHybridIdLoader(\n",
    "            clsre,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(f\"Nonzero slices: \" \\\n",
    "            f\"{sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)*100}%\"\n",
    "        )\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "\n",
    "    elif config['dataset'] == 'organmnist3d':\n",
    "        training_dataset = WrapperOrganMNIST3D(\n",
    "            split='train', root='./data/medmnist', download=True, normalize=True,\n",
    "            max_load_num=300, crop_3d_w_dim_range=None,\n",
    "            disturbed_idxs=None, use_2d_normal_to='W'\n",
    "        )\n",
    "        print(training_dataset.mnist_set.info)\n",
    "        print(\"Classes: \", training_dataset.label_tags)\n",
    "        print(\"Samples: \", len(training_dataset))\n",
    "\n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbc5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    _, all_labels, _ = training_dataset.get_data(use_2d_override=False)\n",
    "    print(all_labels.shape)\n",
    "    sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "    plt.xlabel(\"W\")\n",
    "    plt.ylabel(\"ground truth>0\")\n",
    "    plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c2075ba-4d51-4fc1-82fc-9193a7a86043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed_ids [('108r', 108, 'crossmoda_108_Label_r.nii.gz'), ('181r', 181, 'crossmoda_181_Label_r.nii.gz'), ('160r', 160, 'crossmoda_160_Label_r.nii.gz'), ('198r', 198, 'crossmoda_198_Label_r.nii.gz'), ('173r', 173, 'crossmoda_173_Label_r.nii.gz'), ('142r', 142, 'crossmoda_142_Label_r.nii.gz'), ('179r', 179, 'crossmoda_179_Label_r.nii.gz'), ('165r', 165, 'crossmoda_165_Label_r.nii.gz'), ('120r', 120, 'crossmoda_120_Label_r.nii.gz'), ('205r', 205, 'crossmoda_205_Label_r.nii.gz'), ('118r', 118, 'crossmoda_118_Label_r.nii.gz'), ('204r', 204, 'crossmoda_204_Label_r.nii.gz'), ('112r', 112, 'crossmoda_112_Label_r.nii.gz'), ('154r', 154, 'crossmoda_154_Label_r.nii.gz'), ('171r', 171, 'crossmoda_171_Label_r.nii.gz'), ('174r', 174, 'crossmoda_174_Label_r.nii.gz'), ('144r', 144, 'crossmoda_144_Label_r.nii.gz'), ('180r', 180, 'crossmoda_180_Label_r.nii.gz'), ('185r', 185, 'crossmoda_185_Label_r.nii.gz'), ('127r', 127, 'crossmoda_127_Label_r.nii.gz'), ('195r', 195, 'crossmoda_195_Label_r.nii.gz'), ('123r', 123, 'crossmoda_123_Label_r.nii.gz'), ('167r', 167, 'crossmoda_167_Label_r.nii.gz'), ('135r', 135, 'crossmoda_135_Label_r.nii.gz'), ('166r', 166, 'crossmoda_166_Label_r.nii.gz'), ('148r', 148, 'crossmoda_148_Label_r.nii.gz'), ('209r', 209, 'crossmoda_209_Label_r.nii.gz'), ('210r', 210, 'crossmoda_210_Label_r.nii.gz'), ('168r', 168, 'crossmoda_168_Label_r.nii.gz'), ('134r', 134, 'crossmoda_134_Label_r.nii.gz')]\n",
      "moving_ids [('023r', 23, 'crossmoda_23_Label_r.nii.gz'), ('037r', 37, 'crossmoda_37_Label_r.nii.gz'), ('027r', 27, 'crossmoda_27_Label_r.nii.gz'), ('032r', 32, 'crossmoda_32_Label_r.nii.gz'), ('016r', 16, 'crossmoda_16_Label_r.nii.gz'), ('012r', 12, 'crossmoda_12_Label_r.nii.gz'), ('045r', 45, 'crossmoda_45_Label_r.nii.gz'), ('003r', 3, 'crossmoda_3_Label_r.nii.gz'), ('100r', 100, 'crossmoda_100_Label_r.nii.gz'), ('035r', 35, 'crossmoda_35_Label_r.nii.gz'), ('044r', 44, 'crossmoda_44_Label_r.nii.gz'), ('011r', 11, 'crossmoda_11_Label_r.nii.gz'), ('029r', 29, 'crossmoda_29_Label_r.nii.gz'), ('046r', 46, 'crossmoda_46_Label_r.nii.gz'), ('004r', 4, 'crossmoda_4_Label_r.nii.gz'), ('048r', 48, 'crossmoda_48_Label_r.nii.gz'), ('042r', 42, 'crossmoda_42_Label_r.nii.gz'), ('033r', 33, 'crossmoda_33_Label_r.nii.gz'), ('039r', 39, 'crossmoda_39_Label_r.nii.gz'), ('025r', 25, 'crossmoda_25_Label_r.nii.gz'), ('019r', 19, 'crossmoda_19_Label_r.nii.gz'), ('040r', 40, 'crossmoda_40_Label_r.nii.gz'), ('001r', 1, 'crossmoda_1_Label_r.nii.gz'), ('024r', 24, 'crossmoda_24_Label_r.nii.gz'), ('030r', 30, 'crossmoda_30_Label_r.nii.gz'), ('047r', 47, 'crossmoda_47_Label_r.nii.gz'), ('036r', 36, 'crossmoda_36_Label_r.nii.gz'), ('050r', 50, 'crossmoda_50_Label_r.nii.gz'), ('101r', 101, 'crossmoda_101_Label_r.nii.gz'), ('017r', 17, 'crossmoda_17_Label_r.nii.gz')]\n",
      "30\n",
      "0\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_100_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_100_Label_r.nii.gz\n",
      "fixed: 108, moving: 100\n",
      "fixed: 108r, moving: 100r\n",
      "tensor([[0.9981, 0.7699, 0.5126]])\n",
      "\n",
      "60\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_11_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_11_Label_r.nii.gz\n",
      "fixed: 108, moving: 11\n",
      "fixed: 108r, moving: 011r\n",
      "tensor([[0.9960, 0.1148, 0.5134]])\n",
      "\n",
      "120\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_16_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_16_Label_r.nii.gz\n",
      "fixed: 108, moving: 16\n",
      "fixed: 108r, moving: 016r\n",
      "tensor([[0.9937, 0.4564, 0.4969]])\n",
      "\n",
      "180\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_19_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_19_Label_r.nii.gz\n",
      "fixed: 108, moving: 19\n",
      "fixed: 108r, moving: 019r\n",
      "tensor([[0.9969, 0.2635, 0.4819]])\n",
      "\n",
      "240\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_23_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_23_Label_r.nii.gz\n",
      "fixed: 108, moving: 23\n",
      "fixed: 108r, moving: 023r\n",
      "tensor([[0.9914, 0.4524, 0.6011]])\n",
      "\n",
      "300\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_25_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_25_Label_r.nii.gz\n",
      "fixed: 108, moving: 25\n",
      "fixed: 108r, moving: 025r\n",
      "tensor([[0.9913, 0.3185, 0.5087]])\n",
      "\n",
      "360\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_29_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_29_Label_r.nii.gz\n",
      "fixed: 108, moving: 29\n",
      "fixed: 108r, moving: 029r\n",
      "tensor([[0.9979, 0.7618, 0.5615]])\n",
      "\n",
      "420\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_32_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_32_Label_r.nii.gz\n",
      "fixed: 108, moving: 32\n",
      "fixed: 108r, moving: 032r\n",
      "tensor([[0.9885, 0.4076, 0.5442]])\n",
      "\n",
      "480\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_35_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_35_Label_r.nii.gz\n",
      "fixed: 108, moving: 35\n",
      "fixed: 108r, moving: 035r\n",
      "tensor([[0.9965, 0.2236, 0.4330]])\n",
      "\n",
      "540\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_37_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_37_Label_r.nii.gz\n",
      "fixed: 108, moving: 37\n",
      "fixed: 108r, moving: 037r\n",
      "tensor([[0.9955, 0.4167, 0.4929]])\n",
      "\n",
      "600\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_3_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_3_Label_r.nii.gz\n",
      "fixed: 108, moving: 3\n",
      "fixed: 108r, moving: 003r\n",
      "tensor([[0.9948, 0.4603, 0.3054]])\n",
      "\n",
      "660\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_42_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_42_Label_r.nii.gz\n",
      "fixed: 108, moving: 42\n",
      "fixed: 108r, moving: 042r\n",
      "tensor([[0.9880, 0.3614, 0.3709]])\n",
      "\n",
      "720\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_45_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_45_Label_r.nii.gz\n",
      "fixed: 108, moving: 45\n",
      "fixed: 108r, moving: 045r\n",
      "tensor([[0.9963, 0.6314, 0.5444]])\n",
      "\n",
      "780\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_47_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_47_Label_r.nii.gz\n",
      "fixed: 108, moving: 47\n",
      "fixed: 108r, moving: 047r\n",
      "tensor([[0.9969, 0.2157, 0.4140]])\n",
      "\n",
      "840\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_4_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_4_Label_r.nii.gz\n",
      "fixed: 108, moving: 4\n",
      "fixed: 108r, moving: 004r\n",
      "tensor([[0.9965, 0.2532, 0.3199]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/share/data_supergrover1/heinrich/crossmoda_convex/\"\n",
    "orig_path = \"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/L4_fine_localized_crop/\"\n",
    "# orig_path = \"/share/data_supergrover1/hansen/temp/crossMoDa/preprocessed_new/resampled/localised_crop/source_training/\"\n",
    "\n",
    "registered_files = glob.glob(data_path+\"*.nii.gz\")\n",
    "# print(registered_files)\n",
    "dice_files_right = torch.load(data_path+\"dice_files_right.pth\")\n",
    "# dice_files_left = torch.load(data_path+\"dice_files_left.pth\")\n",
    "\n",
    "fixed_files_right = dice_files_right['target_tumour_right']\n",
    "fixed_files_right = set([elem[0] for elem in fixed_files_right])\n",
    "# fixed_files_left = dice_files_left['target_tumour_left']\n",
    "# fixed_files_left = set([elem[0] for elem in fixed_files_left])\n",
    "\n",
    "moving_files_right = dice_files_right['source_tumour_right']\n",
    "moving_files_right = set([elem[0] for elem in moving_files_right])\n",
    "# moving_files_left = dice_files_left['source_tumour_left']\n",
    "# moving_files_left = set([elem[0] for elem in moving_files_left])\n",
    "\n",
    "# all_fixed_files = sorted(list(fixed_files_left.union(fixed_files_right)))\n",
    "all_fixed_files = list(fixed_files_right)\n",
    "\n",
    "# all_moving_files = sorted(list(moving_files_left.union(moving_files_right)))\n",
    "\n",
    "all_moving_files = list(moving_files_right)\n",
    "\n",
    "# if swap_dir:\n",
    "#     all_fixed_files, all_moving_files = all_moving_files, all_fixed_files\n",
    "\n",
    "# print(\"Fixed files lengths \", len(all_fixed_files))\n",
    "# print(\"Moving files lengths \", len(all_moving_files))\n",
    "\n",
    "def filter_ids(_file):\n",
    "    num_id, lr_id = re.findall(r\"(\\d{1,3})_Label_([lr])\", _file)[0]\n",
    "    _id = f'{int(num_id):03d}{lr_id}'\n",
    "    numeric_short = int(_id[:3])\n",
    "    return (_id, numeric_short, _file)\n",
    "\n",
    "fixed_ids = [filter_ids(f_name) for f_name in all_fixed_files]\n",
    "moving_ids = [filter_ids(f_name) for f_name in all_moving_files]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "used_fids_short = []\n",
    "used_mids_short = []\n",
    "\n",
    "def get_fixed_moving_num(_file):\n",
    "    fixed_num, moving_num = re.findall(r\"(\\d{1,3})_L(\\d{3})\", gz_file)[0]\n",
    "    fixed_num, moving_num = int(fixed_num), int(moving_num)\n",
    "    moving_num, fixed_num = fixed_num, moving_num\n",
    "    return fixed_num, moving_num\n",
    "\n",
    "for gz_file in registered_files:\n",
    "    fixed_num, moving_num = get_fixed_moving_num(gz_file)\n",
    "    used_fids_short.append(fixed_num)\n",
    "    used_mids_short.append(moving_num)\n",
    "\n",
    "used_fids_short = list(set(used_fids_short))\n",
    "used_mids_short = list(set(used_mids_short))\n",
    "\n",
    "fixed_ids = list(filter(lambda elem: elem[1] in used_fids_short, fixed_ids))\n",
    "moving_ids = list(filter(lambda elem: elem[1] in used_mids_short, moving_ids))\n",
    "\n",
    "print(\"fixed_ids\", fixed_ids)\n",
    "print(\"moving_ids\", moving_ids)\n",
    "\n",
    "orig_label_dict = {}\n",
    "\n",
    "# for _id, _, _file in fixed_ids: # TODO\n",
    "#     file_path = orig_path + \"__omitted_labels_target_training__/\" + _file\n",
    "#     file_path = file_path.replace(\"Label_l\", \"hrT2_l_Label\")\n",
    "#     file_path = file_path.replace(\"Label_r\", \"hrT2_r_Label\")\n",
    "#     if os.path.isfile(file_path):\n",
    "#         orig_label_dict[_id] = torch.tensor(nib.load(file_path).get_fdata())\n",
    "\n",
    "for _id, _, _file in moving_ids:\n",
    "\n",
    "    file_path = orig_path + \"source_training_labeled/\" + _file\n",
    "    file_path = file_path.replace(\"Label_l\", \"ceT1_l_Label\")\n",
    "    file_path = file_path.replace(\"Label_r\", \"ceT1_r_Label\")\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        orig_label_dict[_id] = torch.tensor(nib.load(file_path).get_fdata())\n",
    "\n",
    "warped_label_dict = {}\n",
    "for gz_file in registered_files:\n",
    "    warped_label = torch.tensor(nib.load(gz_file).get_fdata()).cuda()\n",
    "    fixed_num, moving_num = get_fixed_moving_num(gz_file)\n",
    "\n",
    "    fixed_id = list(filter(lambda elem: elem[1] == fixed_num, fixed_ids))[0][0]\n",
    "    moving_id = list(filter(lambda elem: elem[1] == moving_num, moving_ids))[0][0]\n",
    "    \n",
    "    dct = warped_label_dict.get(fixed_id, {})\n",
    "    dct[moving_id] = warped_label.to_sparse()\n",
    "    warped_label_dict[fixed_id] = dct\n",
    "    \n",
    "print(len(orig_label_dict))\n",
    "\n",
    "for idx, gz_file in enumerate(registered_files):\n",
    "    fixed_num, moving_num = get_fixed_moving_num(gz_file)\n",
    "\n",
    "    fixed_id, _, fixed_file = list(filter(lambda elem: elem[1] == fixed_num, fixed_ids))[0]\n",
    "    moving_id, _, moving_file = list(filter(lambda elem: elem[1] == moving_num, moving_ids))[0]\n",
    "    \n",
    "    dct = data_dict.get(fixed_id, {})\n",
    "    \n",
    "    orig_label = orig_label_dict[moving_id].cuda() # TODO\n",
    "    # orig_label = orig_label_dict[fixed_id].cuda()\n",
    "    warped_label = warped_label_dict[fixed_id][moving_id].to_dense()\n",
    "\n",
    "    dice = dice3d(F.one_hot(orig_label.long(), 3).unsqueeze(0), \n",
    "          F.one_hot(warped_label.long(), 3).unsqueeze(0), one_hot_torch_style=True)\n",
    "\n",
    "    dct[moving_id] = {\n",
    "        'warped_label': warped_label.to_sparse(),\n",
    "        'dice': dice\n",
    "    }\n",
    "    data_dict[fixed_id] = dct\n",
    "    \n",
    "    if idx % 60 == 0:\n",
    "    # if len(orig_label.unique()) != len(warped_label.unique()) or len(orig_label.unique()) < 2:\n",
    "        print(idx)\n",
    "        print(orig_label.unique())\n",
    "        print(warped_label.unique())\n",
    "        print(\"Registred file: \", gz_file)\n",
    "        print(\"Fixed file: \", fixed_file)\n",
    "        print(\"Moving file: \", moving_file)\n",
    "        print(f\"fixed: {fixed_num}, moving: {moving_num}\")\n",
    "        print(f\"fixed: {fixed_id}, moving: {moving_id}\")\n",
    "        print(dice)\n",
    "        print()\n",
    "    \n",
    "# torch.save(data_dict, THIS_SCRIPT_DIR+\"/data/crossmoda_convex.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a892a0-53a9-42e5-a15d-c02547aad290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique moving_ids 30 [('001r', 1, 'crossmoda_1_Label_r.nii.gz'), ('003r', 3, 'crossmoda_3_Label_r.nii.gz'), ('004r', 4, 'crossmoda_4_Label_r.nii.gz'), ('011r', 11, 'crossmoda_11_Label_r.nii.gz'), ('012r', 12, 'crossmoda_12_Label_r.nii.gz'), ('016r', 16, 'crossmoda_16_Label_r.nii.gz'), ('017r', 17, 'crossmoda_17_Label_r.nii.gz'), ('019r', 19, 'crossmoda_19_Label_r.nii.gz'), ('023r', 23, 'crossmoda_23_Label_r.nii.gz'), ('024r', 24, 'crossmoda_24_Label_r.nii.gz'), ('025r', 25, 'crossmoda_25_Label_r.nii.gz'), ('027r', 27, 'crossmoda_27_Label_r.nii.gz'), ('029r', 29, 'crossmoda_29_Label_r.nii.gz'), ('030r', 30, 'crossmoda_30_Label_r.nii.gz'), ('032r', 32, 'crossmoda_32_Label_r.nii.gz'), ('033r', 33, 'crossmoda_33_Label_r.nii.gz'), ('035r', 35, 'crossmoda_35_Label_r.nii.gz'), ('036r', 36, 'crossmoda_36_Label_r.nii.gz'), ('037r', 37, 'crossmoda_37_Label_r.nii.gz'), ('039r', 39, 'crossmoda_39_Label_r.nii.gz'), ('040r', 40, 'crossmoda_40_Label_r.nii.gz'), ('042r', 42, 'crossmoda_42_Label_r.nii.gz'), ('044r', 44, 'crossmoda_44_Label_r.nii.gz'), ('045r', 45, 'crossmoda_45_Label_r.nii.gz'), ('046r', 46, 'crossmoda_46_Label_r.nii.gz'), ('047r', 47, 'crossmoda_47_Label_r.nii.gz'), ('048r', 48, 'crossmoda_48_Label_r.nii.gz'), ('050r', 50, 'crossmoda_50_Label_r.nii.gz'), ('100r', 100, 'crossmoda_100_Label_r.nii.gz'), ('101r', 101, 'crossmoda_101_Label_r.nii.gz')]\n",
      "unique fixed_ids 30 [('108r', 108, 'crossmoda_108_Label_r.nii.gz'), ('112r', 112, 'crossmoda_112_Label_r.nii.gz'), ('118r', 118, 'crossmoda_118_Label_r.nii.gz'), ('120r', 120, 'crossmoda_120_Label_r.nii.gz'), ('123r', 123, 'crossmoda_123_Label_r.nii.gz'), ('127r', 127, 'crossmoda_127_Label_r.nii.gz'), ('134r', 134, 'crossmoda_134_Label_r.nii.gz'), ('135r', 135, 'crossmoda_135_Label_r.nii.gz'), ('142r', 142, 'crossmoda_142_Label_r.nii.gz'), ('144r', 144, 'crossmoda_144_Label_r.nii.gz'), ('148r', 148, 'crossmoda_148_Label_r.nii.gz'), ('154r', 154, 'crossmoda_154_Label_r.nii.gz'), ('160r', 160, 'crossmoda_160_Label_r.nii.gz'), ('165r', 165, 'crossmoda_165_Label_r.nii.gz'), ('166r', 166, 'crossmoda_166_Label_r.nii.gz'), ('167r', 167, 'crossmoda_167_Label_r.nii.gz'), ('168r', 168, 'crossmoda_168_Label_r.nii.gz'), ('171r', 171, 'crossmoda_171_Label_r.nii.gz'), ('173r', 173, 'crossmoda_173_Label_r.nii.gz'), ('174r', 174, 'crossmoda_174_Label_r.nii.gz'), ('179r', 179, 'crossmoda_179_Label_r.nii.gz'), ('180r', 180, 'crossmoda_180_Label_r.nii.gz'), ('181r', 181, 'crossmoda_181_Label_r.nii.gz'), ('185r', 185, 'crossmoda_185_Label_r.nii.gz'), ('195r', 195, 'crossmoda_195_Label_r.nii.gz'), ('198r', 198, 'crossmoda_198_Label_r.nii.gz'), ('204r', 204, 'crossmoda_204_Label_r.nii.gz'), ('205r', 205, 'crossmoda_205_Label_r.nii.gz'), ('209r', 209, 'crossmoda_209_Label_r.nii.gz'), ('210r', 210, 'crossmoda_210_Label_r.nii.gz')]\n",
      "Quantile tumour:  [0.000 0.087 0.204 0.323 0.463 0.903]\n",
      "Quantile cochlea:  [0.127 0.367 0.438 0.493 0.556 0.694]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"swapdir\", swap_dir)\n",
    "print(\"unique moving_ids\", len(moving_ids), sorted(moving_ids))\n",
    "print(\"unique fixed_ids\", len(fixed_ids), sorted(fixed_ids))\n",
    "# ld_data_dict = torch.load(THIS_SCRIPT_DIR+\"/data/crossmoda_convex.pth\")\n",
    "ld_data_dict = data_dict\n",
    "dices = []\n",
    "for fid, fx in ld_data_dict.items():\n",
    "    for mid, mov in fx.items():\n",
    "        dices.append(mov['dice'])\n",
    "\n",
    "tens_dices = torch.cat(dices)\n",
    "\n",
    "plt.plot(sorted(tens_dices[:,0]), label='bg')\n",
    "plt.plot(sorted(tens_dices[:,1]), label='tumour')\n",
    "plt.plot(sorted(tens_dices[:,2]), label='cochlea')\n",
    "plt.title(\"Sorted dices\")\n",
    "plt.legend()\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "print(\"Quantile tumour: \", np.quantile(tens_dices[:,1], [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]))\n",
    "print(\"Quantile cochlea: \", np.quantile(tens_dices[:,2], [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "285b9633-121d-4a82-8462-b3746824df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6564920544624329, 0.6615490913391113, 0.66754150390625, 0.6853525638580322, 0.6961158514022827, 0.7095019817352295, 0.7232212424278259, 0.7998449206352234, 0.8001408576965332, 0.8199121356010437]\n",
      "torch.Size([10, 128, 128, 128])\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "gt = []\n",
    "wpl = []\n",
    "tumour_dices = []\n",
    "comb_keys = []\n",
    "\n",
    "for key in fixed_keys:\n",
    "    for mov_key in list(warped_label_dict[key].keys()):\n",
    "        warped_label = warped_label_dict[key][mov_key]\n",
    "        tumour_dice = ld_data_dict[key][mov_key]['dice'][0][1].item()\n",
    "        orig_label = orig_label_dict[key]\n",
    "        gt.append(orig_label)\n",
    "        wpl.append(warped_label)\n",
    "        tumour_dices.append(tumour_dice)\n",
    "        comb_keys.append(f\"f{key}_m{mov_key}\")\n",
    "\n",
    "wpl = torch.stack(wpl).to_dense()\n",
    "gt = torch.stack(gt)\n",
    "srt = torch.tensor(np.argsort(tumour_dices)).long()[-10:]\n",
    "tumour_dices = [tumour_dices[idx] for idx in srt]\n",
    "wpl = wpl[srt]\n",
    "gt = gt[srt]\n",
    "\n",
    "comb_keys = [comb_keys[idx] for idx in srt]\n",
    "print(tumour_dices)\n",
    "print(wpl.shape)\n",
    "overlay_text_list = [f\"{dc:.2f}_{key}\" for key, dc in zip(comb_keys, tumour_dices)]\n",
    "\n",
    "frame_elements = [idx%2==0 for idx in range(len(overlay_text_list))]\n",
    "\n",
    "visualize_seg(in_type=\"batch_3D\", reduce_dim=\"W\",\n",
    "    img=gt.unsqueeze(1), # Expert label in BW\n",
    "    seg=wpl.long().cpu(), # Prediction in blue\n",
    "    ground_truth=gt.long(), # Modified label in red\n",
    "    # crop_to_non_zero_seg=True,\n",
    "    crop_to_non_zero_gt=True,\n",
    "    alpha_seg = .5,\n",
    "    alpha_gt = .5,\n",
    "    n_per_row=35,\n",
    "    overlay_text=overlay_text_list,\n",
    "    annotate_color=(0,255,255),\n",
    "    frame_elements=frame_elements,\n",
    "    file_path=\"out.png\",\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0adbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    # Print bare 2D data\n",
    "    # print(\"Displaying 2D bare sample\")\n",
    "    # for img, label in zip(training_dataset.img_data_2d.values(),\n",
    "    #                       training_dataset.label_data_2d.values()):\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=img.unsqueeze(0),\n",
    "    #                 ground_truth=label,\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "    # Print transformed 2D data\n",
    "    training_dataset.train(use_modified=False, augment=True)\n",
    "    print(training_dataset.disturbed_idxs)\n",
    "\n",
    "    print(\"Displaying 2D training sample\")\n",
    "    for dist_stren in [0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 5.0]:\n",
    "        print(dist_stren)\n",
    "        training_dataset.disturb_idxs(list(range(0,20,2)),\n",
    "            disturbance_mode=LabelDisturbanceMode.AFFINE,\n",
    "            disturbance_strength=dist_stren\n",
    "        )\n",
    "        img_stack = []\n",
    "        label_stack = []\n",
    "        mod_label_stack = []\n",
    "\n",
    "        for sample in (training_dataset[idx] for idx in range(20)):\n",
    "            img_stack.append(sample['image'])\n",
    "            label_stack.append(sample['label'])\n",
    "            mod_label_stack.append(sample['modified_label'])\n",
    "\n",
    "        # Change label num == hue shift for display\n",
    "        img_stack = torch.stack(img_stack).unsqueeze(1)\n",
    "        label_stack = torch.stack(label_stack)\n",
    "        mod_label_stack = torch.stack(mod_label_stack)\n",
    "\n",
    "        mod_label_stack*=4\n",
    "\n",
    "        visualize_seg(in_type=\"batch_2D\",\n",
    "            img=img_stack,\n",
    "            # ground_truth=label_stack,\n",
    "            seg=(mod_label_stack-label_stack).abs(),\n",
    "            # crop_to_non_zero_gt=True,\n",
    "            crop_to_non_zero_seg=True,\n",
    "            alpha_seg = .6,\n",
    "            file_path=f'out{dist_stren}.png'\n",
    "        )\n",
    "\n",
    "    # Print transformed 3D data\n",
    "    # training_dataset.train()\n",
    "    # print(\"Displaying 3D training sample\")\n",
    "    # leng = 1# training_dataset.__len__(use_2d_override=False)\n",
    "    # for sample in (training_dataset.get_3d_item(idx) for idx in range(leng)):\n",
    "    #     # training_dataset.set_dilate_kernel_size(1)\n",
    "    #     visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "    #                 img=sample['image'].unsqueeze(0),\n",
    "    #                 ground_truth=sample['label'],\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "#         # training_dataset.set_dilate_kernel_size(7)\n",
    "#         display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "#                     img=sample['image'].unsqueeze(0),\n",
    "#                     ground_truth=sample['modified_label'],\n",
    "#                     crop_to_non_zero_gt=True,\n",
    "#                     alpha_gt = .3)\n",
    "\n",
    "    for sidx in [0,]:\n",
    "        print(f\"Sample {sidx}:\")\n",
    "\n",
    "        training_dataset.eval()\n",
    "        sample_eval = training_dataset.get_3d_item(sidx)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "\n",
    "        training_dataset.train()\n",
    "        print(\"Train sample with ground-truth overlay\")\n",
    "        sample_train = training_dataset.get_3d_item(sidx)\n",
    "        print(sample_train['label'].unique())\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_train['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_train['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt=.3)\n",
    "\n",
    "        print(\"Eval/train diff with diff overlay\")\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=(sample_eval['image'] - sample_train['image']).unsqueeze(0),\n",
    "                    ground_truth=(sample_eval['label'] - sample_train['label']).clamp(min=0),\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "    train_plotset = (training_dataset.get_3d_item(idx) for idx in (55, 81, 63))\n",
    "    for sample in train_plotset:\n",
    "        print(f\"Sample {sample['dataset_idx']}:\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .6)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use module.named_modules() to retrieve valid keychains for layers.\n",
    "       e.g.\n",
    "       first_keychain = list(module.keys())[0]\n",
    "       new_first_replacee = torch.nn.Conv1d(1,2,3)\n",
    "       set_module(first_keychain, torch.nn.Conv1d(1,2,3))\n",
    "    \"\"\"\n",
    "\n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(_path, **statefuls):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "    _path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for name, stful in statefuls.items():\n",
    "        if stful != None:\n",
    "            torch.save(stful.state_dict(), _path.joinpath(name+'.pth'))\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config, dataset_len, num_classes, THIS_SCRIPT_DIR, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        # Use vanilla torch model\n",
    "        lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(\n",
    "            pretrained=False, progress=True, num_classes=num_classes\n",
    "        )\n",
    "        set_module(lraspp, 'backbone.0.0',\n",
    "            torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2),\n",
    "                            padding=(1, 1), bias=False)\n",
    "        )\n",
    "    else:\n",
    "        # Use custom 3d model\n",
    "        lraspp = MobileNet_LRASPP_3D(\n",
    "            in_num=in_channels, num_classes=num_classes,\n",
    "            use_checkpointing=True\n",
    "        )\n",
    "\n",
    "    # lraspp.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "    lraspp.to(device)\n",
    "    print(f\"Param count lraspp: {sum(p.numel() for p in lraspp.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(lraspp.parameters(), lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # Add data paramters embedding and optimizer\n",
    "    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len, 1, sparse=True)\n",
    "        # p_offset = torch.zeros(1, layout=torch.strided, requires_grad=True)\n",
    "        # p_offset.grad = torch.sparse_coo_tensor([[0]], 1., size=(1,))\n",
    "\n",
    "        # embedding.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "        # embedding.sigmoid_offset.register_hook(lambda grad: torch.sparse_coo_tensor([[0]], grad, size=(1,)))\n",
    "        embedding = embedding.to(device)\n",
    "\n",
    "    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len*config.grid_size_y*config.grid_size_x, 1, sparse=True).to(device)\n",
    "    else:\n",
    "        embedding = None\n",
    "\n",
    "\n",
    "    if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "        optimizer_dp = torch.optim.SparseAdam(\n",
    "            embedding.parameters(), lr=config.lr_inst_param,\n",
    "            betas=(0.9, 0.999), eps=1e-08)\n",
    "        torch.nn.init.normal_(embedding.weight.data, mean=config.init_inst_param, std=0.00)\n",
    "\n",
    "        if _path and _path.is_dir():\n",
    "            print(f\"Loading embedding and dp_optimizer from {_path}\")\n",
    "            optimizer_dp.load_state_dict(torch.load(_path.joinpath('optimizer_dp.pth'), map_location=device))\n",
    "            embedding.load_state_dict(torch.load(_path.joinpath('embedding.pth'), map_location=device))\n",
    "\n",
    "        print(f\"Param count embedding: {sum(p.numel() for p in embedding.parameters())}\")\n",
    "\n",
    "    else:\n",
    "        optimizer_dp = None\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading lr-aspp model, optimizers and grad scalers from {_path}\")\n",
    "        lraspp.load_state_dict(torch.load(_path.joinpath('lraspp.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "\n",
    "    return (lraspp, optimizer, optimizer_dp, embedding, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f1722",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "\n",
    "\n",
    "\n",
    "def save_parameter_figure(_path, title, text, parameters, reweighted_parameters, dices):\n",
    "    # Show weights and weights with compensation\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12, 4), dpi=80)\n",
    "    sc1 = axs[0].scatter(\n",
    "        range(len(parameters)),\n",
    "        parameters.cpu().detach(), c=dices,s=1, cmap='plasma', vmin=0., vmax=1.)\n",
    "    sc2 = axs[1].scatter(\n",
    "        range(len(reweighted_parameters)),\n",
    "        reweighted_parameters.cpu().detach(), s=1,c=dices, cmap='plasma', vmin=0., vmax=1.)\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.text(0, 0, text)\n",
    "    axs[0].set_title('Bare parameters')\n",
    "    axs[1].set_title('Reweighted parameters')\n",
    "    axs[0].set_ylim(-10, 10)\n",
    "    axs[1].set_ylim(-3, 1)\n",
    "    plt.colorbar(sc2)\n",
    "    plt.savefig(_path)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "\n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "\n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "\n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "\n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio\n",
    "\n",
    "def log_data_parameter_stats(log_path, epx, data_parameters):\n",
    "    \"\"\"Log stats for data parameters on wandb.\"\"\"\n",
    "    wandb.log({f'{log_path}/highest': torch.max(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/lowest': torch.min(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/mean': torch.mean(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/std': torch.std(data_parameters).item()}, step=epx)\n",
    "\n",
    "\n",
    "\n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_class_dices(log_prefix, log_postfix, class_dices, log_idx):\n",
    "    if not class_dices:\n",
    "        return\n",
    "\n",
    "    for cls_name in class_dices[0].keys():\n",
    "        log_path = f\"{log_prefix}{cls_name}{log_postfix}\"\n",
    "\n",
    "        cls_dices = list(map(lambda dct: dct[cls_name], class_dices))\n",
    "        mean_per_class =np.nanmean(cls_dices)\n",
    "        print(log_path, f\"{mean_per_class*100:.2f}%\")\n",
    "        wandb.log({log_path: mean_per_class}, step=log_idx)\n",
    "\n",
    "def CELoss(logits, targets, bin_weight=None):\n",
    "    # -logits[0,targets[0,0,0],0,0]+torch.log(logits[0,:,0,0].exp().sum())\n",
    "    targets = torch.nn.functional.one_hot(targets).permute(0,3,1,2)\n",
    "    loss = -targets*F.log_softmax(logits, 1)\n",
    "\n",
    "    if bin_weight is not None:\n",
    "        brdcast_shape = torch.Size((loss.shape[0], loss.shape[1], *((loss.dim()-2)*[1])))\n",
    "        inv_weight = (bin_weight+np.exp(1)).log()+np.exp(1)\n",
    "        inv_weight = inv_weight/inv_weight.mean()\n",
    "        loss = inv_weight.view(brdcast_shape)*loss\n",
    "\n",
    "    return loss.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embedding_idxs(idxs, grid_size_y, grid_size_x):\n",
    "    with torch.no_grad():\n",
    "        t_sz = grid_size_y * grid_size_x\n",
    "        return ((idxs*t_sz).long().repeat(t_sz).view(t_sz, idxs.numel())+torch.tensor(range(t_sz)).to(idxs).view(t_sz,1)).permute(1,0).reshape(-1)\n",
    "\n",
    "def inference_wrap(lraspp, img, use_2d, use_mind):\n",
    "    with torch.inference_mode():\n",
    "        b_img = img.unsqueeze(0).unsqueeze(0).float()\n",
    "        if use_2d and use_mind:\n",
    "            # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "            b_img = mindssc(b_img.unsqueeze(0)).squeeze(2)\n",
    "        elif not use_2d and use_mind:\n",
    "            # MIND 3D in Bx1xDxHxW out BxMINDxDxHxW\n",
    "            b_img = mindssc(b_img)\n",
    "        elif use_2d or not use_2d:\n",
    "            # 2D Bx1xHxW\n",
    "            # 3D out Bx1xDxHxW\n",
    "            pass\n",
    "\n",
    "        b_out = lraspp(b_img)['out']\n",
    "        b_out = b_out.argmax(1)\n",
    "        return b_out\n",
    "\n",
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "\n",
    "    fold_iter = enumerate(kf.split(training_dataset))\n",
    "\n",
    "    if config.get('fold_override', None):\n",
    "        selected_fold = config.get('fold_override', 0)\n",
    "        fold_iter = list(fold_iter)[selected_fold:selected_fold+1]\n",
    "    elif config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "\n",
    "    if config.wandb_mode != 'disabled':\n",
    "        # Log dataset info\n",
    "        training_dataset.eval()\n",
    "        dataset_info = [[smp['dataset_idx'], smp['id'], smp['image_path'], smp['label_path']] \\\n",
    "                        for smp in training_dataset]\n",
    "        wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', 'id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    "\n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        # Training happens in 2D, validation happens in 3D:\n",
    "        # Read 2D dataset idxs which are used for training,\n",
    "        # get their 3D super-ids and substract these from all 3D ids to get val_3d_idxs\n",
    "\n",
    "        if config.use_2d_normal_to is not None:\n",
    "            n_dims = (-2,-1)\n",
    "            trained_3d_dataset_ids = training_dataset.get_3d_from_2d_identifiers(train_idxs, 'id')\n",
    "            # trained_3d_trained_ids = training_dataset.switch_3d_identifiers(trained_3d_dataset_idxs)\n",
    "            all_3d_ids = training_dataset.get_3d_ids()\n",
    "            val_3d_ids = set(all_3d_ids) - set(trained_3d_dataset_ids)\n",
    "            val_3d_idxs = list({\n",
    "                training_dataset.extract_short_3d_id(_id):idx \\\n",
    "                    for idx, _id in enumerate(all_3d_ids) if _id in val_3d_ids}.values())\n",
    "        else:\n",
    "            n_dims = (-3,-2,-1)\n",
    "            val_3d_idxs = val_idxs\n",
    "        print(\"Will run validation with these 3D samples:\", val_3d_idxs)\n",
    "\n",
    "        _, _, all_modified_segs = training_dataset.get_data()\n",
    "\n",
    "        non_empty_train_idxs = train_idxs[(all_modified_segs[train_idxs].sum(dim=n_dims) > 0)]\n",
    "\n",
    "        ### Disturb dataset (only non-emtpy idxs)###\n",
    "        proposed_disturbed_idxs = np.random.choice(non_empty_train_idxs, size=int(len(non_empty_train_idxs)*config.disturbed_percentage), replace=False)\n",
    "        proposed_disturbed_idxs = torch.tensor(proposed_disturbed_idxs)\n",
    "        training_dataset.disturb_idxs(proposed_disturbed_idxs,\n",
    "            disturbance_mode=config.disturbance_mode,\n",
    "            disturbance_strength=config.disturbance_strength\n",
    "        )\n",
    "\n",
    "        disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "        disturbed_bool_vect[training_dataset.disturbed_idxs] = 1.\n",
    "\n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, training_dataset.disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(training_dataset.disturbed_idxs))\n",
    "\n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in training_dataset.disturbed_idxs])},\n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "\n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels = 12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "\n",
    "        class_weights = 1/(torch.bincount(all_modified_segs.reshape(-1).long())).float().pow(.35)\n",
    "        class_weights /= class_weights.mean()\n",
    "\n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "            sampler=train_subsampler, pin_memory=True, drop_last=False,\n",
    "            # collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "        )\n",
    "\n",
    "        training_dataset.set_augment_at_collate(False)\n",
    "\n",
    "#         val_dataloader = DataLoader(training_dataset, batch_size=config.val_batch_size,\n",
    "#                                     sampler=val_subsampler, pin_memory=True, drop_last=False)\n",
    "\n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        epx_start = config.get('checkpoint_epx', 0)\n",
    "\n",
    "        if config.checkpoint_name:\n",
    "            # Load from checkpoint\n",
    "            _path = f\"{config.mdl_save_prefix}/{config.checkpoint_name}_fold{fold_idx}_epx{epx_start}\"\n",
    "        else:\n",
    "            _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx_start}\"\n",
    "\n",
    "        (lraspp, optimizer, optimizer_dp, embedding, scaler) = get_model(config, len(training_dataset), len(training_dataset.label_tags),\n",
    "            THIS_SCRIPT_DIR=THIS_SCRIPT_DIR, _path=_path, device='cuda')\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=500, T_mult=2)\n",
    "\n",
    "        if optimizer_dp:\n",
    "            scheduler_dp = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer_dp, T_0=500, T_mult=2)\n",
    "        else:\n",
    "            scheduler_dp = None\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare corr coefficient scoring\n",
    "        training_dataset.eval(use_modified=True)\n",
    "        wise_labels, mod_labels = list(zip(*[(sample['label'], sample['modified_label']) \\\n",
    "            for sample in training_dataset]))\n",
    "        wise_labels, mod_labels = torch.stack(wise_labels), torch.stack(mod_labels)\n",
    "\n",
    "        dice_func = dice2d if config.use_2d_normal_to is not None else dice3d\n",
    "\n",
    "        wise_dice = dice_func(\n",
    "            torch.nn.functional.one_hot(wise_labels, len(training_dataset.label_tags)),\n",
    "            torch.nn.functional.one_hot(mod_labels, len(training_dataset.label_tags)),\n",
    "            one_hot_torch_style=True, nan_for_unlabeled_target=False\n",
    "        )\n",
    "\n",
    "        gt_num = (mod_labels > 0).sum(dim=n_dims)\n",
    "        t_metric = (gt_num+np.exp(1)).log()+np.exp(1)\n",
    "\n",
    "        if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "            union_wise_mod_label = torch.logical_or(wise_labels, mod_labels)\n",
    "            union_wise_mod_label = union_wise_mod_label.cuda()\n",
    "\n",
    "        class_weights = class_weights.cuda()\n",
    "        gt_num = gt_num.cuda()\n",
    "        t_metric = t_metric.cuda()\n",
    "\n",
    "        for epx in range(epx_start, config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "\n",
    "            lraspp.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            training_dataset.train(use_modified=(epx >= config.start_disturbing_after_ep))\n",
    "            wandb.log({\"use_modified\": float(training_dataset.use_modified)}, step=global_idx)\n",
    "\n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            class_dices = []\n",
    "\n",
    "            # Load data\n",
    "            for batch_idx, batch in enumerate(train_dataloader):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if optimizer_dp:\n",
    "                    optimizer_dp.zero_grad()\n",
    "\n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "                b_spat_aug_grid = batch['spat_augment_grid']\n",
    "\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "                b_img = b_img.float()\n",
    "\n",
    "                b_img = b_img.cuda()\n",
    "                b_seg_modified = b_seg_modified.cuda()\n",
    "                b_idxs_dataset = b_idxs_dataset.cuda()\n",
    "                b_seg = b_seg.cuda()\n",
    "                b_spat_aug_grid = b_spat_aug_grid.cuda()\n",
    "\n",
    "                if training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "                elif not training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 3D\n",
    "                    b_img = mindssc(b_img.unsqueeze(1))\n",
    "                else:\n",
    "                    b_img = b_img.unsqueeze(1)\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=True):\n",
    "                    assert b_img.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input image for model must be {len(n_dims)+2}D: BxCxSPATIAL but is {b_img.shape}\"\n",
    "\n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    ### Calculate loss ###\n",
    "                    assert logits.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input shape for loss must be BxNUM_CLASSESxSPATIAL but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == len(n_dims)+1, \\\n",
    "                        f\"Target shape for loss must be BxSPATIAL but is {b_seg_modified.shape}\"\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                        # batch_bins = torch.zeros([len(b_idxs_dataset), len(training_dataset.label_tags)]).to(logits.device)\n",
    "                        # bin_list = [slc.view(-1).bincount() for slc in b_seg_modified]\n",
    "                        # for b_idx, _bins in enumerate(bin_list):\n",
    "                        #     batch_bins[b_idx][:len(_bins)] = _bins\n",
    "                        # loss = CELoss(logits, b_seg_modified, bin_weight=batch_bins)\n",
    "\n",
    "                        loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified)\n",
    "                        loss = loss.mean(n_dims)\n",
    "\n",
    "                        bare_weight = embedding(b_idxs_dataset).squeeze()\n",
    "\n",
    "                        weight = torch.sigmoid(bare_weight)\n",
    "                        weight = weight/weight.mean()\n",
    "\n",
    "                        # This improves scores significantly: Reweight with log(gt_numel)\n",
    "                        weight = weight/t_metric[b_idxs_dataset]\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = logits.argmax(1)\n",
    "\n",
    "                        if config.use_risk_regularization:\n",
    "                            p_pred_num = (logits_for_score > 0).sum(dim=n_dims).detach()\n",
    "                            risk_regularization = -weight*p_pred_num/(logits_for_score.shape[-2]*logits_for_score.shape[-1])\n",
    "                            loss = (loss*weight).sum() + risk_regularization.sum()\n",
    "                        else:\n",
    "                            loss = (loss*weight).sum()\n",
    "\n",
    "                    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                        loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified)\n",
    "                        m_dp_idxs = map_embedding_idxs(b_idxs_dataset, config.grid_size_y, config.grid_size_x)\n",
    "                        weight = embedding(m_dp_idxs)\n",
    "                        weight = weight.reshape(-1, config.grid_size_y, config.grid_size_x)\n",
    "                        weight = weight.unsqueeze(1)\n",
    "                        weight = torch.nn.functional.interpolate(\n",
    "                            weight,\n",
    "                            size=(b_seg_modified.shape[-2:]),\n",
    "                            mode='bilinear',\n",
    "                            align_corners=True\n",
    "                        )\n",
    "                        weight = weight/weight.mean()\n",
    "                        weight = F.grid_sample(weight, b_spat_aug_grid,\n",
    "                            padding_mode='border', align_corners=False)\n",
    "                        loss = (loss.unsqueeze(1)*weight).sum()\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = (logits*weight).argmax(1)\n",
    "\n",
    "                    else:\n",
    "                        loss = nn.CrossEntropyLoss(class_weights)(logits, b_seg_modified)\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = logits.argmax(1)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                    scaler.step(optimizer_dp)\n",
    "\n",
    "                scaler.update()\n",
    "\n",
    "                epx_losses.append(loss.item())\n",
    "\n",
    "                # Calculate dice score\n",
    "                b_dice = dice_func(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(b_seg, len(training_dataset.label_tags)), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_over_all(\n",
    "                    b_dice, exclude_bg=True))\n",
    "                class_dices.append(get_batch_dice_per_class(\n",
    "                    b_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                ###  Scheduler management ###\n",
    "                if config.use_cosine_annealing:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if config.save_dp_figures and batch_idx % 10 == 0:\n",
    "                    # Output data parameter figure\n",
    "                    train_params = embedding.weight[train_idxs].squeeze()\n",
    "                    # order = np.argsort(train_params.cpu().detach()) # Order by DP value\n",
    "                    order = torch.arange(len(train_params))\n",
    "                    wise_corr_coeff = np.corrcoef(train_params.cpu().detach(), wise_dice[train_idxs][:,1].cpu().detach())[0,1]\n",
    "                    dp_figure_path = Path(f\"data/output_figures/{wandb.run.name}_fold{fold_idx}/dp_figure_epx{epx:03d}_batch{batch_idx:03d}.png\")\n",
    "                    dp_figure_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    save_parameter_figure(dp_figure_path, wandb.run.name, f\"corr. coeff. DP vs. dice(expert label, train gt): {wise_corr_coeff:4f}\",\n",
    "                        train_params[order], train_params[order]/t_metric[train_idxs][order], dices=wise_dice[train_idxs][:,1][order])\n",
    "\n",
    "                if config.debug:\n",
    "                    break\n",
    "\n",
    "            ### Logging ###\n",
    "            print(f\"### Log epoch {epx} @ {time.time()-t0:.2f}s\")\n",
    "            print(\"### Training\")\n",
    "            ### Log wandb data ###\n",
    "            # Log the epoch idx per fold - so we can recover the diagram by setting\n",
    "            # ref_epoch_idx as x-axis in wandb interface\n",
    "            wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "\n",
    "            mean_loss = torch.tensor(epx_losses).mean()\n",
    "            wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "\n",
    "            mean_dice = np.nanmean(dices)\n",
    "            print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "            wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "\n",
    "            log_class_dices(\"scores/dice_mean_\", f\"_fold{fold_idx}\", class_dices, global_idx)\n",
    "\n",
    "            # Log data parameters of disturbed samples\n",
    "            if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                # Calculate dice score corr coeff (unknown to network)\n",
    "                train_params = embedding.weight[train_idxs].squeeze()\n",
    "                order = np.argsort(train_params.cpu().detach())\n",
    "                wise_corr_coeff = np.corrcoef(train_params[order].cpu().detach(), wise_dice[train_idxs][:,1][order].cpu().detach())[0,1]\n",
    "\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/wise_corr_coeff_fold{fold_idx}': wise_corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                print(f'data_parameters/wise_corr_coeff_fold{fold_idx}', f\"{wise_corr_coeff:.2f}\")\n",
    "\n",
    "                # Log stats of data parameters and figure\n",
    "                log_data_parameter_stats(f'data_parameters/iter_stats_fold{fold_idx}', global_idx, embedding.weight.data)\n",
    "\n",
    "                # Map gridded instance parameters\n",
    "                if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                    m_tr_idxs = map_embedding_idxs(train_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                    ).cuda()\n",
    "                    masks = union_wise_mod_label[train_idxs].float()\n",
    "                    masked_weights = torch.nn.functional.interpolate(\n",
    "                        embedding(m_tr_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                        size=(masks.shape[-2:]),\n",
    "                        mode='bilinear',\n",
    "                        align_corners=True\n",
    "                    ).squeeze(1) * masks\n",
    "                    masked_weights[masked_weights==0.] = float('nan')\n",
    "\n",
    "            if (epx % config.save_every == 0 and epx != 0) \\\n",
    "                or (epx+1 == config.epochs):\n",
    "                _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx}\"\n",
    "                save_model(\n",
    "                    _path,\n",
    "                    lraspp=lraspp,\n",
    "                    optimizer=optimizer, optimizer_dp=optimizer_dp,\n",
    "                    scheduler=scheduler, scheduler_dp=scheduler_dp,\n",
    "                    embedding=embedding,\n",
    "                    scaler=scaler)\n",
    "                (lraspp, optimizer, optimizer_dp, embedding, scaler) = \\\n",
    "                    get_model(\n",
    "                        config, len(training_dataset),\n",
    "                        len(training_dataset.label_tags),\n",
    "                        THIS_SCRIPT_DIR=THIS_SCRIPT_DIR,\n",
    "                        _path=_path, device='cuda')\n",
    "\n",
    "            print()\n",
    "            print(\"### Validation\")\n",
    "            lraspp.eval()\n",
    "            training_dataset.eval()\n",
    "\n",
    "            val_dices = []\n",
    "            val_class_dices = []\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    for val_idx in val_3d_idxs:\n",
    "                        val_sample = training_dataset.get_3d_item(val_idx)\n",
    "                        stack_dim = training_dataset.use_2d_normal_to\n",
    "                        # Create batch out of single val sample\n",
    "                        b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                        b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "\n",
    "                        B = b_val_img.shape[0]\n",
    "\n",
    "                        b_val_img = b_val_img.unsqueeze(1).float().cuda()\n",
    "                        b_val_seg = b_val_seg.cuda()\n",
    "\n",
    "                        if training_dataset.use_2d():\n",
    "                            b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=training_dataset.use_2d_normal_to)\n",
    "\n",
    "                            if config.use_mind:\n",
    "                                # MIND 2D model, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                                b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                            output_val = lraspp(b_val_img_2d)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "                            # Prepare logits for scoring\n",
    "                            # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                            val_logits_for_score = make_3d_from_2d_stack(\n",
    "                                val_logits_for_score.unsqueeze(1), stack_dim, B\n",
    "                            ).squeeze(1)\n",
    "\n",
    "                        else:\n",
    "                            if config.use_mind:\n",
    "                                # MIND 3D model shape BxMINDxDxHxW\n",
    "                                b_val_img = mindssc(b_val_img)\n",
    "                            else:\n",
    "                                # 3D model shape Bx1xDxHxW\n",
    "                                pass\n",
    "\n",
    "                            output_val = lraspp(b_val_img)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "\n",
    "                        b_val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(val_logits_for_score, len(training_dataset.label_tags)),\n",
    "                            torch.nn.functional.one_hot(b_val_seg, len(training_dataset.label_tags)),\n",
    "                            one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        # Get mean score over batch\n",
    "                        val_dices.append(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=True))\n",
    "\n",
    "                        val_class_dices.append(get_batch_dice_per_class(\n",
    "                            b_val_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                        if config.do_plot:\n",
    "                            print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                            print(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=False))\n",
    "                            # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                            display_seg(in_type=\"single_3D\",\n",
    "                                reduce_dim=\"W\",\n",
    "                                img=val_sample['image'].unsqueeze(0).cpu(),\n",
    "                                seg=val_logits_for_score_3d.squeeze(0).cpu(), # CHECK TODO\n",
    "                                ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                crop_to_non_zero_seg=True,\n",
    "                                crop_to_non_zero_gt=True,\n",
    "                                alpha_seg=.3,\n",
    "                                alpha_gt=.0\n",
    "                            )\n",
    "\n",
    "                    mean_val_dice = np.nanmean(val_dices)\n",
    "                    print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                    wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                    log_class_dices(\"scores/val_dice_mean_\", f\"_fold{fold_idx}\", val_class_dices, global_idx)\n",
    "\n",
    "            print()\n",
    "            # End of training loop\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "            # Write sample data\n",
    "\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            all_idxs = torch.tensor(range(len(training_dataset))).cuda()\n",
    "            train_label_snapshot_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/train_label_snapshot.pth\")\n",
    "            seg_viz_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weighted_samples.png\")\n",
    "\n",
    "            train_label_snapshot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if str(config.data_param_mode) == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                dp_weights = embedding(all_idxs)\n",
    "                save_data = []\n",
    "                data_generator = zip(\n",
    "                    dp_weights[train_idxs], \\\n",
    "                    disturbed_bool_vect[train_idxs],\n",
    "                    torch.utils.data.Subset(training_dataset, train_idxs)\n",
    "                )\n",
    "\n",
    "                for dp_weight, disturb_flg, sample in data_generator:\n",
    "                    data_tuple = ( \\\n",
    "                        dp_weight,\n",
    "                        bool(disturb_flg.item()),\n",
    "                        sample['id'],\n",
    "                        sample['dataset_idx'],\n",
    "                        sample['image'],\n",
    "                        sample['label'],\n",
    "                        sample['modified_label'],\n",
    "                        inference_wrap(lraspp, sample['image'].cuda(), use_2d=training_dataset.use_2d(), use_mind=config.use_mind)\n",
    "                    )\n",
    "                    save_data.append(data_tuple)\n",
    "\n",
    "                save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "                (dp_weight, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _imgs,\n",
    "                 _labels, _modified_labels, _predictions) = zip(*save_data)\n",
    "\n",
    "                dp_weight = torch.stack(dp_weight)\n",
    "                dataset_idxs = torch.stack(dataset_idxs)\n",
    "                _imgs = torch.stack(_imgs)\n",
    "                _labels = torch.stack(_labels)\n",
    "                _modified_labels = torch.stack(_modified_labels)\n",
    "                _predictions = torch.stack(_predictions)\n",
    "\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'data_parameters': dp_weight.cpu(),\n",
    "                        'disturb_flags': disturb_flags,\n",
    "                        'd_ids': d_ids,\n",
    "                        'dataset_idxs': dataset_idxs.cpu(),\n",
    "                        'labels': _labels.cpu().to_sparse(),\n",
    "                        'modified_labels': _modified_labels.cpu().to_sparse(),\n",
    "                        'train_predictions': _predictions.cpu().to_sparse(),\n",
    "                    },\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "            elif str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                # Log histogram of clean and disturbed parameters\n",
    "                if not training_dataset.use_2d():\n",
    "                    raise NotImplementedError(\"Script does not support 3D model and GRIDDED_INSTANCE_PARAMS yet.\")\n",
    "\n",
    "                m_all_idxs = map_embedding_idxs(all_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                ).cuda()\n",
    "                masks = union_wise_mod_label[all_idxs].float()\n",
    "                all_weights = torch.nn.functional.interpolate(\n",
    "                    embedding(m_all_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                    size=(masks.shape[-2:]),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=True\n",
    "                ).squeeze(1)\n",
    "\n",
    "                masked_weights = all_weights * masks\n",
    "                masked_weights[masked_weights==0.] = float('nan')\n",
    "                dp_weights = np.nanmean(masked_weights.detach().cpu(), axis=n_dims)\n",
    "\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weightmap.png\")\n",
    "\n",
    "                save_data = []\n",
    "                data_generator = zip(\n",
    "                    dp_weights[train_idxs],\n",
    "                    all_weights[train_idxs],\n",
    "                    disturbed_bool_vect[train_idxs],\n",
    "                    torch.utils.data.Subset(training_dataset,train_idxs)\n",
    "                )\n",
    "\n",
    "                for dp_weight, weightmap, disturb_flg, sample in data_generator:\n",
    "                    data_tuple = (\n",
    "                        dp_weight,\n",
    "                        weightmap,\n",
    "                        bool(disturb_flg.item()),\n",
    "                        sample['id'],\n",
    "                        sample['dataset_idx'],\n",
    "                        sample['image'],\n",
    "                        sample['label'],\n",
    "                        sample['modified_label']\n",
    "                    )\n",
    "                    save_data.append(data_tuple)\n",
    "\n",
    "                save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "\n",
    "                (dp_weight, dp_weightmap, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _2d_imgs,\n",
    "                 _2d_labels, _2d_modified_labels) = zip(*save_data)\n",
    "\n",
    "                dp_weight = torch.stack(dp_weight)\n",
    "                dp_weightmap = torch.stack(dp_weightmap)\n",
    "                dataset_idxs = torch.stack(dataset_idxs)\n",
    "                _2d_imgs = torch.stack(_2d_imgs)\n",
    "                _2d_labels = torch.stack(_2d_labels)\n",
    "                _2d_modified_labels = torch.stack(_2d_modified_labels)\n",
    "                _2d_predictions = torch.stack(_2d_predictions)\n",
    "\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'data_parameters': dp_weight.cpu(),\n",
    "                        'data_parameter_weightmaps': dp_weightmap.cpu(),\n",
    "                        'disturb_flags': disturb_flags,\n",
    "                        'd_ids': d_ids,\n",
    "                        'dataset_idxs': dataset_idxs.cpu(),\n",
    "                        'labels': _2d_labels.cpu().to_sparse(),\n",
    "                        'modified_labels': _2d_modified_labels.cpu().to_sparse(),\n",
    "                        'train_predictions': _2d_predictions.cpu().to_sparse(),\n",
    "                    },\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "                print(\"Writing weight map image.\")\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}_data_parameter_weightmap.png\")\n",
    "                visualize_seg(in_type=\"batch_2D\",\n",
    "                    img=torch.stack(dp_weightmap).unsqueeze(1),\n",
    "                    seg=torch.stack(_2d_modified_labels),\n",
    "                    alpha_seg = 0.,\n",
    "                    n_per_row=70,\n",
    "                    overlay_text=overlay_text_list,\n",
    "                    annotate_color=(0,255,255),\n",
    "                    frame_elements=disturb_flags,\n",
    "                    file_path=weightmap_out_path,\n",
    "                )\n",
    "\n",
    "            if len(training_dataset.disturbed_idxs) > 0:\n",
    "                # Log histogram\n",
    "                separated_params = list(zip(dp_weights[clean_idxs], dp_weights[training_dataset.disturbed_idxs]))\n",
    "                s_table = wandb.Table(columns=['clean_idxs', 'disturbed_idxs'], data=separated_params)\n",
    "                fields = {\"primary_bins\" : \"clean_idxs\", \"secondary_bins\" : \"disturbed_idxs\", \"title\" : \"Data parameter composite histogram\"}\n",
    "                composite_histogram = wandb.plot_table(vega_spec_name=\"rap1ide/composite_histogram\", data_table=s_table, fields=fields)\n",
    "                wandb.log({f\"data_parameters/separated_params_fold_{fold_idx}\": composite_histogram})\n",
    "\n",
    "            # Write out data of modified and un-modified labels and an overview image\n",
    "            print(\"Writing train sample image.\")\n",
    "            # overlay text example: d_idx=0, dp_i=1.00, dist? False\n",
    "            overlay_text_list = [f\"id:{d_id} dp:{instance_p.item():.2f}\" \\\n",
    "                for d_id, instance_p, disturb_flg in zip(d_ids, dp_weight, disturb_flags)]\n",
    "\n",
    "            if training_dataset.use_2d():\n",
    "                reduce_dim = None\n",
    "                in_type = \"batch_2D\"\n",
    "            else:\n",
    "                reduce_dim = \"W\"\n",
    "                in_type = \"batch_3D\"\n",
    "\n",
    "            use_2d = training_dataset.use_2d()\n",
    "            scf = 1/training_dataset.pre_interpolation_factor\n",
    "\n",
    "            show_img = interpolate_sample(b_label=_labels, scale_factor=scf, use_2d=use_2d)[1].unsqueeze(1)\n",
    "            show_seg = interpolate_sample(b_label=4*_predictions.squeeze(1), scale_factor=scf, use_2d=use_2d)[1]\n",
    "            show_gt = interpolate_sample(b_label=_modified_labels, scale_factor=scf, use_2d=use_2d)[1]\n",
    "\n",
    "            visualize_seg(in_type=in_type, reduce_dim=reduce_dim,\n",
    "                img=show_img, # Expert label in BW\n",
    "                seg=show_seg, # Prediction in blue\n",
    "                ground_truth=show_gt, # Modified label in red\n",
    "                crop_to_non_zero_seg=False,\n",
    "                alpha_seg = .5,\n",
    "                alpha_gt = .5,\n",
    "                n_per_row=70,\n",
    "                overlay_text=overlay_text_list,\n",
    "                annotate_color=(0,255,255),\n",
    "                frame_elements=disturb_flags,\n",
    "                file_path=seg_viz_out_path,\n",
    "            )\n",
    "\n",
    "        # End of fold loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fc80a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Config overrides\n",
    "# config_dict['wandb_mode'] = 'disabled'\n",
    "# config_dict['debug'] = True\n",
    "# Model loading\n",
    "# config_dict['checkpoint_name'] = 'treasured-water-717'\n",
    "# # config_dict['fold_override'] = 0\n",
    "# config_dict['checkpoint_epx'] = 39\n",
    "\n",
    "# Define sweep override dict\n",
    "sweep_config_dict = dict(\n",
    "    method='grid',\n",
    "    metric=dict(goal='maximize', name='scores/val_dice_mean_tumour_fold0'),\n",
    "    parameters=dict(\n",
    "        # disturbance_mode=dict(\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        # reg_state=dict(\n",
    "        #     values=['best','combined']\n",
    "        # ),\n",
    "        disturbance_strength=dict(\n",
    "            values=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "        ),\n",
    "        disturbed_percentage=dict(\n",
    "            values=[0.3, 0.6]\n",
    "        ),\n",
    "        data_param_mode=dict(\n",
    "            values=[\n",
    "                DataParamMode.INSTANCE_PARAMS,\n",
    "                DataParamMode.DISABLED,\n",
    "            ]\n",
    "        ),\n",
    "        # use_risk_regularization=dict(\n",
    "        #     values=[False, True]\n",
    "        # )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549637cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def normal_run():\n",
    "    with wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        ) as run:\n",
    "\n",
    "        run_name = run.name\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init() as run:\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "\n",
    "        run_name = run.name\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "\n",
    "if config_dict['do_sweep']:\n",
    "    # Integrate all config_dict entries into sweep_dict.parameters -> sweep overrides config_dict\n",
    "    cp_config_dict = copy.deepcopy(config_dict)\n",
    "    # cp_config_dict.update(copy.deepcopy(sweep_config_dict['parameters']))\n",
    "    for del_key in sweep_config_dict['parameters'].keys():\n",
    "        if del_key in cp_config_dict:\n",
    "            del cp_config_dict[del_key]\n",
    "    merged_sweep_config_dict = copy.deepcopy(sweep_config_dict)\n",
    "    # merged_sweep_config_dict.update(cp_config_dict)\n",
    "    for key, value in cp_config_dict.items():\n",
    "        merged_sweep_config_dict['parameters'][key] = dict(value=value)\n",
    "    # Convert enum values in parameters to string. They will be identified by their numerical index otherwise\n",
    "    for key, param_dict in merged_sweep_config_dict['parameters'].items():\n",
    "        if 'value' in param_dict and isinstance(param_dict['value'], Enum):\n",
    "            param_dict['value'] = str(param_dict['value'])\n",
    "        if 'values' in param_dict:\n",
    "            param_dict['values'] = [str(elem) if isinstance(elem, Enum) else elem for elem in param_dict['values']]\n",
    "\n",
    "        merged_sweep_config_dict['parameters'][key] = param_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(merged_sweep_config_dict, project=\"curriculum_deeplab\")\n",
    "    wandb.agent(sweep_id, function=sweep_run)\n",
    "\n",
    "else:\n",
    "    normal_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7469925",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# def inference_DL(run_name, config, inf_dataset):\n",
    "\n",
    "#     score_dicts = []\n",
    "\n",
    "#     fold_iter = range(config.num_folds)\n",
    "#     if config_dict['only_first_fold']:\n",
    "#         fold_iter = fold_iter[0:1]\n",
    "\n",
    "#     for fold_idx in fold_iter:\n",
    "#         lraspp, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "#         lraspp.eval()\n",
    "#         inf_dataset.eval()\n",
    "#         stack_dim = config.use_2d_normal_to\n",
    "\n",
    "#         inf_dices = []\n",
    "#         inf_dices_tumour = []\n",
    "#         inf_dices_cochlea = []\n",
    "\n",
    "#         for inf_sample in inf_dataset:\n",
    "#             global_idx = get_global_idx(fold_idx, sample_idx, config.epochs)\n",
    "#             crossmoda_id = sample['crossmoda_id']\n",
    "#             with amp.autocast(enabled=True):\n",
    "#                 with torch.no_grad():\n",
    "\n",
    "#                     # Create batch out of single val sample\n",
    "#                     b_inf_img = inf_sample['image'].unsqueeze(0)\n",
    "#                     b_inf_seg = inf_sample['label'].unsqueeze(0)\n",
    "\n",
    "#                     B = b_inf_img.shape[0]\n",
    "\n",
    "#                     b_inf_img = b_inf_img.unsqueeze(1).float().cuda()\n",
    "#                     b_inf_seg = b_inf_seg.cuda()\n",
    "#                     b_inf_img_2d = make_2d_stack_from_3d(b_inf_img, stack_dim=stack_dim)\n",
    "\n",
    "#                     if config.use_mind:\n",
    "#                         b_inf_img_2d = mindssc(b_inf_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "#                     output_inf = lraspp(b_inf_img_2d)['out']\n",
    "\n",
    "#                     # Prepare logits for scoring\n",
    "#                     # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "#                     inf_logits_for_score = make_3d_from_2d_stack(output_inf, stack_dim, B)\n",
    "#                     inf_logits_for_score = inf_logits_for_score.argmax(1)\n",
    "\n",
    "#                     inf_dice = dice3d(\n",
    "#                         torch.nn.functional.one_hot(inf_logits_for_score, 3),\n",
    "#                         torch.nn.functional.one_hot(b_inf_seg, 3),\n",
    "#                         one_hot_torch_style=True\n",
    "#                     )\n",
    "#                     inf_dices.append(get_batch_dice_wo_bg(inf_dice))\n",
    "#                     inf_dices_tumour.append(get_batch_dice_tumour(inf_dice))\n",
    "#                     inf_dices_cochlea.append(get_batch_dice_cochlea(inf_dice))\n",
    "\n",
    "#                     if config.do_plot:\n",
    "#                         print(\"Inference 3D image label/ground-truth\")\n",
    "#                         print(inf_dice)\n",
    "#                         # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "#                         display_seg(in_type=\"single_3D\",\n",
    "#                             reduce_dim=\"W\",\n",
    "#                             img=inf_sample['image'].unsqueeze(0).cpu(),\n",
    "#                             seg=inf_logits_for_score.squeeze(0).cpu(),\n",
    "#                             ground_truth=b_inf_seg.squeeze(0).cpu(),\n",
    "#                             crop_to_non_zero_seg=True,\n",
    "#                             crop_to_non_zero_gt=True,\n",
    "#                             alpha_seg=.4,\n",
    "#                             alpha_gt=.2\n",
    "#                         )\n",
    "\n",
    "#             if config.debug:\n",
    "#                 break\n",
    "\n",
    "#         mean_inf_dice = np.nanmean(inf_dices)\n",
    "#         mean_inf_dice_tumour = np.nanmean(inf_dices_tumour)\n",
    "#         mean_inf_dice_cochlea = np.nanmean(inf_dices_cochlea)\n",
    "\n",
    "#         print(f'inf_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_inf_dice*100:.2f}%\")\n",
    "#         print(f'inf_dice_mean_tumour_fold{fold_idx}', f\"{mean_inf_dice_tumour*100:.2f}%\")\n",
    "#         print(f'inf_dice_mean_cochlea_fold{fold_idx}', f\"{mean_inf_dice_cochlea*100:.2f}%\")\n",
    "#         wandb.log({f'scores/inf_dice_mean_wo_bg_fold{fold_idx}': mean_inf_dice}, step=global_idx)\n",
    "#         wandb.log({f'scores/inf_dice_mean_tumour_fold{fold_idx}': mean_inf_dice_tumour}, step=global_idx)\n",
    "#         wandb.log({f'scores/inf_dice_mean_cochlea_fold{fold_idx}': mean_inf_dice_cochlea}, step=global_idx)\n",
    "\n",
    "#         # Store data for inter-fold scoring\n",
    "#         class_dice_list = inf_dices.tolist()[0]\n",
    "#         for class_idx, class_dice in enumerate(class_dice_list):\n",
    "#             score_dicts.append(\n",
    "#                 {\n",
    "#                     'fold_idx': fold_idx,\n",
    "#                     'crossmoda_id': crossmoda_id,\n",
    "#                     'class_idx': class_idx,\n",
    "#                     'class_dice': class_dice,\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     mean_oa_inf_dice = np.nanmean(torch.tensor([score['class_dice'] for score in score_dicts]))\n",
    "#     print(f\"Mean dice over all folds, classes and samples: {mean_oa_inf_dice*100:.2f}%\")\n",
    "#     wandb.log({'scores/mean_dice_all_folds_samples_classes': mean_oa_inf_dice}, step=global_idx)\n",
    "\n",
    "#     return score_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a608620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds_scores = []\n",
    "# run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"testing\", job_type=\"test\",\n",
    "#         config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "#         mode=config_dict['wandb_mode']\n",
    "# )\n",
    "# config = wandb.config\n",
    "# score_dicts = inference_DL(run_name, config, validation_dataset)\n",
    "# folds_scores.append(score_dicts)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984e8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
