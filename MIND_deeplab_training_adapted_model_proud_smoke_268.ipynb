{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3175666",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name                     Util    Mem free  Cuda             User(s)\n",
      "----  --------------------------  ------  ----------  ---------------  -------------\n",
      "   2  NVIDIA GeForce RTX 2080 Ti     0 %   11016 MiB  11.5(495.29.05)\n",
      "   3  NVIDIA GeForce RTX 2080 Ti     0 %    8834 MiB  11.5(495.29.05)  andresen\n",
      "   0  NVIDIA GeForce RTX 2080 Ti     0 %    2575 MiB  11.5(495.29.05)  schneider\n",
      "   1  NVIDIA GeForce RTX 2080 Ti     0 %      55 MiB  11.5(495.29.05)  grossbroehmer\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   2  NVIDIA GeForce RTX 2080 Ti  ->  cuda:0\n",
      "1.9.0a0+gitdfbd030\n",
      "8204\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Running in: /share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import glob\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -4\"))\n",
    "import pickle\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import functools\n",
    "from enum import Enum, auto\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import visualize_seg\n",
    "\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "from curriculum_deeplab.utils import interpolate_sample, in_notebook, dilate_label_class, LabelDisturbanceMode\n",
    "from curriculum_deeplab.CrossmodaHybridIdLoader import CrossmodaHybridIdLoader, get_crossmoda_data_load_closure\n",
    "from curriculum_deeplab.MobileNet_LR_ASPP_3D import MobileNet_LRASPP_3D\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "if in_notebook:\n",
    "    THIS_SCRIPT_DIR = os.path.abspath('')\n",
    "else:\n",
    "    THIS_SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "print(f\"Running in: {THIS_SCRIPT_DIR}\")\n",
    "\n",
    "def get_batch_dice_per_class(b_dice, class_tags, exclude_bg=True) -> dict:\n",
    "    score_dict = {}\n",
    "    for cls_idx, cls_tag in enumerate(class_tags):\n",
    "        if exclude_bg and cls_idx == 0:\n",
    "            continue\n",
    "\n",
    "        if torch.all(torch.isnan(b_dice[:,cls_idx])):\n",
    "            score = float('nan')\n",
    "        else:\n",
    "            score = np.nanmean(b_dice[:,cls_idx]).item()\n",
    "\n",
    "        score_dict[cls_tag] = score\n",
    "\n",
    "    return score_dict\n",
    "\n",
    "def get_batch_dice_over_all(b_dice, exclude_bg=True) -> float:\n",
    "\n",
    "    start_idx = 1 if exclude_bg else 0\n",
    "    if torch.all(torch.isnan(b_dice[:,start_idx:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,start_idx:]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5750fa2f-e706-4c65-ac09-c3ab8050b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParamMode(Enum):\n",
    "    INSTANCE_PARAMS = auto()\n",
    "    GRIDDED_INSTANCE_PARAMS = auto()\n",
    "    DISABLED = auto()\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "        See https://stackoverflow.com/questions/49901590/python-using-copy-deepcopy-on-dotdict\n",
    "    \"\"\"\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        try:\n",
    "            return self[item]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError from e\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "\n",
    "    'use_mind': False,\n",
    "    'epochs': 40,\n",
    "\n",
    "    'batch_size': 16,\n",
    "    'val_batch_size': 1,\n",
    "    'use_2d_normal_to': None,\n",
    "    'train_patchwise': True,\n",
    "\n",
    "    'dataset': 'crossmoda',\n",
    "    'reg_state': \"acummulate_convex_adam_FT2_MT1\",\n",
    "    'train_set_max_len': None,\n",
    "    'crop_3d_w_dim_range': (45, 95),\n",
    "    'crop_2d_slices_gt_num_threshold': 0,\n",
    "\n",
    "    'lr': 0.0005,\n",
    "    'use_cosine_annealing': True,\n",
    "\n",
    "    # Data parameter config\n",
    "    'data_param_mode': DataParamMode.INSTANCE_PARAMS,\n",
    "    'init_inst_param': 0.0,\n",
    "    'lr_inst_param': 0.1,\n",
    "    'use_risk_regularization': True,\n",
    "\n",
    "    'grid_size_y': 64,\n",
    "    'grid_size_x': 64,\n",
    "    # ),\n",
    "\n",
    "    'save_every': 200,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'do_plot': False,\n",
    "    'save_dp_figures': False,\n",
    "    'debug': True,\n",
    "    'wandb_mode': 'disabled', # e.g. online, disabled\n",
    "    'checkpoint_name': None,\n",
    "    'do_sweep': False,\n",
    "\n",
    "    'disturbance_mode': LabelDisturbanceMode.AFFINE,\n",
    "    'disturbance_strength': 2.,\n",
    "    'disturbed_percentage': .3,\n",
    "    'start_disturbing_after_ep': 0,\n",
    "\n",
    "    'start_dilate_kernel_sz': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de18b0cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2457023498.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3946837/2457023498.py\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    combined_label_data =\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(config):\n",
    "    if config.reg_state:\n",
    "        print(\"Loading registered data.\")\n",
    "\n",
    "        REG_STATES = [\n",
    "            \"combined\", \"best_1\", \"best_n\",\n",
    "            \"multiple\", \"mix_combined_best\",\n",
    "            \"best\", \"cummulate_combined_best\"]\n",
    "\n",
    "        # assert config.reg_state in REG_STATES, f\"Unknown registration version. Choose one of {REG_STATES}\"\n",
    "\n",
    "        if config.reg_state == \"mix_combined_best\":\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "\n",
    "            perm = np.random.permutation(len(loaded_identifier))\n",
    "            _clen = int(.5*len(loaded_identifier))\n",
    "            best_choice = perm[:_clen]\n",
    "            combined_choice = perm[_clen:]\n",
    "\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)[best_choice]\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)[combined_choice]\n",
    "            label_data = torch.zeros([107,128,128,128])\n",
    "            label_data[best_choice] = best_label_data\n",
    "            label_data[combined_choice] = combined_label_data\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"cummulate_combined_best\":\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)\n",
    "            label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier] + [_id+':var001' for _id in loaded_identifier]\n",
    "        \n",
    "        elif config.reg_state == \"acummulate_convex_adam_FT2_MT1\":\n",
    "            \n",
    "            label_data = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220113_crossmoda_convex/crossmoda_convex.pth\")\n",
    "            combined_label_data = \n",
    "            # best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)\n",
    "            # combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)\n",
    "            # label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            # loaded_identifier = [_id+':var000' for _id in loaded_identifier] + [_id+':var001' for _id in loaded_identifier]\n",
    "\n",
    "        else:\n",
    "            label_data = torch.cat([label_data_left[config.reg_state+'_all'][:44], label_data_right[config.reg_state+'_all'][:63]], dim=0)\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier]\n",
    "        raise(False)\n",
    "        modified_3d_label_override = {}\n",
    "        for idx, identifier in enumerate(loaded_identifier):\n",
    "            nl_id = int(re.findall(r'\\d+', identifier)[0])\n",
    "            var_id = int(re.findall(r':var(\\d+)$', identifier)[0])\n",
    "            lr_id = re.findall(r'([lr])\\.nii\\.gz', identifier)[0]\n",
    "\n",
    "            crossmoda_var_id = f\"{nl_id:03d}{lr_id}:var{var_id:03d}\"\n",
    "\n",
    "            modified_3d_label_override[crossmoda_var_id] = label_data[idx]\n",
    "\n",
    "        prevent_disturbance = True\n",
    "\n",
    "    else:\n",
    "        modified_3d_label_override = None\n",
    "        prevent_disturbance = False\n",
    "                  \n",
    "    if config.dataset == 'crossmoda':\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.\n",
    "        clsre = get_crossmoda_data_load_closure(\n",
    "            base_dir=\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "            domain='source', state='l4', use_additional_data=False,\n",
    "            size=(128,128,128), resample=True, normalize=True, crop_3d_w_dim_range=config.crop_3d_w_dim_range,\n",
    "            ensure_labeled_pairs=True,\n",
    "            debug=config.debug\n",
    "        )\n",
    "        training_dataset = CrossmodaHybridIdLoader(\n",
    "            clsre,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(f\"Nonzero slices: \" \\\n",
    "            f\"{sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)*100}%\"\n",
    "        )\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "    if config.dataset == 'ixi':\n",
    "        raise NotImplementedError()\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.\n",
    "        clsre = get_ixi_data_load_closure()\n",
    "        training_dataset = IXIHybridIdLoader(\n",
    "            clsre,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(f\"Nonzero slices: \" \\\n",
    "            f\"{sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)*100}%\"\n",
    "        )\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "\n",
    "    elif config['dataset'] == 'organmnist3d':\n",
    "        training_dataset = WrapperOrganMNIST3D(\n",
    "            split='train', root='./data/medmnist', download=True, normalize=True,\n",
    "            max_load_num=300, crop_3d_w_dim_range=None,\n",
    "            disturbed_idxs=None, use_2d_normal_to='W'\n",
    "        )\n",
    "        print(training_dataset.mnist_set.info)\n",
    "        print(\"Classes: \", training_dataset.label_tags)\n",
    "        print(\"Samples: \", len(training_dataset))\n",
    "\n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    _, all_labels, _ = training_dataset.get_data(use_2d_override=False)\n",
    "    print(all_labels.shape)\n",
    "    sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "    plt.xlabel(\"W\")\n",
    "    plt.ylabel(\"ground truth>0\")\n",
    "    plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c2075ba-4d51-4fc1-82fc-9193a7a86043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed files lengths  30 30 30\n",
      "Moving files lengths  63 44 63\n",
      "fixed_ids ['168r', '134r', '198r', '127r', '148r', '181r', '112r', '154r', '205r', '210r', '165r', '120r', '142r', '195r', '123r', '166r', '135r', '180r', '174r', '160r', '108r', '118r', '209r', '185r', '171r', '173r', '179r', '204r', '167r', '144r'] [168, 134, 198, 127, 148, 181, 112, 154, 205, 210, 165, 120, 142, 195, 123, 166, 135, 180, 174, 160, 108, 118, 209, 185, 171, 173, 179, 204, 167, 144]\n",
      "moving_ids ['027r', '082r', '083r', '094r', '024r', '053r', '098r', '017r', '097r', '035r', '009r', '001r', '047r', '060r', '074r', '012r', '004r', '065r', '044r', '048r', '011r', '032r', '052r', '054r', '077r', '005r', '003r', '096r', '007r', '040r', '073r', '099r', '085r', '091r', '057r', '037r', '095r', '080r', '046r', '025r', '039r', '072r', '101r', '023r', '016r', '045r', '059r', '092r', '006r', '036r', '019r', '070r', '076r', '084r', '100r', '033r', '078r', '042r', '050r', '030r', '086r', '068r', '029r'] [27, 82, 83, 94, 24, 53, 98, 17, 97, 35, 9, 1, 47, 60, 74, 12, 4, 65, 44, 48, 11, 32, 52, 54, 77, 5, 3, 96, 7, 40, 73, 99, 85, 91, 57, 37, 95, 80, 46, 25, 39, 72, 101, 23, 16, 45, 59, 92, 6, 36, 19, 70, 76, 84, 100, 33, 78, 42, 50, 30, 86, 68, 29]\n",
      "30\n",
      "0\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_100_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_77_Label_r.nii.gz\n",
      "fixed: 108, moving: 100\n",
      "fixed: 108r, moving: 100r\n",
      "tensor([[0.9959, 0.3257, 0.4598]])\n",
      "\n",
      "60\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_11_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_9_Label_r.nii.gz\n",
      "fixed: 108, moving: 11\n",
      "fixed: 108r, moving: 011r\n",
      "tensor([[0.9943, 0.0859, 0.3344]])\n",
      "\n",
      "120\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_16_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_11_Label_r.nii.gz\n",
      "fixed: 108, moving: 16\n",
      "fixed: 108r, moving: 016r\n",
      "tensor([[0.9946, 0.1029, 0.3308]])\n",
      "\n",
      "180\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_19_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_54_Label_r.nii.gz\n",
      "fixed: 108, moving: 19\n",
      "fixed: 108r, moving: 019r\n",
      "tensor([[0.9960, 0.2316, 0.4207]])\n",
      "\n",
      "240\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_23_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_48_Label_r.nii.gz\n",
      "fixed: 108, moving: 23\n",
      "fixed: 108r, moving: 023r\n",
      "tensor([[0.9959, 0.3403, 0.3856]])\n",
      "\n",
      "300\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_25_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_4_Label_r.nii.gz\n",
      "fixed: 108, moving: 25\n",
      "fixed: 108r, moving: 025r\n",
      "tensor([[0.9980, 0.5349, 0.3904]])\n",
      "\n",
      "360\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_29_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_40_Label_r.nii.gz\n",
      "fixed: 108, moving: 29\n",
      "fixed: 108r, moving: 029r\n",
      "tensor([[0.9958, 0.3175, 0.4431]])\n",
      "\n",
      "420\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_32_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_1_Label_r.nii.gz\n",
      "fixed: 108, moving: 32\n",
      "fixed: 108r, moving: 032r\n",
      "tensor([[0.9947, 0.2462, 0.3309]])\n",
      "\n",
      "480\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_35_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_94_Label_r.nii.gz\n",
      "fixed: 108, moving: 35\n",
      "fixed: 108r, moving: 035r\n",
      "tensor([[0.9959, 0.2689, 0.3970]])\n",
      "\n",
      "540\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_37_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_74_Label_r.nii.gz\n",
      "fixed: 108, moving: 37\n",
      "fixed: 108r, moving: 037r\n",
      "tensor([[0.9947, 0.1542, 0.3527]])\n",
      "\n",
      "600\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_3_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_47_Label_r.nii.gz\n",
      "fixed: 108, moving: 3\n",
      "fixed: 108r, moving: 003r\n",
      "tensor([[0.9938, 0.1987, 0.3147]])\n",
      "\n",
      "660\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_42_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_3_Label_r.nii.gz\n",
      "fixed: 108, moving: 42\n",
      "fixed: 108r, moving: 042r\n",
      "tensor([[0.9952, 0.3367, 0.3585]])\n",
      "\n",
      "720\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_45_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_32_Label_r.nii.gz\n",
      "fixed: 108, moving: 45\n",
      "fixed: 108r, moving: 045r\n",
      "tensor([[0.9954, 0.2832, 0.3833]])\n",
      "\n",
      "780\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_47_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_53_Label_r.nii.gz\n",
      "fixed: 108, moving: 47\n",
      "fixed: 108r, moving: 047r\n",
      "tensor([[0.9960, 0.2288, 0.4175]])\n",
      "\n",
      "840\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_convex/crossmoda_4_L108.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_17_Label_r.nii.gz\n",
      "fixed: 108, moving: 4\n",
      "fixed: 108r, moving: 004r\n",
      "tensor([[0.9959, 0.2584, 0.3810]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# swap_dir = True\n",
    "\n",
    "data_path = \"/share/data_supergrover1/heinrich/crossmoda_convex/\"\n",
    "orig_path = \"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/L4_fine_localized_crop/\"\n",
    "# orig_path = \"/share/data_supergrover1/hansen/temp/crossMoDa/preprocessed_new/resampled/localised_crop/source_training/\"\n",
    "\n",
    "registered_files = glob.glob(data_path+\"*.nii.gz\")\n",
    "# print(registered_files)\n",
    "dice_files_right = torch.load(data_path+\"dice_files_right.pth\")\n",
    "dice_files_left = torch.load(data_path+\"dice_files_left.pth\")\n",
    "\n",
    "fixed_files_right = dice_files_right['target_tumour_right']\n",
    "fixed_files_right = set([elem[0] for elem in fixed_files_right])\n",
    "fixed_files_left = dice_files_left['target_tumour_left']\n",
    "fixed_files_left = set([elem[0] for elem in fixed_files_left])\n",
    "\n",
    "moving_files_right = dice_files_right['source_tumour_right']\n",
    "moving_files_right = set([elem[0] for elem in moving_files_right])\n",
    "moving_files_left = dice_files_left['source_tumour_left']\n",
    "moving_files_left = set([elem[0] for elem in moving_files_left])\n",
    "\n",
    "all_fixed_files = sorted(list(fixed_files_left.union(fixed_files_right)))\n",
    "all_fixed_files = list(fixed_files_right)\n",
    "\n",
    "all_moving_files = sorted(list(moving_files_left.union(moving_files_right)))\n",
    "\n",
    "all_moving_files = list(moving_files_right)\n",
    "\n",
    "# if swap_dir:\n",
    "#     all_fixed_files, all_moving_files = all_moving_files, all_fixed_files\n",
    "\n",
    "print(\"Fixed files lengths \", len(fixed_files_right), len(fixed_files_left), len(all_fixed_files))\n",
    "print(\"Moving files lengths \", len(moving_files_right), len(moving_files_left), len(all_moving_files))\n",
    "\n",
    "fixed_ids = [re.findall(r\"(\\d{1,3})_Label_([lr])\", f_name)[0] for f_name in all_fixed_files]\n",
    "fixed_ids = [f'{int(num_id):03d}{lr_id}' for num_id, lr_id in fixed_ids]\n",
    "\n",
    "moving_ids = [re.findall(r\"(\\d{1,3})_Label_([lr])\", f_name)[0] for f_name in all_moving_files]\n",
    "moving_ids = [f'{int(num_id):03d}{lr_id}' for num_id, lr_id in moving_ids]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "fid_lookup_list = [int(_id[:3]) for _id in fixed_ids]\n",
    "mid_lookup_list = [int(_id[:3]) for _id in moving_ids]\n",
    "\n",
    "used_fidxs = []\n",
    "used_midxs = []\n",
    "\n",
    "for gz_file in registered_files:\n",
    "    fixed_num, moving_num = re.findall(r\"(\\d{1,3})_L(\\d{3})\", gz_file)[0]\n",
    "    fixed_num, moving_num = int(fixed_num), int(moving_num)\n",
    "    moving_num, fixed_num = fixed_num, moving_num\n",
    "    \n",
    "    # if not swap_dir:\n",
    "    #     fixed_num, moving_num = moving_num, fixed_num\n",
    "    lookup_fidx = fid_lookup_list.index(fixed_num)\n",
    "    used_fidxs.append(lookup_fidx)\n",
    "\n",
    "    lookup_midx = mid_lookup_list.index(moving_num)\n",
    "    used_midxs.append(lookup_midx)\n",
    "\n",
    "used_fidxs = list(set(used_fidxs))\n",
    "used_midxs = list(set(used_midxs))\n",
    "\n",
    "print(\"fixed_ids\", fixed_ids, fid_lookup_list)\n",
    "print(\"moving_ids\", moving_ids, mid_lookup_list)\n",
    "\n",
    "# Clean idxs\n",
    "fixed_ids = [_id for idx, _id in enumerate(fixed_ids) if idx in used_fidxs]\n",
    "fid_lookup_list = [_id for idx, _id in enumerate(fid_lookup_list) if idx in used_fidxs]\n",
    "\n",
    "moving_ids = [_id for idx, _id in enumerate(moving_ids) if idx in used_midxs]\n",
    "mid_lookup_list = [_id for idx, _id in enumerate(mid_lookup_list) if idx in used_midxs]\n",
    "\n",
    "orig_label_dict = {}\n",
    "# if swap_dir:\n",
    "for _id, _file in list(zip(fixed_ids, all_fixed_files)):\n",
    "    file_path = orig_path + \"__omitted_labels_target_training__/\" + _file\n",
    "    file_path = file_path.replace(\"Label_l\", \"hrT2_l_Label\")\n",
    "    file_path = file_path.replace(\"Label_r\", \"hrT2_r_Label\")\n",
    "    if os.path.isfile(file_path):\n",
    "        orig_label_dict[_id] = torch.tensor(nib.load(file_path).get_fdata())\n",
    "# else:\n",
    "# for _id, _file in list(zip(fixed_ids, all_fixed_files)):\n",
    "\n",
    "#     # file_path = orig_path + \"/__omitted_labels_target_training__/\" + _file\n",
    "#     # file_path = file_path.replace(\"Label_l\", \"hrT2_l_Label\")\n",
    "#     # file_path = file_path.replace(\"Label_r\", \"hrT2_r_Label\")\n",
    "#     file_path = orig_path + _file\n",
    "#     if os.path.isfile(file_path):\n",
    "#         orig_label_dict[_id] = torch.tensor(nib.load(file_path).get_fdata())\n",
    "\n",
    "warped_label_dict = {}\n",
    "for gz_file in registered_files:\n",
    "    warped_label = torch.tensor(nib.load(gz_file).get_fdata()).cuda()\n",
    "    fixed_num, moving_num = re.findall(r\"(\\d{1,3})_L(\\d{3})\", gz_file)[0]\n",
    "    fixed_num, moving_num = int(fixed_num), int(moving_num)\n",
    "    moving_num, fixed_num = fixed_num, moving_num\n",
    "    # if not swap_dir:\n",
    "    #     fixed_num, moving_num = moving_num, fixed_num\n",
    "\n",
    "    lookup_fidx = fid_lookup_list.index(fixed_num)\n",
    "    lookup_midx = mid_lookup_list.index(moving_num)\n",
    "    fixed_id = fixed_ids[lookup_fidx]\n",
    "    moving_id = moving_ids[lookup_midx]\n",
    "    \n",
    "    dct = warped_label_dict.get(fixed_id, {})\n",
    "    dct[moving_id] = warped_label.to_sparse() #TODO\n",
    "    warped_label_dict[fixed_id] = dct\n",
    "    # dct[fixed_id] = warped_label.to_sparse() #TODO\n",
    "    # warped_label_dict[moving_id] = dct\n",
    "    \n",
    "print(len(orig_label_dict))\n",
    "\n",
    "for idx, gz_file in enumerate(registered_files):\n",
    "    \n",
    "    fixed_num, moving_num = re.findall(r\"(\\d{1,3})_L(\\d{3})\", gz_file)[0]\n",
    "    fixed_num, moving_num = int(fixed_num), int(moving_num)\n",
    "    moving_num, fixed_num = fixed_num, moving_num\n",
    "\n",
    "    lookup_fidx = fid_lookup_list.index(fixed_num)\n",
    "    lookup_midx = mid_lookup_list.index(moving_num)\n",
    "    fixed_id = fixed_ids[lookup_fidx]\n",
    "    moving_id = moving_ids[lookup_midx]\n",
    "    dct = data_dict.get(fixed_id, {})\n",
    "    \n",
    "    orig_label = orig_label_dict[fixed_id].cuda()\n",
    "    warped_label = warped_label_dict[fixed_id][moving_id].to_dense()\n",
    "\n",
    "    dice = dice3d(F.one_hot(orig_label.long(), 3).unsqueeze(0), \n",
    "          F.one_hot(warped_label.long(), 3).unsqueeze(0), one_hot_torch_style=True)\n",
    "    \n",
    "    # flip_crit = 'l' in moving_id or 'l' in fixed_id\n",
    "#     if flip_crit:\n",
    "#         fdice = dice3d(F.one_hot(orig_label.long(), 3).unsqueeze(0), \n",
    "#                       F.one_hot(warped_label.flip(dims=(-3,)).long(), 3).unsqueeze(0), one_hot_torch_style=True)\n",
    "    \n",
    "#         print(\"flip dice \", fdice[0][2] > dice[0][2], fdice, dice)\n",
    "\n",
    "    dct[moving_id] = {\n",
    "        'warped_label': warped_label.to_sparse(),\n",
    "        'dice': dice\n",
    "    }\n",
    "    data_dict[fixed_id] = dct\n",
    "    \n",
    "    if idx % 60 == 0:\n",
    "    # if len(orig_label.unique()) != len(warped_label.unique()) or len(orig_label.unique()) < 2:\n",
    "        print(idx)\n",
    "        print(orig_label.unique())\n",
    "        print(warped_label.unique())\n",
    "        print(\"Registred file: \", gz_file)\n",
    "        print(\"Fixed file: \", all_fixed_files[lookup_fidx])\n",
    "        print(\"Moving file: \", all_moving_files[lookup_midx])\n",
    "        print(f\"fixed: {fixed_num}, moving: {moving_num}\")\n",
    "        print(f\"fixed: {fixed_id}, moving: {moving_id}\")\n",
    "        print(dice)\n",
    "        print()\n",
    "    \n",
    "# torch.save(data_dict, THIS_SCRIPT_DIR+\"/data/crossmoda_convex.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9a892a0-53a9-42e5-a15d-c02547aad290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique moving_ids 30 ['001r', '003r', '004r', '011r', '012r', '016r', '017r', '019r', '023r', '024r', '025r', '027r', '029r', '030r', '032r', '033r', '035r', '036r', '037r', '039r', '040r', '042r', '044r', '045r', '046r', '047r', '048r', '050r', '100r', '101r']\n",
      "unique fixed_ids 30 ['108r', '112r', '118r', '120r', '123r', '127r', '134r', '135r', '142r', '144r', '148r', '154r', '160r', '165r', '166r', '167r', '168r', '171r', '173r', '174r', '179r', '180r', '181r', '185r', '195r', '198r', '204r', '205r', '209r', '210r']\n",
      "Quantile tumour:  [0.000 0.089 0.245 0.339 0.483 0.820]\n",
      "Quantile cochlea:  [0.062 0.332 0.410 0.462 0.516 0.684]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2vklEQVR4nO3deXxU5fn//9c1SchGEshGQhYIqxAWgQAKKqBSEPcVqfUrarW24lq1Wq31o/20Wv24VaWi1O2n4m4VcUdEAZFFloQtIRCSANlXsmfu3x9nEiZhSYBMJjO5no9HHjNnmXOuGSZvTu5zzn2LMQallFKez+buApRSSnUMDXSllPISGuhKKeUlNNCVUspLaKArpZSX0EBXSikvoYGuVCsiskxEftvOdaeKSI7TdJqITHVVbUodjQa6cjsROU1EVopImYgUi8gKERl/nNvqLyJGRHw7us72MMYkG2OWuWPfSrnlS69UExEJBRYDvwfeBXoApwO1x7Et/T6rbk2P0JW7DQEwxrxtjGk0xlQbY74yxmwCEBGbiDwgIlkiki8ir4tImGNZ09H49SKyB1gKLHdst1REKkXkVMe614nIVhEpEZEvRaRfUwEiMl1Etjn+QngOkCMVKyKBIvKqYztbgPGtlu8WkbMdz31E5M8islNEKkRknYgkOJadJCJfO/4i2S4iVzhtY5aIbHG8JldE7jrhT1l1Cxroyt12AI0i8pqInCMivVstn+v4mQYMAHoCz7VaZwowDJgBnOGY18sY09MYs0pELgT+DFwCRAE/AG8DiEgk8CHwABAJ7AQmH6XevwIDHT8zgGuOsu6dwBxgFhAKXAdUiUgw8DXwFhANXAm8ICLDHa9bCPzOGBMCjMD6j0qpNmmgK7cyxpQDpwEGeAkoEJFPRKSPY5WrgCeNMZnGmErgPuDKVs0rDxljDhhjqo+wm5uAfxhjthpjGoC/Ayc7jtJnAWnGmPeNMfXA08D+o5R8BfC/xphiY0w28OxR1v0t8IAxZruxbDTGFAHnAbuNMa8YYxqMMb8AHwCXO15XDwwXkVBjTIkxZv1R9qFUMw105XaOoJ1rjInHOiLtixWsOJ5nOa2ehXXup4/TvOw2dtEPeEZESkWkFCjGalaJc2y/+fXG6q3uaNvr22p51pFWBBKwjvgPV8/EpnocNV0FxDiWX4r1H02WiHzf1GykVFs00FWXYozZBryKFewAe7ECsEki0ADkOb/sCM+bZGM1YfRy+gk0xqwE9mEFLwAiIs7Th7Gv1fLEo6ybjdU0c7j537eqp6cx5vcAxpg1xpgLsZpjPsY6WaxUmzTQlVs5Tg7+UUTiHdMJWO3OPzlWeRu4Q0SSRKQnVnPJO46mk8MpAOxY7e1N/g3cJyLJjn2EiUhT88ZnQLKIXOJoxrmVg0fKh/OuY1u9HTXfcpR1XwYeEZHBYhklIhFYV/UMEZGrRcTP8TNeRIaJSA8RuUpEwhxNQOWO96NUmzTQlbtVABOB1SJyACvIU4E/Opb/B3gD6+qVXUANRwlRY0wV8L/ACkdzxinGmI+Ax4BFIlLu2P45jvULsdquHwWKgMHAiqPU+z9YzSy7gK8ctR3Jk1j/AXyFFcwLgUBjTAXwK6yToXux2uwfA/wdr7sa2O2o9Sas5hil2iQ6wIVSSnkHPUJXSikvoYGulFJeQgNdKaW8hAa6Ukp5Cbd1ZhQZGWn69+/vrt0rpZRHWrduXaExJupwy9wW6P3792ft2rXu2r1SSnkkETni3cna5KKUUl5CA10ppbyEBrpSSnkJDXSllPISGuhKKeUl2gx0EfmPY+iv1CMsFxF5VkQyRGSTiIzt+DKVUkq1pT1H6K8CM4+y/BysHuoGAzcC80+8LKWUUseqzevQjTHLRaT/UVa5EHjdMdLLTyLSS0RijTH7OqpIpVT7GWMwBuzGYHA8mpaPdgM0P7emDS2XmyO8zjht1263HqHl8iM9GsA47ctwcD7O81stM44VDs4/dBu0nu/oSPaI+zjMdnCusUW9R9iH0zQ47dMcfR9nDevD6IReHf5v3xE3FsXRckiuHMe8QwJdRG7EOoonMfFoA70odzPG0Gg3NDoeG+yGxkZDfaOdukY79Y2GBqfn9Y126hvtNDRar7FeD412KzBaPkJDo50Gu2n+ZXcODXCedppnPxg8dsdvSvNrm5db8xpbbds5fBqd9mWt59im/eC2Dgmj5nlHDkUr5FoGofN0U912x2+2c5Da7a2C0hwMFef31xQKzkHcugbtEbvriw4N6LKB3m7GmAXAAoCUlBS3fu2afvkbHb/ojc2/3AeDrDkA7C3DzfmnofnRjt0ODXb7IcvtxlhB59iGFY52Gg002q1gszttq/W2Dz63O0LSftjtNtpNc6A277vFaw8337F+Y6tpe9dPBRGwiSBOz31sYs1rMQ0igo84PXfMd1636bk0r+e8/YOvbXoUwGYT/GyCcHA7B7cH0HJbNhFwWi40vabpdbSo4+DyQ7ctradxrvHg9lpP20SAg6+z2Zzf48F6m18H2GxOdXKUGqTVdjh0vaZ/u6Z/NxyfXVMN0rxcHMtbTrd43sY2HEtabucw+3BsotV2W66H0Gadh9Tael3nFV2gIwI9l5ZjLMY75rlVQUUtDy/eQnpeBYWVddQ2NFLfaG8OtK56FCMCvjYrcHwcwePrY8MmcnC+reXzw0338PPB5jxfBB8fx3TzdsVpuzZ8bOBjs+Frkxavdd52D18bfjYbfr6Cn48NPx8bPXxs+PoIvjYbfj7Wa32afqltNO/feb6voxabzSk0msPuYHC2Dh/nQFVKtdQRgf4JME9EFmENJVbmyvbzLXvL2ZBd6vjT1/lP6oN/ntoNbMwu5fPU/UwbGsWYxN74+9ro4WtrGXxyMFB8bAePoHxsB8PHx2aFR+vXNQWij812hIB0LG9aZrNhs4GvzXbkIHbUo5RSx6PNQBeRt4GpQKSI5AB/BfwAjDH/BpYAs4AMoAq41lXFAvyQXsA/Pt/WrnUn9A/nlWsnuLIcpZTqMtw2pmhKSoo5nt4WK2sbOFDb0KJNsvWf6E3thT18bHrEq5TyKiKyzhiTcrhlbus+93j19Pelp7/Hla2UUi6nt/4rpZSX0EBXSikvoYGulFJeQgNdKaW8hAa6Ukp5CQ10pZTyEhroSinlJTTQlVLKS2igK6WUl9BAV0opL6GBrpRSXkIDXSmlvIQGulJKeQkNdKWU8hIa6Eop5SU00JVSyktooCullJfQQFdKKS+hga6UUl5CA10ppbyEBrpSSnkJDXSllPISGuhKKeUlNNCVUspLaKArpZSX0EBXSikvoYGulFJeQgNdKaU6U025yzatga6UUp2lqhieGAJrFrpk8xroSinVWbZ9Bg3VEDfOJZtvV6CLyEwR2S4iGSJy72GWJ4rIdyLyi4hsEpFZHV+qUkp5uKIMsPlBzCiXbL7NQBcRH+B54BxgODBHRIa3Wu0B4F1jzBjgSuCFji5UKaU8Xlk2hMaCzTWNI+3Z6gQgwxiTaYypAxYBF7ZaxwChjudhwN6OK1EppbxAdSmkfgBhCS7bRXsCPQ7IdprOccxz9hDwGxHJAZYAtxxuQyJyo4isFZG1BQUFx1GuUkp5qH0brcfhF7lsFx113D8HeNUYEw/MAt4QkUO2bYxZYIxJMcakREVFddCulVLKA+RvtR6TL3LZLtoT6LmA898I8Y55zq4H3gUwxqwCAoDIjihQKaW8woF8EB8Idt3BbHsCfQ0wWESSRKQH1knPT1qtswc4C0BEhmEFurapKKVUk6oiCIoAEZftos1AN8Y0APOAL4GtWFezpInIwyJygWO1PwI3iMhG4G1grjHGuKpopZTyOE2B7kK+7VnJGLME62Sn87wHnZ5vASZ3bGlKKeVFqoohKNylu9A7RZVSqjOU74WefVy6Cw10pZRyNXsjlOVA734u3Y0GulJKuVplHtjrXXpTEWigK6WU61WXWo8uPimqga6UUq5WW2E9+oe4dDca6Eop5Woa6Eop5SVqHaMUaaArpZSH0yN0pZTyEk2B3qOnS3ejga6UUq5WsQ98AyEgzKW70UBXSilXy99i3VTkwo65QANdKaVcqyIPdi6FyCEu35UGulJKuVKBY2CL0XNcvisNdKWUcqWsVdZjn2SX70oDXSmlXMXeCOteAf8wCG09FHPH00BXSilX2fOT1THXuf8HPtbwE1uKtlDTUOOS3WmgK6WUq6R/BTY/GHoOAIXVhcxePJv3d7zvkt1poCullKvs2wjRw8DfuqFoR/EOAIb0ds0VLxroSinlCsbA/k0QO6p51vaS7YAGulJKeQ5jYNf31sDQMaObZ+8o2UF0UDS9Anq5ZLftGiRaKaVUOxVmwHtzIW+zdbv/kBkAGGNYn7ee4RHDXbZrPUJXSqmOsm8TvDQNyrJh+sMwb03zOKKbCjex98Bezk4822W71yN0pZTqKB//wWpuuW7JITcSvbjxRYJ8g5iWOM1lu9cjdKWU6ghpH1vNLFPuOSTMtxdv54fcH7hh1A2E9gh1WQka6EopdaLqa+C/N4OPPwyZecjiN7a8QaBvIJcPudylZWiTi1JKnagdX0BdJVz1AUS1vCTRGMMPuT8wvd90wvy1P3SllOq67Hb48n7o1Q8GTD1k8Y6SHRTXFDM6avShr+1geoSulFLHq6rY6nyrPAcufL65vxZnL21+iWC/YGb0n+HycjTQlVLqeNRVwfxJ1vByEYPhpHMPWaXR3siPuT8yK2mWy5tbQANdKaWOjTGwazmseckK81lPQMr1YGvZgt1ob2Rh6kIO1B9gbJ+xnVJau9rQRWSmiGwXkQwRufcI61whIltEJE1E3urYMpVSqgtorIcPb4DXL4AdX8H4G2D8bw8Jc4AP0j/gX7/8i5GRI5neb3qnlNfmEbqI+ADPA9OBHGCNiHxijNnitM5g4D5gsjGmRESiXVWwUkq5hd0OXz8Im9+DUbPhvKegR/BhV62qr+LD9A9JCkvizVlvIi4eHLpJe5pcJgAZxphMABFZBFwIbHFa5wbgeWNMCYAxJr+jC1VKKbcxBj64HtI+hEFnwyULDrtadUM1i7Yt4uXNL1NeV84DEx/otDCH9gV6HJDtNJ0DTGy1zhAAEVkB+AAPGWO+aL0hEbkRuBEgMTHxeOpVSqnOU7QTfnkD0j6Ckt0w7Hy4/LUWq+w/sJ+3tr3Furx17CjeQU1jDeP6jOPOcXcyKmrU4bfrIh11UtQXGAxMBeKB5SIy0hhT6rySMWYBsAAgJSXFdNC+lVKq4/34FHzzENh8IX4CnDqvxcnP6oZqPt35Kf9c808a7A2MjhrNZUMu48zEM0npk9KpR+ZN2hPouUCC03S8Y56zHGC1MaYe2CUiO7ACfk2HVKmUUp3BGFj/Omx6B7JWwEnnwTmPQVh8i9Wq6qv43de/Y0PBBgb1GsTDkx5mZNRINxV9UHsCfQ0wWESSsIL8SuDXrdb5GJgDvCIikVhNMJkdWKdSSrnWgUL46i+w8S2IHApnPWgdlfv6W4vrD/BZ5mdsK97G6n2ryanM4c8T/8wVQ67Ax+bj5uItbQa6MaZBROYBX2K1j//HGJMmIg8Da40xnziW/UpEtgCNwN3GmCJXFq6UUh2mZDe8dKZ15+cpf4AZfwenJpNNBZt4aNVDpJekE9IjhKG9h/KnCX/ijPgz3FfzYYgx7mnKTklJMWvXrnXLvpVSCoDqEti4CJb+L9RXwXVfQMKE5sV5B/J4Zv0zfJr5KT1sPfi/qf/HlPgpbmkfbyIi64wxKYdbpneKKqW6p1XPw4pnoDLPunX/ohcgYQJ2YyejNIPX0l7ji11fUGev4+JBF3Pr2FuJDIx0d9VHpYGulOpeGmrh3Wtgx+cQlwIXzackbgyfZH7K2qVvsD5vPeV15QCclXgWvx/9e4aGD3Vz0e2jga6U6h6MgcW3U7v9C0qqC9gz8VrWxQxiXebbbFp9H9UN1SSGJHJ2v7MZGz2WcX3GER8S3/Z2uxANdKWUVyuvKuSjZQ/w/d4VZEkD+VE9gDjI/xbJX8rQ8KFcMPACLh50McmRyW1uryvTQFdKeZ2q+ipyKnPYXbSVv698mCJTx0AfG6eGjyK+/zQiAyOJDorm5OiTXTrGZ2fTQFdKebzcyly2FW9je/F2Pt35KTmVOc3LwhobeSY4mTOveLfFpYjeSANdKeVRSmtKWZ67nNTCVEpqSsitzGVz4WYABGGAfzg3V9TSv7qCmKA+nDTlIQJGXOrmqjuHBrpSqktqsDeQXpLOtuJt5FXlkVaYxs6yneRU5GAwBPoGEh0UTZ+gPlwz/BqmVVVzUupignf9AmGJcOWbkDDe3W+jU2mgK6W6BGMM6/PXs2jbIjLLMsmuyKa6obp5+YCwAQyPGM75A89nYsxERkaOxK+2Ar79H1j/KezbABGD4Ix7YMo94OPnvjfjJhroSim32lO+h++yv+PL3V+yuXAzYf5hjIkaQ0qfFEZFjWJk5EgiAyMJ8gs6+KLaSvj6r7Dxbetuz6TTD+l7pTvSQFdKuZQxhrLaMvYe2Mveyr3kVuay/8B+dpfvZlfZLvYd2Ifd2Okf2p8HJj7ABYMuINA38Egbg9QPYOnfoGQXDJkJ0/4MsaM79011URroSqkOUVZbxsaCjaQVpVFeW05eVR67ynaxt3IvVQ1VLdYN8g0iKiiKEZEjOG/AeVw06KK2b+LZ+R0suRuK0q3eEK98G06a5cJ35Hk00JVSx6TR3khBdQFFNUWU15aTVpTGZ5mfkVGaAVhXmgT7BRMeEM6AXgM4JfYUYoNjiesZR9+efenbsy+hPULb38HVrh/g499DWTb06geXvAQjLjvswMzdnQa6UqpN+VX5LNy8kNX7VrOnYg/19voWy5Mjkpl38jzGRI9hROSIlu3dx6O6FHZ+CxnfwoY3wT8Uzn4Ixl4DQeEntm0vpoGulAKsywRzKnKaLw0sryunuKaYjQUbSS9JB+DU2FM5I/4M4kPiiQyMJMw/jKjAKBJCEjqmS9nyvbD6RfhpPjTWgo8/DJ0FFzwHwREnvn0vp4GuVDdkjCGvKo99B/axZv8aPt35KbmVuS2OvAUhpEcIw8KHcdvY25jUdxLDI4Z3fDH1NZC5zOr9cMNb0FgHyZfAhBsgfny3vPzweGmgK9UNlNWWsb14O9uKt7E2by2ZZZlklWc1L58QM4FpCdMY0GsAA8MGkhiaSEiPEGzi4nbq3T/Cp7dbJzp9A2DUbJh0K0QNce1+vZQGulJepsHeQGF1IRsKNrA+bz0/7/uZzLJMDNboZH2C+jCk9xAuHnQxQ8OH0i+0HwkhCW1stYMZA9/81RpgokdPuHShNSCzX0Dn1uFlNNCV8lA1DTUUVhdSXFPM3gN7ySzNJK0ojZW5K2kwDQAE+gYyJnoM0/tPZ0z0GPqH9qdvz77uLbwky7qOfPO7kDARrnoPAsLcW5OX0EBXqovbUrSFn/f9zIGGA5TVlpFVnsXO0p3kVeW1WE8QYoJjuGjwRQztPZThEcMZFjEMP1sXaYO2N1rjd379F2sw5gk3wox/gI/GUEfRT1KpLsAYQ1FNEbvKdpFVnsXust1kVWSRW5nbfIUJQLBfMP1C+zG2z1gG9RpEVGAUEYER9PbvzdDwofTw6eHGd3EU+zfDt49A+pcQMxKu/hhiR7m7Kq+jga5UJ2m0N5JWlMbmws0t7qTMr8qnoLqA2sba5nV72HqQGJpITHAMFw68kPMHnk8v/16uP0nZkfK3QeZ3sOUT2LPSugTx9D/CmX/x+n7J3UUDXSkXKastI7UwlXV561iXt460orQWoR3mH8agXoMYGTWSPkF9iAmOISk0iX5h/YgNjvWs8HZWtBO+uM86Ggfr7s7T74JJt0BgL7eW5u000JU6TrWNteyt3EtRdRG5lblkV2Szp2IPORU57KnYQ1ltGQA+4kNyRDJXDL2CEREjmBA7gd7+vfGx+bj5HXSwshz4+SVY/xoYO0y7H0ZfCWEJekTeSTTQlToMYwwH6g+QX5VPbmUuuZW5lNSWUFFXQVF1ETtKdrCrbBeNprH5NTaxERscS0JIAjP6zSAhJIEhvYdwcvTJJ34rfFdmDGz5GJbcAwfyIXo4XPAviE9xd2Xdjga66rbq7fWU15ZTWlvK2v1r2VOxh4KqAvKq8kgvSaeivuKQ1wT5BtE7oDeDeg1iWsI0ksKSiAiMoG9wX+J6xuHX3e5qLN9rdZyVuQwih8CvF0HcOHdX1W1poCuvZYwhvyqfnaU7ySjNYH/VftJL0smtzKWouuiQLl0DfAKIDoomKiiKGUkz6BfSj6igKPr2tMI6PCAcX5v+ygBQvg+W3AU7vgDxgRl/h4k3gbc1I3kY/XYqr1HXWMeqvav4IfcHdpTsIKM0g4q6g0fZfja/5uuzo4OiCe0RSph/GKE9QukX2o/kiOSO6WDKWx0ogk2LIGetdUReXQyn3Azjr4eIge6uTqGBrjxUXWMdv+T/QlZ5FoszF5NdkU1pTSkNpoFA30CGhQ/jnP7nMLDXQAb1GsSg3oPo7d9bA/t4pH8Nq/8NWaug/gD0jIH+p1lBPmCqu6tTTjTQVZdXVV/F1uKtbC/ezo6SHaSXpJNemt48gHCIXwhn9TuLqMAoRkeNJiUmhWC/YDdX7QUa6mDx7bDpHQiOgpPnwLi51o1BqkvSQFddVoO9ge+yv+NvP/2N4ppiwLp2e2jvoVw6+FJSYlI4KfwkooOiu87t7d6gutQaVOKX/w/yt8Dwi+C8p3RgCQ/QrkAXkZnAM4AP8LIx5tEjrHcp8D4w3hiztsOqVF5vT/ke0orSyCrPoqi6iDX715BVnkWDaSAxJJH7JtzHmOgxRAdFa7OJqxTvgm2L4ecFULrHuvzwvKeto3L9zD1Cm4EuIj7A88B0IAdYIyKfGGO2tFovBLgNWO2KQpV3MMZQUF1ATkUO2RXZ/Jj7I1uLt7bom7unX08G9hrI3BFzGRo+lGkJ0/D38Xdj1V6ufC98cgtkfGNNR50Ec96BoTPdW5c6Zu05Qp8AZBhjMgFEZBFwIbCl1XqPAI8Bd3dohcrj1DbWUlJTQklNCcU1xRRWF7K1eCs/7f2J3MpcahprmtcN7RHKxNiJXDr4Uk7teyoDwgZ03Q6mvEVjPWz7zLoZKC8NCneAzQ+m3Gu1k/fu7+4K1XFqT6DHAdlO0znAROcVRGQskGCM+UxEjhjoInIjcCNAYmLisVeruiRjDD/k/sCSXUv4MffH5lvenfn7+JPSJ4XJcZOJD4knISSBuJ5xxAbHEuCrgxp0iupSa4i3H5+y7ugM6Qt9x1ht5CMugehh7q5QnaATPikqIjbgSWBuW+saYxYACwBSUlLMie5buU9lXSVbi7fyY+6PfLX7K3Iqcwj0DeS0uNMYFj6MMP8wIgIiCA8MJzwgnLiecXpTjjstewyW/d16Hj0czn4IRl2h43V6mfb8huUCzuNTxTvmNQkBRgDLHCerYoBPROQCPTHqPQ7UH2B3+W425m9kff56vsn6prkfk7HRY7l+5PVcOOhCvdqkq6nYDxvfhuX/hPgJMPVeSJqig0p4qfb8q64BBotIElaQXwn8ummhMaYMiGyaFpFlwF0a5p6ttrGWt7a+xaaCTeRV5ZFamNo8JmVv/95cNuQypiZMZXjEcMID9HK2LqmhFl4+G8qyIeEUuOpdHerNy7UZ6MaYBhGZB3yJddnif4wxaSLyMLDWGPOJq4tUrmc3djJKM/g261tSi1JJLUyluKaY/qH9iQqK4qphVzEqahQnR51MTHCMXjrY1ZXlwDtXW2F+2SuQfLFeetgNtOvvLmPMEmBJq3kPHmHdqSdelnK1BnsDG/I38H3O96zcu5LsiuzmOy8Hhg1kROQIpvebzkWDLnJvoap96qog/SvY/jns2wgFW6350x/RMO9GtCGtm6i317OpYBM7S3fyS/4v/JD7A2W1ZfjafBnXZxwTYiYwpPcQa2T4sP7uLle1l91u3Qj0/aNQXWI1qSScAsPOh+EXQswId1eoOpEGuheqa6wjrSiN7cXbWZ6znIzSDPKq8rAbO2C1gU+Jn8KU+ClM6juJnj16urli1W5VxZD9MxTvtG7Lz1plPR8wDSbcAAPPBL9Ad1ep3EQD3Ussz1nOF7u+YFfZLnaU7KDOXgdAZGAkE2MnEt8znsG9BzMyciSxwbHaBu5JqkuhMN0ao3PNy9aROEBgOMSPhyn3wKjZ2qyiNNA9WVltGYszF/NB+gekl6QTHhDOkN5DuPKkKxnbZyzDwocRHRSt1397otQPYOd3kJcKe39xzBRImACn/AGSzoDA3hriqgX9TfdAxTXF/HXlX1m9bzXVDdWMihrFH07+A9cmX6t3XXoye6MV3l//FbJ+BB9/6DMcJt9mHYnHjILe/dxdperCNNA9TGZZJnd9fxfZ5dmcO+BcLhh4AWP7jHV3WepE7PoBcn6G1A+tI3KbL5w6D858QNvD1THRQO/ijDGsz1/P+rz1fJb5GZllmQT7BfPUtKc4Le40d5enjpUxkLvOalLJ3wIlWVCyy1oWFAlT74Ox10BorHvrVB5JA72LWrt/LZ/v+pyl2UsprC4EYFCvQcwbM4/zB5xPbE/9hfco+Vth+xJY96rV17hvgNWnSuwoq0+ViTfpABLqhGmgdzG/5P/Cn3/4MzmVOQT5BjExdiLTEqYxJWGKjonpSYyxbvDZttjqZ7zpxGbCKVZzyug5EBDq3hqV19FAdyO7sbMhfwPZFdmU15WTU5HDRxkf0cu/F3el3MXlQy4nyC/I3WWqttRWWs0mxZmQt8UK8rxU67Z7gL5j4Yy7rRCPGOjeWpVX00DvZMYYdpTsYMXeFXy5+0u2FB0cJ8RXfJmWOI1bxtxCUliSG6tUR9VQC1krrWvD922Eze9CY93B5VHDIG4snP5H625NbUpRnUQDvZNsLtjMezveY0XuCvKr8wEY0nsID576IBNjJhLmH0awX7BeM96V/fwSbPkv7N0AdRXWvB4hMPAsGD0bwgdAWIIGuHIbTQ8XyjuQR3ppOv/N+C9f7P6CQN9ATo87ndPiTmNy3GSig6LdXaI6moY62PktlOfCxnesSwujhsGIi2Hwr6xrw3v20Zt7VJehge4CaUVpLNi4gO+yv8Ng8LP58Zthv+G3I39LRGCEu8tTR1N3ADK+hX0brKPxogxrfnAUTLsfTrtDR/lRXZYGegfaXryd17e8zpLMJQT4BnD+wPO5aNBFDO41mF4Bvdxdnjqaop2w6R34aT7UloPYIGEiTLoVBp0NIbFgs7m7SqWOSgP9BO0q28VH6R/xzZ5vyK7Ixld8uWzIZdw46kaigqLcXZ46mvpq2LUclj4C+zcDAgOmwqRbIPFU6KFXGCnPooF+nOzGztPrn2bRtkXU2+sZHTWaK4deyawBs4gMjGx7A8o9yvdB2odWF7Q7l1pH44Hh8Kv/hRGXQGhfd1eo1HHTQD9G5XXlfJT+EV/s+oLUolQmx03mL6f8hbiece4uTbVWWWDdXl+ea93Ys/VTqNhnLevVD046F5IvgX6TwF/7hFeeTwP9GPyQ8wP/+PkfZFdkMyBsAHeMu4Nrk6/Vuze7mtJs2PweLH8c6qsOzh803eq5sP9pEDPSffUp5SIa6O2UXpLOzd/eTM8ePXl40sNcNOgiDfKuwBiozLOuDc/4BjK+hpLd1rKkM6yTmuEDrKYU7blQeTkN9HZotDdy+3e30zugNx9c8IG2kXcFxsD61+Cnfx8cENkvGBLGQ8r1MHg6RA9zb41KdTIN9Hb4ZOcn7KnYw6OnP6ph3hVUFcPyJ+Cn56HvGJjxd+soPGmKXpmiujUN9DY02Bt4YeMLxATHMDVhqrvL6b4KdsCal2D3CshPs+alXA/n/p/eqamUgwb6UdiNnb+s+Av7D+zn6WlPE+wX7O6Suof6GqsdvGCbdVXK9iVWkPv6Wzf7JF9sndhMPEXDXCknGuhHsCF/A/M3zmfl3pXMHjqbqfFT3V2S96oqhh1fQM5aq/fC3LUtl4fGwym/h1Nv1uvElToKDfTD+DH3R37/ze8JDwjnhpE3MG/MPGyit327RGU+vHKO1WeKf6jVX/ikW6FPsnVSMzQOgiL0SFypdtBAb+W5X57jxU0vEtojlHfOe4eY4Bh3l+Sd6qqs0XwW32GNdv+bD2HANO0vRakToIHuZFPBJl7c9CITYyfqFS2uYIx1JP7TfNjwJjTUWKP5nPcU9D3Z3dUp5fE00B12lOzgtu9uI8w/jIcnPaxh3pFqymDtK9Z148WZ4NPDGhh50NkweIZeaqhUB9FAB4qqi7ht6W3YsPGfGf+hb0898dZh9m2Ct6+0+lOJGQnn/BOGXwQhfdxdmVJep12BLiIzgWcAH+BlY8yjrZbfCfwWaAAKgOuMMVkdXGuHa7A3sGrvKv698d8UVBewcMZChvQe4u6yvEPBdqtpZdO7YPOFqz+CgWe6uyrVBdTX15OTk0NNTY27S+nSAgICiI+Px8+v/QOqtBnoIuIDPA9MB3KANSLyiTFmi9NqvwApxpgqEfk98E9g9jFV38kWbFrAa2mvUV5XTqBvIPdPvJ/RUaPdXZbnK9oJ3zxkXYbo08O6BX/mP/RyQ9UsJyeHkJAQ+vfvr/0hHYExhqKiInJyckhKav+A8e05Qp8AZBhjMgFEZBFwIdAc6MaY75zW/wn4Tbsr6GTfZ3/P0uylfJj+IRNiJjB76GxOjz+dQF/tuOmE1JTB+tdhxTPWdeXj5sIZd2mQq0PU1NRomLdBRIiIiKCgoOCYXteeQI8Dsp2mc4CJR1n/euDzwy0QkRuBGwESExPbWWLHqLfX89bWt3hy3ZPYjZ2JMRN5ZPIjxPaM7dQ6vErxLvjpBcjfavU3XlcJ0ckwdwlEadOVOjIN87Ydz2fUoSdFReQ3QAow5XDLjTELgAUAKSkppiP3fTSN9kYeXf0o7+54l7HRY3n2zGcJ8w/rrN17F2Ngz0/WaD8r/wX2euvSw+EXwsm/tm7JV0q5RXsCPRdIcJqOd8xrQUTOBu4HphhjajumvBOXVZ7FX1b8hV/yf+GqYVfxp/F/0qOD42EMbPkYlj1mdVcrNogfD+c9DX2Gu7s6pY7J7t27Oe+880hNTXV3KR2qPYG+BhgsIklYQX4l8GvnFURkDPAiMNMYk9/hVR6nN7a8wdPrnsbf159/nP4Pzk06V8P8WNntULgD1r0Kq+dD1DA490kYcSkE9nJ3dUopJ20GujGmQUTmAV9iXbb4H2NMmog8DKw1xnwCPA70BN5zBOYeY8wFLqy7TYXVhTy+5nEm9Z3EI5MfISooyp3leKbGenhlFuT8bE0PvxAuXQg+7b+MSqmj+Z9P09iyt7xDtzm8byh/PT+5zfUaGhq46qqrWL9+PcnJybz++ussW7aMO++8k+DgYCZPnkxmZiaLFy/u0PpcqV1t6MaYJcCSVvMedHp+dgfXdUI+3fkpf1/9d0SEO8bdoWF+PMpy4cs/W2F+2h0w5mprEAn9C0d5ie3bt7Nw4UImT57Mddddx5NPPsmLL77I8uXLSUpKYs6cOe4u8Zh53Z2iRdVF/O2nvzEgbAD3TbyPoeFD3V2S5yjeBRvftvof378ZxAeSL4GpfwbfHu6uTnmh9hxJu0pCQgKTJ08G4De/+Q3PPvssAwYMaL7ue86cOSxYsMBt9R0Prwl0YwwfZ3zM8xuep7qhmnsm3MOoqFHuLstz5K6DNy6BmlJIPBXOetC6RT9ioLsrU8olWp9PKysrc1MlHcdr+ip9fO3jPLjyQeoa6/j39H8zJnqMu0vyHI318M7/A79AuGkFXPcFnP5HDXPl1fbs2cOqVasAeOuttzj77LPJzMxk9+7dALzzzjturO74eM0R+srclcT1jOOziz/Dx+bj7nI8x/7N8MV9UJ4Dl7wEMSPcXZFSnWLo0KE8//zzXHfddQwfPpxnn32WUaNGMXPmTIKDgxk/fry7SzxmXhHo24u3k1WexTXJ12iYt8Vuh+zVkPo+7PgSyrLB5gen32V1aatUN9C/f3+2bdt2yPxp06axbds2jDHcfPPNpKSkuKG64+fxgV5WW8YtS28hPCCciwdf7O5yurbc9fD5n6wrV3wDrI6zTrsDTjoXQnRkJqVeeuklXnvtNerq6hgzZgy/+93v3F3SMfHoQC+qLuLSTy6lqKaIV2a8Qr/Qfu4uqWuqLoXlj8Oq56xxOyfdanWcFaDdHyjl7I477uCOO+5wdxnHzWMDvcHewL0/3EtxTTHPnfkcKTGe9adRp9iz2uo8a+snYOww8gqY9U8I7O3uypRSLuCxgf7W1rf4ad9PXD38aqYkHLYvsO4t+2d4ZSYEhsP4G2DoTGsQZr0xSCmv5ZGBfqD+AM9veJ5TYk/h7pS73V1O11JbYZ3s/OxO8PGHW9dr04pS3YRHBvqLm16kqqGKa5Ov1c62mtRWwA9PWgNMmEYIjoKzH9IwV6ob8bhAN8bw1e6vmBw3mUlxk9xdTtdQtBPevQbyNsPQc2HCbyFpKti85r4xpTpUaWkpb731Fn/4wx/cXUqH8rjf+P0H9pNbmcuUeG03t4Z9ewNePguK0q2eEOe8ZQ3GrGGu1BGVlpbywgsvuLsMABobGztsWx53hF7VUAVAb/9ufKVGTbl1GeKal6G+yhox6NKX9VZ95Xk+v9e6W7kjxYyEcx496ir33nsvO3fu5OSTT8bPz48+ffo0d5M7b948UlJSmDt3Lv3792fOnDl8/vnn+Pr6smDBAu677z4yMjK4++67uemmmzDGcM899/D5558jIjzwwAPMnj2bZcuW8cQTTxxxu7Nnz+brr7/mnnvu4corr+yQt+5xgV7TWAOAv4+/mytxg8IM+Pwe2LPKCvKhs+DUeZB4Cugdskq126OPPkpqaiobNmxoDt4jSUxMZMOGDdxxxx3MnTuXFStWUFNTw4gRI7jpppv48MMP2bBhAxs3bqSwsJDx48dzxhlntFlDREQE69ev78i35YGB3uAIdN9uEOh2OxTvhN0/QtZKSP0A/HvC2P8HIy6DBM/ra0KpFto4ku4KLrjAGqtn5MiRVFZWEhISQkhICP7+/pSWlvLjjz8yZ84cfHx86NOnD1OmTGHNmjWEhoYedbuzZ8/u8Fo9LtBrG6zhSgN9A91ciQsVpls3BG1bApX7rXkBvSDlOpg0D3r3d2d1SnkVX19f7HZ783RNTU2L5f7+1sGjzWZrft403dDQcNzbDQ4OPqG6D8fjzpx5bZOL3Q6pH8Jr58NzKfDLmxCfAuc9BfPWwp92w7lPaJgr1QFCQkKoqKgAoF+/fmzZsoXa2lpKS0v59ttvj2lbp59+Ou+88w6NjY0UFBSwfPlyJkyYcMLbPR4ed4Te1OQS4Bvg5ko6QGWBdanhtiWwbTFU7IPeSTDlTzD+t9Az2t0VKuWVIiIimDx5MiNGjOCcc87hiiuuYMSIESQlJTFmzLGNpXDxxRezatUqRo8ejYjwz3/+k5gYq7O7E9nu8RBjjMt3cjgpKSlm7dq1x/y6j9I/4sGVD/LlpV/St2dfF1TWCcr3Wic3t35qTfsGwOBfwbDzrSHffDzu/1ml2m3r1q0MGzbM3WV4hMN9ViKyzhhz2M6rPC45qhuqAQ89Qq8ugQ9ugMzvwN5gHYUPOx/6jtE7OpVSJ8zjAr220TopGuDjQYFelgNfPQC7lkNVEYy7FlKuhdjR7q5MKeVFPC7QpyZMJSY4xjNOihbthC//DBnfgM0XBp4Fp90OCRPcXZlSygt5XKAnhSWRFJbk7jLalrsO/jMT7I3WdePjrrGaVpRSykU8LtA9QsF2eG+ude34NZ9C9Enurkgp1Q143HXoXd7uFfDO1daVLJe/qmGulOo0GugdpboEvv4rvDrLOvE5ZxH0n+zuqpRSLrB7925GjBhx2GVz587l/fff7+SKLNrk0hEa6mDBNCjZZV1PfunLehmiUqrTaaCfiMoCWPmMdct+eS786m9W74c6ipJS7fLYz4+xrXhbh27zpPCT+NOEP7W53uuvv84TTzyBiDBq1CgeeeQRrrvuOgoLC4mKiuKVV14hMTGRvLw8brrpJjIzMwGYP38+ffv2pbGxkRtuuIGVK1cSFxfHf//7XwIDW/YxtW7dOu68804qKyuJjIzk1VdfJTY2lpdeeokFCxZQV1fHoEGDeOONNwgKCjrh965NLseq7gD8/BK8dCY8MRhW/gv6jIDLXoFJt2iYK+UB0tLS+Nvf/sbSpUvZuHEjzzzzDLfccgvXXHMNmzZt4qqrruLWW28F4NZbb2XKlCls3LiR9evXk5ycDEB6ejo333wzaWlp9OrViw8++KDFPurr67nlllt4//33WbduHddddx33338/AJdccglr1qxh48aNDBs2jIULF3bI+9Ij9PaoO2B1wr93A6x/HfLTIHwgnHE3JF8EfZLdXaFSHqk9R9KusHTpUi6//HIiIyMBCA8PZ9WqVXz44YcAXH311dxzzz3N677++usA+Pj4EBYWRklJCUlJSZx88skAjBs3jt27d7fYx/bt20lNTWX69OmANTJRbGwsAKmpqTzwwAOUlpZSWVnJjBkzOuR9tSvQRWQm8AzgA7xsjHm01XJ/4HVgHFAEzDbG7O6QCt2lsR4q8+G/f7Du8DSObjCDIuDX78KQjvkHUEp5JueudH18fKiurm6x3BhDcnIyq1atOuS1c+fO5eOPP2b06NG8+uqrLFu2rENqarPJRUR8gOeBc4DhwBwRGd5qteuBEmPMIOAp4LEOqc7V7HZrXM78bZD2Eax6ARZdBU+NgEci4anhkLnMulX/yrfhj9vh7p0a5kp5uDPPPJP33nuPoqIiAIqLi5k0aRKLFi0C4M033+T0008H4KyzzmL+/PmAdZRdVlbWrn0MHTqUgoKC5kCvr68nLS0NgIqKCmJjY6mvr+fNN9/ssPfVniP0CUCGMSYTQEQWARcCW5zWuRB4yPH8feA5ERHjiq4c179htVsbO2CsR2MHYxw/zvOdlreYZ8A0Wk0ptCoxNA7ix1t3d/qHQswI6H9ah78NpZT7JCcnc//99zNlyhR8fHwYM2YM//rXv7j22mt5/PHHm0+KAjzzzDPceOONLFy4EB8fH+bPn9/cdHI0PXr04P333+fWW2+lrKyMhoYGbr/9dpKTk3nkkUeYOHEiUVFRTJw4sblv9hPVZve5InIZMNMY81vH9NXARGPMPKd1Uh3r5DimdzrWKWy1rRuBGwESExPHZWVlHXvF2z6DTe9aJx/FZv3Q9NzpEaflTfNbzLNZw7n5h0BwFEQPh5BYq0nFpueKlXIV7T63/bp097nGmAXAArD6Qz+ujZx0rvWjlFKqhfYciuYCCU7T8Y55h11HRHyBMKyTo0oppTpJewJ9DTBYRJJEpAdwJfBJq3U+Aa5xPL8MWOqS9nOllFfQeGjb8XxGbQa6MaYBmAd8CWwF3jXGpInIwyJygWO1hUCEiGQAdwL3HnMlSqluISAggKKiIg31ozDGUFRUREDAsQ3k43FjiiqlPFt9fT05OTnU1NS4u5QuLSAggPj4ePz8/FrM7zInRZVSys/Pj6QkDxikxgPp9XlKKeUlNNCVUspLaKArpZSXcNtJUREpAI7jVlEAIoHCNtfqPvTzaEk/j4P0s2jJGz6PfsaYqMMtcFugnwgRWXuks7zdkX4eLenncZB+Fi15++ehTS5KKeUlNNCVUspLeGqgL3B3AV2Mfh4t6edxkH4WLXn15+GRbehKKaUO5alH6EoppVrRQFdKKS/hcYEuIjNFZLuIZIiI1/fqKCIJIvKdiGwRkTQRuc0xP1xEvhaRdMdjb8d8EZFnHZ/PJhEZ69534Boi4iMiv4jIYsd0koisdrzvdxxdPSMi/o7pDMfy/m4t3AVEpJeIvC8i20Rkq4ic2l2/HyJyh+P3JFVE3haRgO703fCoQG/ngNXepgH4ozFmOHAKcLPjPd8LfGuMGQx8y8Eui88BBjt+bgTmd37JneI2rO6cmzwGPOUYqLwEa+By8NQBzI/NM8AXxpiTgNFYn0u3+36ISBxwK5BijBkB+GCN39B9vhvGGI/5AU4FvnSavg+4z911dfJn8F9gOrAdiHXMiwW2O56/CMxxWr95PW/5wRo161vgTGAxIFh3//m2/p5g9eN/quO5r2M9cfd76MDPIgzY1fo9dcfvBxAHZAPhjn/rxcCM7vTd8KgjdA7+gzXJcczrFhx/Eo4BVgN9jDH7HIv2A30cz7vDZ/Q0cA9gd0xHAKXGGowFWr7n5s/DsbzMsb63SAIKgFccTVAvi0gw3fD7YYzJBZ4A9gD7sP6t19GNvhueFujdloj0BD4AbjfGlDsvM9YhRre4/lREzgPyjTHr3F1LF+ELjAXmG2PGAAdoNWJYd/l+OM4TXIj1n1xfIBiY6daiOpmnBXp7Bqz2OiLihxXmbxpjPnTMzhORWMfyWCDfMd/bP6PJwAUishtYhNXs8gzQyzFAObR8z94+gHkOkGOMWe2Yfh8r4Lvj9+NsYJcxpsAYUw98iPV96TbfDU8L9PYMWO1VRESwxmzdaox50mmR88Dc12C1rTfN/3+OqxlOAcqc/vT2eMaY+4wx8caY/lj//kuNMVcB32ENUA6Hfh5eO4C5MWY/kC0iQx2zzgK20D2/H3uAU0QkyPF70/RZdJ/vhrsb8Y/jxMcsYAewE7jf3fV0wvs9DevP5U3ABsfPLKy2vm+BdOAbINyxvmBdCbQT2Ix1xt/t78NFn81UYLHj+QDgZyADeA/wd8wPcExnOJYPcHfdLvgcTgbWOr4jHwO9u+v3A/gfYBuQCrwB+Hen74be+q+UUl7C05pclFJKHYEGulJKeQkNdKWU8hIa6Eop5SU00JVSyktooCullJfQQFdKKS/x/wNzc9bIcBm/hQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"swapdir\", swap_dir)\n",
    "print(\"unique moving_ids\", len(moving_ids), sorted(moving_ids))\n",
    "print(\"unique fixed_ids\", len(fixed_ids), sorted(fixed_ids))\n",
    "# ld_data_dict = torch.load(THIS_SCRIPT_DIR+\"/data/crossmoda_convex.pth\")\n",
    "ld_data_dict = data_dict\n",
    "dices = []\n",
    "for fid, fx in ld_data_dict.items():\n",
    "    for mid, mov in fx.items():\n",
    "        dices.append(mov['dice'])\n",
    "\n",
    "tens_dices = torch.cat(dices)\n",
    "\n",
    "plt.plot(sorted(tens_dices[:,0]), label='bg')\n",
    "plt.plot(sorted(tens_dices[:,1]), label='tumour')\n",
    "plt.plot(sorted(tens_dices[:,2]), label='cochlea')\n",
    "plt.title(\"Sorted dices\")\n",
    "plt.legend()\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "print(\"Quantile tumour: \", np.quantile(tens_dices[:,1], [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]))\n",
    "print(\"Quantile cochlea: \", np.quantile(tens_dices[:,2], [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "285b9633-121d-4a82-8462-b3746824df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6564920544624329, 0.6615490913391113, 0.66754150390625, 0.6853525638580322, 0.6961158514022827, 0.7095019817352295, 0.7232212424278259, 0.7998449206352234, 0.8001408576965332, 0.8199121356010437]\n",
      "torch.Size([10, 128, 128, 128])\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "gt = []\n",
    "wpl = []\n",
    "tumour_dices = []\n",
    "comb_keys = []\n",
    "\n",
    "for key in fixed_keys:\n",
    "    for mov_key in list(warped_label_dict[key].keys()):\n",
    "        warped_label = warped_label_dict[key][mov_key]\n",
    "        tumour_dice = ld_data_dict[key][mov_key]['dice'][0][1].item()\n",
    "        orig_label = orig_label_dict[key]\n",
    "        gt.append(orig_label)\n",
    "        wpl.append(warped_label)\n",
    "        tumour_dices.append(tumour_dice)\n",
    "        comb_keys.append(f\"f{key}_m{mov_key}\")\n",
    "\n",
    "wpl = torch.stack(wpl).to_dense()\n",
    "gt = torch.stack(gt)\n",
    "srt = torch.tensor(np.argsort(tumour_dices)).long()[-10:]\n",
    "tumour_dices = [tumour_dices[idx] for idx in srt]\n",
    "wpl = wpl[srt]\n",
    "gt = gt[srt]\n",
    "\n",
    "comb_keys = [comb_keys[idx] for idx in srt]\n",
    "print(tumour_dices)\n",
    "print(wpl.shape)\n",
    "overlay_text_list = [f\"{dc:.2f}_{key}\" for key, dc in zip(comb_keys, tumour_dices)]\n",
    "\n",
    "frame_elements = [idx%2==0 for idx in range(len(overlay_text_list))]\n",
    "\n",
    "visualize_seg(in_type=\"batch_3D\", reduce_dim=\"W\",\n",
    "    img=gt.unsqueeze(1), # Expert label in BW\n",
    "    seg=wpl.long().cpu(), # Prediction in blue\n",
    "    ground_truth=gt.long(), # Modified label in red\n",
    "    # crop_to_non_zero_seg=True,\n",
    "    crop_to_non_zero_gt=True,\n",
    "    alpha_seg = .5,\n",
    "    alpha_gt = .5,\n",
    "    n_per_row=35,\n",
    "    overlay_text=overlay_text_list,\n",
    "    annotate_color=(0,255,255),\n",
    "    frame_elements=frame_elements,\n",
    "    file_path=\"out.png\",\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0adbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    # Print bare 2D data\n",
    "    # print(\"Displaying 2D bare sample\")\n",
    "    # for img, label in zip(training_dataset.img_data_2d.values(),\n",
    "    #                       training_dataset.label_data_2d.values()):\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=img.unsqueeze(0),\n",
    "    #                 ground_truth=label,\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "    # Print transformed 2D data\n",
    "    training_dataset.train(use_modified=False, augment=True)\n",
    "    print(training_dataset.disturbed_idxs)\n",
    "\n",
    "    print(\"Displaying 2D training sample\")\n",
    "    for dist_stren in [0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 5.0]:\n",
    "        print(dist_stren)\n",
    "        training_dataset.disturb_idxs(list(range(0,20,2)),\n",
    "            disturbance_mode=LabelDisturbanceMode.AFFINE,\n",
    "            disturbance_strength=dist_stren\n",
    "        )\n",
    "        img_stack = []\n",
    "        label_stack = []\n",
    "        mod_label_stack = []\n",
    "\n",
    "        for sample in (training_dataset[idx] for idx in range(20)):\n",
    "            img_stack.append(sample['image'])\n",
    "            label_stack.append(sample['label'])\n",
    "            mod_label_stack.append(sample['modified_label'])\n",
    "\n",
    "        # Change label num == hue shift for display\n",
    "        img_stack = torch.stack(img_stack).unsqueeze(1)\n",
    "        label_stack = torch.stack(label_stack)\n",
    "        mod_label_stack = torch.stack(mod_label_stack)\n",
    "\n",
    "        mod_label_stack*=4\n",
    "\n",
    "        visualize_seg(in_type=\"batch_2D\",\n",
    "            img=img_stack,\n",
    "            # ground_truth=label_stack,\n",
    "            seg=(mod_label_stack-label_stack).abs(),\n",
    "            # crop_to_non_zero_gt=True,\n",
    "            crop_to_non_zero_seg=True,\n",
    "            alpha_seg = .6,\n",
    "            file_path=f'out{dist_stren}.png'\n",
    "        )\n",
    "\n",
    "    # Print transformed 3D data\n",
    "    # training_dataset.train()\n",
    "    # print(\"Displaying 3D training sample\")\n",
    "    # leng = 1# training_dataset.__len__(use_2d_override=False)\n",
    "    # for sample in (training_dataset.get_3d_item(idx) for idx in range(leng)):\n",
    "    #     # training_dataset.set_dilate_kernel_size(1)\n",
    "    #     visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "    #                 img=sample['image'].unsqueeze(0),\n",
    "    #                 ground_truth=sample['label'],\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "#         # training_dataset.set_dilate_kernel_size(7)\n",
    "#         display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "#                     img=sample['image'].unsqueeze(0),\n",
    "#                     ground_truth=sample['modified_label'],\n",
    "#                     crop_to_non_zero_gt=True,\n",
    "#                     alpha_gt = .3)\n",
    "\n",
    "    for sidx in [0,]:\n",
    "        print(f\"Sample {sidx}:\")\n",
    "\n",
    "        training_dataset.eval()\n",
    "        sample_eval = training_dataset.get_3d_item(sidx)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "\n",
    "        training_dataset.train()\n",
    "        print(\"Train sample with ground-truth overlay\")\n",
    "        sample_train = training_dataset.get_3d_item(sidx)\n",
    "        print(sample_train['label'].unique())\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_train['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_train['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt=.3)\n",
    "\n",
    "        print(\"Eval/train diff with diff overlay\")\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=(sample_eval['image'] - sample_train['image']).unsqueeze(0),\n",
    "                    ground_truth=(sample_eval['label'] - sample_train['label']).clamp(min=0),\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "    train_plotset = (training_dataset.get_3d_item(idx) for idx in (55, 81, 63))\n",
    "    for sample in train_plotset:\n",
    "        print(f\"Sample {sample['dataset_idx']}:\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .6)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use module.named_modules() to retrieve valid keychains for layers.\n",
    "       e.g.\n",
    "       first_keychain = list(module.keys())[0]\n",
    "       new_first_replacee = torch.nn.Conv1d(1,2,3)\n",
    "       set_module(first_keychain, torch.nn.Conv1d(1,2,3))\n",
    "    \"\"\"\n",
    "\n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(_path, **statefuls):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "    _path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for name, stful in statefuls.items():\n",
    "        if stful != None:\n",
    "            torch.save(stful.state_dict(), _path.joinpath(name+'.pth'))\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config, dataset_len, num_classes, THIS_SCRIPT_DIR, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        # Use vanilla torch model\n",
    "        lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(\n",
    "            pretrained=False, progress=True, num_classes=num_classes\n",
    "        )\n",
    "        set_module(lraspp, 'backbone.0.0',\n",
    "            torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2),\n",
    "                            padding=(1, 1), bias=False)\n",
    "        )\n",
    "    else:\n",
    "        # Use custom 3d model\n",
    "        lraspp = MobileNet_LRASPP_3D(\n",
    "            in_num=in_channels, num_classes=num_classes,\n",
    "            use_checkpointing=True\n",
    "        )\n",
    "\n",
    "    # lraspp.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "    lraspp.to(device)\n",
    "    print(f\"Param count lraspp: {sum(p.numel() for p in lraspp.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(lraspp.parameters(), lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # Add data paramters embedding and optimizer\n",
    "    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len, 1, sparse=True)\n",
    "        # p_offset = torch.zeros(1, layout=torch.strided, requires_grad=True)\n",
    "        # p_offset.grad = torch.sparse_coo_tensor([[0]], 1., size=(1,))\n",
    "\n",
    "        # embedding.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "        # embedding.sigmoid_offset.register_hook(lambda grad: torch.sparse_coo_tensor([[0]], grad, size=(1,)))\n",
    "        embedding = embedding.to(device)\n",
    "\n",
    "    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len*config.grid_size_y*config.grid_size_x, 1, sparse=True).to(device)\n",
    "    else:\n",
    "        embedding = None\n",
    "\n",
    "\n",
    "    if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "        optimizer_dp = torch.optim.SparseAdam(\n",
    "            embedding.parameters(), lr=config.lr_inst_param,\n",
    "            betas=(0.9, 0.999), eps=1e-08)\n",
    "        torch.nn.init.normal_(embedding.weight.data, mean=config.init_inst_param, std=0.00)\n",
    "\n",
    "        if _path and _path.is_dir():\n",
    "            print(f\"Loading embedding and dp_optimizer from {_path}\")\n",
    "            optimizer_dp.load_state_dict(torch.load(_path.joinpath('optimizer_dp.pth'), map_location=device))\n",
    "            embedding.load_state_dict(torch.load(_path.joinpath('embedding.pth'), map_location=device))\n",
    "\n",
    "        print(f\"Param count embedding: {sum(p.numel() for p in embedding.parameters())}\")\n",
    "\n",
    "    else:\n",
    "        optimizer_dp = None\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading lr-aspp model, optimizers and grad scalers from {_path}\")\n",
    "        lraspp.load_state_dict(torch.load(_path.joinpath('lraspp.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "\n",
    "    return (lraspp, optimizer, optimizer_dp, embedding, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f1722",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "\n",
    "\n",
    "\n",
    "def save_parameter_figure(_path, title, text, parameters, reweighted_parameters, dices):\n",
    "    # Show weights and weights with compensation\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12, 4), dpi=80)\n",
    "    sc1 = axs[0].scatter(\n",
    "        range(len(parameters)),\n",
    "        parameters.cpu().detach(), c=dices,s=1, cmap='plasma', vmin=0., vmax=1.)\n",
    "    sc2 = axs[1].scatter(\n",
    "        range(len(reweighted_parameters)),\n",
    "        reweighted_parameters.cpu().detach(), s=1,c=dices, cmap='plasma', vmin=0., vmax=1.)\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.text(0, 0, text)\n",
    "    axs[0].set_title('Bare parameters')\n",
    "    axs[1].set_title('Reweighted parameters')\n",
    "    axs[0].set_ylim(-10, 10)\n",
    "    axs[1].set_ylim(-3, 1)\n",
    "    plt.colorbar(sc2)\n",
    "    plt.savefig(_path)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "\n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "\n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "\n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "\n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio\n",
    "\n",
    "def log_data_parameter_stats(log_path, epx, data_parameters):\n",
    "    \"\"\"Log stats for data parameters on wandb.\"\"\"\n",
    "    wandb.log({f'{log_path}/highest': torch.max(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/lowest': torch.min(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/mean': torch.mean(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/std': torch.std(data_parameters).item()}, step=epx)\n",
    "\n",
    "\n",
    "\n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_class_dices(log_prefix, log_postfix, class_dices, log_idx):\n",
    "    if not class_dices:\n",
    "        return\n",
    "\n",
    "    for cls_name in class_dices[0].keys():\n",
    "        log_path = f\"{log_prefix}{cls_name}{log_postfix}\"\n",
    "\n",
    "        cls_dices = list(map(lambda dct: dct[cls_name], class_dices))\n",
    "        mean_per_class =np.nanmean(cls_dices)\n",
    "        print(log_path, f\"{mean_per_class*100:.2f}%\")\n",
    "        wandb.log({log_path: mean_per_class}, step=log_idx)\n",
    "\n",
    "def CELoss(logits, targets, bin_weight=None):\n",
    "    # -logits[0,targets[0,0,0],0,0]+torch.log(logits[0,:,0,0].exp().sum())\n",
    "    targets = torch.nn.functional.one_hot(targets).permute(0,3,1,2)\n",
    "    loss = -targets*F.log_softmax(logits, 1)\n",
    "\n",
    "    if bin_weight is not None:\n",
    "        brdcast_shape = torch.Size((loss.shape[0], loss.shape[1], *((loss.dim()-2)*[1])))\n",
    "        inv_weight = (bin_weight+np.exp(1)).log()+np.exp(1)\n",
    "        inv_weight = inv_weight/inv_weight.mean()\n",
    "        loss = inv_weight.view(brdcast_shape)*loss\n",
    "\n",
    "    return loss.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embedding_idxs(idxs, grid_size_y, grid_size_x):\n",
    "    with torch.no_grad():\n",
    "        t_sz = grid_size_y * grid_size_x\n",
    "        return ((idxs*t_sz).long().repeat(t_sz).view(t_sz, idxs.numel())+torch.tensor(range(t_sz)).to(idxs).view(t_sz,1)).permute(1,0).reshape(-1)\n",
    "\n",
    "def inference_wrap(lraspp, img, use_2d, use_mind):\n",
    "    with torch.inference_mode():\n",
    "        b_img = img.unsqueeze(0).unsqueeze(0).float()\n",
    "        if use_2d and use_mind:\n",
    "            # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "            b_img = mindssc(b_img.unsqueeze(0)).squeeze(2)\n",
    "        elif not use_2d and use_mind:\n",
    "            # MIND 3D in Bx1xDxHxW out BxMINDxDxHxW\n",
    "            b_img = mindssc(b_img)\n",
    "        elif use_2d or not use_2d:\n",
    "            # 2D Bx1xHxW\n",
    "            # 3D out Bx1xDxHxW\n",
    "            pass\n",
    "\n",
    "        b_out = lraspp(b_img)['out']\n",
    "        b_out = b_out.argmax(1)\n",
    "        return b_out\n",
    "\n",
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "\n",
    "    fold_iter = enumerate(kf.split(training_dataset))\n",
    "\n",
    "    if config.get('fold_override', None):\n",
    "        selected_fold = config.get('fold_override', 0)\n",
    "        fold_iter = list(fold_iter)[selected_fold:selected_fold+1]\n",
    "    elif config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "\n",
    "    if config.wandb_mode != 'disabled':\n",
    "        # Log dataset info\n",
    "        training_dataset.eval()\n",
    "        dataset_info = [[smp['dataset_idx'], smp['id'], smp['image_path'], smp['label_path']] \\\n",
    "                        for smp in training_dataset]\n",
    "        wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', 'id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    "\n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        # Training happens in 2D, validation happens in 3D:\n",
    "        # Read 2D dataset idxs which are used for training,\n",
    "        # get their 3D super-ids and substract these from all 3D ids to get val_3d_idxs\n",
    "\n",
    "        if config.use_2d_normal_to is not None:\n",
    "            n_dims = (-2,-1)\n",
    "            trained_3d_dataset_ids = training_dataset.get_3d_from_2d_identifiers(train_idxs, 'id')\n",
    "            # trained_3d_trained_ids = training_dataset.switch_3d_identifiers(trained_3d_dataset_idxs)\n",
    "            all_3d_ids = training_dataset.get_3d_ids()\n",
    "            val_3d_ids = set(all_3d_ids) - set(trained_3d_dataset_ids)\n",
    "            val_3d_idxs = list({\n",
    "                training_dataset.extract_short_3d_id(_id):idx \\\n",
    "                    for idx, _id in enumerate(all_3d_ids) if _id in val_3d_ids}.values())\n",
    "        else:\n",
    "            n_dims = (-3,-2,-1)\n",
    "            val_3d_idxs = val_idxs\n",
    "        print(\"Will run validation with these 3D samples:\", val_3d_idxs)\n",
    "\n",
    "        _, _, all_modified_segs = training_dataset.get_data()\n",
    "\n",
    "        non_empty_train_idxs = train_idxs[(all_modified_segs[train_idxs].sum(dim=n_dims) > 0)]\n",
    "\n",
    "        ### Disturb dataset (only non-emtpy idxs)###\n",
    "        proposed_disturbed_idxs = np.random.choice(non_empty_train_idxs, size=int(len(non_empty_train_idxs)*config.disturbed_percentage), replace=False)\n",
    "        proposed_disturbed_idxs = torch.tensor(proposed_disturbed_idxs)\n",
    "        training_dataset.disturb_idxs(proposed_disturbed_idxs,\n",
    "            disturbance_mode=config.disturbance_mode,\n",
    "            disturbance_strength=config.disturbance_strength\n",
    "        )\n",
    "\n",
    "        disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "        disturbed_bool_vect[training_dataset.disturbed_idxs] = 1.\n",
    "\n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, training_dataset.disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(training_dataset.disturbed_idxs))\n",
    "\n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in training_dataset.disturbed_idxs])},\n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "\n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels = 12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "\n",
    "        class_weights = 1/(torch.bincount(all_modified_segs.reshape(-1).long())).float().pow(.35)\n",
    "        class_weights /= class_weights.mean()\n",
    "\n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "            sampler=train_subsampler, pin_memory=True, drop_last=False,\n",
    "            # collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "        )\n",
    "\n",
    "        training_dataset.set_augment_at_collate(False)\n",
    "\n",
    "#         val_dataloader = DataLoader(training_dataset, batch_size=config.val_batch_size,\n",
    "#                                     sampler=val_subsampler, pin_memory=True, drop_last=False)\n",
    "\n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        epx_start = config.get('checkpoint_epx', 0)\n",
    "\n",
    "        if config.checkpoint_name:\n",
    "            # Load from checkpoint\n",
    "            _path = f\"{config.mdl_save_prefix}/{config.checkpoint_name}_fold{fold_idx}_epx{epx_start}\"\n",
    "        else:\n",
    "            _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx_start}\"\n",
    "\n",
    "        (lraspp, optimizer, optimizer_dp, embedding, scaler) = get_model(config, len(training_dataset), len(training_dataset.label_tags),\n",
    "            THIS_SCRIPT_DIR=THIS_SCRIPT_DIR, _path=_path, device='cuda')\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=500, T_mult=2)\n",
    "\n",
    "        if optimizer_dp:\n",
    "            scheduler_dp = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer_dp, T_0=500, T_mult=2)\n",
    "        else:\n",
    "            scheduler_dp = None\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare corr coefficient scoring\n",
    "        training_dataset.eval(use_modified=True)\n",
    "        wise_labels, mod_labels = list(zip(*[(sample['label'], sample['modified_label']) \\\n",
    "            for sample in training_dataset]))\n",
    "        wise_labels, mod_labels = torch.stack(wise_labels), torch.stack(mod_labels)\n",
    "\n",
    "        dice_func = dice2d if config.use_2d_normal_to is not None else dice3d\n",
    "\n",
    "        wise_dice = dice_func(\n",
    "            torch.nn.functional.one_hot(wise_labels, len(training_dataset.label_tags)),\n",
    "            torch.nn.functional.one_hot(mod_labels, len(training_dataset.label_tags)),\n",
    "            one_hot_torch_style=True, nan_for_unlabeled_target=False\n",
    "        )\n",
    "\n",
    "        gt_num = (mod_labels > 0).sum(dim=n_dims)\n",
    "        t_metric = (gt_num+np.exp(1)).log()+np.exp(1)\n",
    "\n",
    "        if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "            union_wise_mod_label = torch.logical_or(wise_labels, mod_labels)\n",
    "            union_wise_mod_label = union_wise_mod_label.cuda()\n",
    "\n",
    "        class_weights = class_weights.cuda()\n",
    "        gt_num = gt_num.cuda()\n",
    "        t_metric = t_metric.cuda()\n",
    "\n",
    "        for epx in range(epx_start, config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "\n",
    "            lraspp.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            training_dataset.train(use_modified=(epx >= config.start_disturbing_after_ep))\n",
    "            wandb.log({\"use_modified\": float(training_dataset.use_modified)}, step=global_idx)\n",
    "\n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            class_dices = []\n",
    "\n",
    "            # Load data\n",
    "            for batch_idx, batch in enumerate(train_dataloader):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if optimizer_dp:\n",
    "                    optimizer_dp.zero_grad()\n",
    "\n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "                b_spat_aug_grid = batch['spat_augment_grid']\n",
    "\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "                b_img = b_img.float()\n",
    "\n",
    "                b_img = b_img.cuda()\n",
    "                b_seg_modified = b_seg_modified.cuda()\n",
    "                b_idxs_dataset = b_idxs_dataset.cuda()\n",
    "                b_seg = b_seg.cuda()\n",
    "                b_spat_aug_grid = b_spat_aug_grid.cuda()\n",
    "\n",
    "                if training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "                elif not training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 3D\n",
    "                    b_img = mindssc(b_img.unsqueeze(1))\n",
    "                else:\n",
    "                    b_img = b_img.unsqueeze(1)\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=True):\n",
    "                    assert b_img.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input image for model must be {len(n_dims)+2}D: BxCxSPATIAL but is {b_img.shape}\"\n",
    "\n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    ### Calculate loss ###\n",
    "                    assert logits.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input shape for loss must be BxNUM_CLASSESxSPATIAL but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == len(n_dims)+1, \\\n",
    "                        f\"Target shape for loss must be BxSPATIAL but is {b_seg_modified.shape}\"\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                        # batch_bins = torch.zeros([len(b_idxs_dataset), len(training_dataset.label_tags)]).to(logits.device)\n",
    "                        # bin_list = [slc.view(-1).bincount() for slc in b_seg_modified]\n",
    "                        # for b_idx, _bins in enumerate(bin_list):\n",
    "                        #     batch_bins[b_idx][:len(_bins)] = _bins\n",
    "                        # loss = CELoss(logits, b_seg_modified, bin_weight=batch_bins)\n",
    "\n",
    "                        loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified)\n",
    "                        loss = loss.mean(n_dims)\n",
    "\n",
    "                        bare_weight = embedding(b_idxs_dataset).squeeze()\n",
    "\n",
    "                        weight = torch.sigmoid(bare_weight)\n",
    "                        weight = weight/weight.mean()\n",
    "\n",
    "                        # This improves scores significantly: Reweight with log(gt_numel)\n",
    "                        weight = weight/t_metric[b_idxs_dataset]\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = logits.argmax(1)\n",
    "\n",
    "                        if config.use_risk_regularization:\n",
    "                            p_pred_num = (logits_for_score > 0).sum(dim=n_dims).detach()\n",
    "                            risk_regularization = -weight*p_pred_num/(logits_for_score.shape[-2]*logits_for_score.shape[-1])\n",
    "                            loss = (loss*weight).sum() + risk_regularization.sum()\n",
    "                        else:\n",
    "                            loss = (loss*weight).sum()\n",
    "\n",
    "                    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                        loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified)\n",
    "                        m_dp_idxs = map_embedding_idxs(b_idxs_dataset, config.grid_size_y, config.grid_size_x)\n",
    "                        weight = embedding(m_dp_idxs)\n",
    "                        weight = weight.reshape(-1, config.grid_size_y, config.grid_size_x)\n",
    "                        weight = weight.unsqueeze(1)\n",
    "                        weight = torch.nn.functional.interpolate(\n",
    "                            weight,\n",
    "                            size=(b_seg_modified.shape[-2:]),\n",
    "                            mode='bilinear',\n",
    "                            align_corners=True\n",
    "                        )\n",
    "                        weight = weight/weight.mean()\n",
    "                        weight = F.grid_sample(weight, b_spat_aug_grid,\n",
    "                            padding_mode='border', align_corners=False)\n",
    "                        loss = (loss.unsqueeze(1)*weight).sum()\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = (logits*weight).argmax(1)\n",
    "\n",
    "                    else:\n",
    "                        loss = nn.CrossEntropyLoss(class_weights)(logits, b_seg_modified)\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = logits.argmax(1)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                    scaler.step(optimizer_dp)\n",
    "\n",
    "                scaler.update()\n",
    "\n",
    "                epx_losses.append(loss.item())\n",
    "\n",
    "                # Calculate dice score\n",
    "                b_dice = dice_func(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(b_seg, len(training_dataset.label_tags)), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_over_all(\n",
    "                    b_dice, exclude_bg=True))\n",
    "                class_dices.append(get_batch_dice_per_class(\n",
    "                    b_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                ###  Scheduler management ###\n",
    "                if config.use_cosine_annealing:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if config.save_dp_figures and batch_idx % 10 == 0:\n",
    "                    # Output data parameter figure\n",
    "                    train_params = embedding.weight[train_idxs].squeeze()\n",
    "                    # order = np.argsort(train_params.cpu().detach()) # Order by DP value\n",
    "                    order = torch.arange(len(train_params))\n",
    "                    wise_corr_coeff = np.corrcoef(train_params.cpu().detach(), wise_dice[train_idxs][:,1].cpu().detach())[0,1]\n",
    "                    dp_figure_path = Path(f\"data/output_figures/{wandb.run.name}_fold{fold_idx}/dp_figure_epx{epx:03d}_batch{batch_idx:03d}.png\")\n",
    "                    dp_figure_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    save_parameter_figure(dp_figure_path, wandb.run.name, f\"corr. coeff. DP vs. dice(expert label, train gt): {wise_corr_coeff:4f}\",\n",
    "                        train_params[order], train_params[order]/t_metric[train_idxs][order], dices=wise_dice[train_idxs][:,1][order])\n",
    "\n",
    "                if config.debug:\n",
    "                    break\n",
    "\n",
    "            ### Logging ###\n",
    "            print(f\"### Log epoch {epx} @ {time.time()-t0:.2f}s\")\n",
    "            print(\"### Training\")\n",
    "            ### Log wandb data ###\n",
    "            # Log the epoch idx per fold - so we can recover the diagram by setting\n",
    "            # ref_epoch_idx as x-axis in wandb interface\n",
    "            wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "\n",
    "            mean_loss = torch.tensor(epx_losses).mean()\n",
    "            wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "\n",
    "            mean_dice = np.nanmean(dices)\n",
    "            print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "            wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "\n",
    "            log_class_dices(\"scores/dice_mean_\", f\"_fold{fold_idx}\", class_dices, global_idx)\n",
    "\n",
    "            # Log data parameters of disturbed samples\n",
    "            if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                # Calculate dice score corr coeff (unknown to network)\n",
    "                train_params = embedding.weight[train_idxs].squeeze()\n",
    "                order = np.argsort(train_params.cpu().detach())\n",
    "                wise_corr_coeff = np.corrcoef(train_params[order].cpu().detach(), wise_dice[train_idxs][:,1][order].cpu().detach())[0,1]\n",
    "\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/wise_corr_coeff_fold{fold_idx}': wise_corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                print(f'data_parameters/wise_corr_coeff_fold{fold_idx}', f\"{wise_corr_coeff:.2f}\")\n",
    "\n",
    "                # Log stats of data parameters and figure\n",
    "                log_data_parameter_stats(f'data_parameters/iter_stats_fold{fold_idx}', global_idx, embedding.weight.data)\n",
    "\n",
    "                # Map gridded instance parameters\n",
    "                if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                    m_tr_idxs = map_embedding_idxs(train_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                    ).cuda()\n",
    "                    masks = union_wise_mod_label[train_idxs].float()\n",
    "                    masked_weights = torch.nn.functional.interpolate(\n",
    "                        embedding(m_tr_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                        size=(masks.shape[-2:]),\n",
    "                        mode='bilinear',\n",
    "                        align_corners=True\n",
    "                    ).squeeze(1) * masks\n",
    "                    masked_weights[masked_weights==0.] = float('nan')\n",
    "\n",
    "            if (epx % config.save_every == 0 and epx != 0) \\\n",
    "                or (epx+1 == config.epochs):\n",
    "                _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx}\"\n",
    "                save_model(\n",
    "                    _path,\n",
    "                    lraspp=lraspp,\n",
    "                    optimizer=optimizer, optimizer_dp=optimizer_dp,\n",
    "                    scheduler=scheduler, scheduler_dp=scheduler_dp,\n",
    "                    embedding=embedding,\n",
    "                    scaler=scaler)\n",
    "                (lraspp, optimizer, optimizer_dp, embedding, scaler) = \\\n",
    "                    get_model(\n",
    "                        config, len(training_dataset),\n",
    "                        len(training_dataset.label_tags),\n",
    "                        THIS_SCRIPT_DIR=THIS_SCRIPT_DIR,\n",
    "                        _path=_path, device='cuda')\n",
    "\n",
    "            print()\n",
    "            print(\"### Validation\")\n",
    "            lraspp.eval()\n",
    "            training_dataset.eval()\n",
    "\n",
    "            val_dices = []\n",
    "            val_class_dices = []\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    for val_idx in val_3d_idxs:\n",
    "                        val_sample = training_dataset.get_3d_item(val_idx)\n",
    "                        stack_dim = training_dataset.use_2d_normal_to\n",
    "                        # Create batch out of single val sample\n",
    "                        b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                        b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "\n",
    "                        B = b_val_img.shape[0]\n",
    "\n",
    "                        b_val_img = b_val_img.unsqueeze(1).float().cuda()\n",
    "                        b_val_seg = b_val_seg.cuda()\n",
    "\n",
    "                        if training_dataset.use_2d():\n",
    "                            b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=training_dataset.use_2d_normal_to)\n",
    "\n",
    "                            if config.use_mind:\n",
    "                                # MIND 2D model, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                                b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                            output_val = lraspp(b_val_img_2d)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "                            # Prepare logits for scoring\n",
    "                            # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                            val_logits_for_score = make_3d_from_2d_stack(\n",
    "                                val_logits_for_score.unsqueeze(1), stack_dim, B\n",
    "                            ).squeeze(1)\n",
    "\n",
    "                        else:\n",
    "                            if config.use_mind:\n",
    "                                # MIND 3D model shape BxMINDxDxHxW\n",
    "                                b_val_img = mindssc(b_val_img)\n",
    "                            else:\n",
    "                                # 3D model shape Bx1xDxHxW\n",
    "                                pass\n",
    "\n",
    "                            output_val = lraspp(b_val_img)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "\n",
    "                        b_val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(val_logits_for_score, len(training_dataset.label_tags)),\n",
    "                            torch.nn.functional.one_hot(b_val_seg, len(training_dataset.label_tags)),\n",
    "                            one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        # Get mean score over batch\n",
    "                        val_dices.append(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=True))\n",
    "\n",
    "                        val_class_dices.append(get_batch_dice_per_class(\n",
    "                            b_val_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                        if config.do_plot:\n",
    "                            print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                            print(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=False))\n",
    "                            # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                            display_seg(in_type=\"single_3D\",\n",
    "                                reduce_dim=\"W\",\n",
    "                                img=val_sample['image'].unsqueeze(0).cpu(),\n",
    "                                seg=val_logits_for_score_3d.squeeze(0).cpu(), # CHECK TODO\n",
    "                                ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                crop_to_non_zero_seg=True,\n",
    "                                crop_to_non_zero_gt=True,\n",
    "                                alpha_seg=.3,\n",
    "                                alpha_gt=.0\n",
    "                            )\n",
    "\n",
    "                    mean_val_dice = np.nanmean(val_dices)\n",
    "                    print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                    wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                    log_class_dices(\"scores/val_dice_mean_\", f\"_fold{fold_idx}\", val_class_dices, global_idx)\n",
    "\n",
    "            print()\n",
    "            # End of training loop\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "            # Write sample data\n",
    "\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            all_idxs = torch.tensor(range(len(training_dataset))).cuda()\n",
    "            train_label_snapshot_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/train_label_snapshot.pth\")\n",
    "            seg_viz_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weighted_samples.png\")\n",
    "\n",
    "            train_label_snapshot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if str(config.data_param_mode) == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                dp_weights = embedding(all_idxs)\n",
    "                save_data = []\n",
    "                data_generator = zip(\n",
    "                    dp_weights[train_idxs], \\\n",
    "                    disturbed_bool_vect[train_idxs],\n",
    "                    torch.utils.data.Subset(training_dataset, train_idxs)\n",
    "                )\n",
    "\n",
    "                for dp_weight, disturb_flg, sample in data_generator:\n",
    "                    data_tuple = ( \\\n",
    "                        dp_weight,\n",
    "                        bool(disturb_flg.item()),\n",
    "                        sample['id'],\n",
    "                        sample['dataset_idx'],\n",
    "                        sample['image'],\n",
    "                        sample['label'],\n",
    "                        sample['modified_label'],\n",
    "                        inference_wrap(lraspp, sample['image'].cuda(), use_2d=training_dataset.use_2d(), use_mind=config.use_mind)\n",
    "                    )\n",
    "                    save_data.append(data_tuple)\n",
    "\n",
    "                save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "                (dp_weight, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _imgs,\n",
    "                 _labels, _modified_labels, _predictions) = zip(*save_data)\n",
    "\n",
    "                dp_weight = torch.stack(dp_weight)\n",
    "                dataset_idxs = torch.stack(dataset_idxs)\n",
    "                _imgs = torch.stack(_imgs)\n",
    "                _labels = torch.stack(_labels)\n",
    "                _modified_labels = torch.stack(_modified_labels)\n",
    "                _predictions = torch.stack(_predictions)\n",
    "\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'data_parameters': dp_weight.cpu(),\n",
    "                        'disturb_flags': disturb_flags,\n",
    "                        'd_ids': d_ids,\n",
    "                        'dataset_idxs': dataset_idxs.cpu(),\n",
    "                        'labels': _labels.cpu().to_sparse(),\n",
    "                        'modified_labels': _modified_labels.cpu().to_sparse(),\n",
    "                        'train_predictions': _predictions.cpu().to_sparse(),\n",
    "                    },\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "            elif str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                # Log histogram of clean and disturbed parameters\n",
    "                if not training_dataset.use_2d():\n",
    "                    raise NotImplementedError(\"Script does not support 3D model and GRIDDED_INSTANCE_PARAMS yet.\")\n",
    "\n",
    "                m_all_idxs = map_embedding_idxs(all_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                ).cuda()\n",
    "                masks = union_wise_mod_label[all_idxs].float()\n",
    "                all_weights = torch.nn.functional.interpolate(\n",
    "                    embedding(m_all_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                    size=(masks.shape[-2:]),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=True\n",
    "                ).squeeze(1)\n",
    "\n",
    "                masked_weights = all_weights * masks\n",
    "                masked_weights[masked_weights==0.] = float('nan')\n",
    "                dp_weights = np.nanmean(masked_weights.detach().cpu(), axis=n_dims)\n",
    "\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weightmap.png\")\n",
    "\n",
    "                save_data = []\n",
    "                data_generator = zip(\n",
    "                    dp_weights[train_idxs],\n",
    "                    all_weights[train_idxs],\n",
    "                    disturbed_bool_vect[train_idxs],\n",
    "                    torch.utils.data.Subset(training_dataset,train_idxs)\n",
    "                )\n",
    "\n",
    "                for dp_weight, weightmap, disturb_flg, sample in data_generator:\n",
    "                    data_tuple = (\n",
    "                        dp_weight,\n",
    "                        weightmap,\n",
    "                        bool(disturb_flg.item()),\n",
    "                        sample['id'],\n",
    "                        sample['dataset_idx'],\n",
    "                        sample['image'],\n",
    "                        sample['label'],\n",
    "                        sample['modified_label']\n",
    "                    )\n",
    "                    save_data.append(data_tuple)\n",
    "\n",
    "                save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "\n",
    "                (dp_weight, dp_weightmap, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _2d_imgs,\n",
    "                 _2d_labels, _2d_modified_labels) = zip(*save_data)\n",
    "\n",
    "                dp_weight = torch.stack(dp_weight)\n",
    "                dp_weightmap = torch.stack(dp_weightmap)\n",
    "                dataset_idxs = torch.stack(dataset_idxs)\n",
    "                _2d_imgs = torch.stack(_2d_imgs)\n",
    "                _2d_labels = torch.stack(_2d_labels)\n",
    "                _2d_modified_labels = torch.stack(_2d_modified_labels)\n",
    "                _2d_predictions = torch.stack(_2d_predictions)\n",
    "\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'data_parameters': dp_weight.cpu(),\n",
    "                        'data_parameter_weightmaps': dp_weightmap.cpu(),\n",
    "                        'disturb_flags': disturb_flags,\n",
    "                        'd_ids': d_ids,\n",
    "                        'dataset_idxs': dataset_idxs.cpu(),\n",
    "                        'labels': _2d_labels.cpu().to_sparse(),\n",
    "                        'modified_labels': _2d_modified_labels.cpu().to_sparse(),\n",
    "                        'train_predictions': _2d_predictions.cpu().to_sparse(),\n",
    "                    },\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "                print(\"Writing weight map image.\")\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}_data_parameter_weightmap.png\")\n",
    "                visualize_seg(in_type=\"batch_2D\",\n",
    "                    img=torch.stack(dp_weightmap).unsqueeze(1),\n",
    "                    seg=torch.stack(_2d_modified_labels),\n",
    "                    alpha_seg = 0.,\n",
    "                    n_per_row=70,\n",
    "                    overlay_text=overlay_text_list,\n",
    "                    annotate_color=(0,255,255),\n",
    "                    frame_elements=disturb_flags,\n",
    "                    file_path=weightmap_out_path,\n",
    "                )\n",
    "\n",
    "            if len(training_dataset.disturbed_idxs) > 0:\n",
    "                # Log histogram\n",
    "                separated_params = list(zip(dp_weights[clean_idxs], dp_weights[training_dataset.disturbed_idxs]))\n",
    "                s_table = wandb.Table(columns=['clean_idxs', 'disturbed_idxs'], data=separated_params)\n",
    "                fields = {\"primary_bins\" : \"clean_idxs\", \"secondary_bins\" : \"disturbed_idxs\", \"title\" : \"Data parameter composite histogram\"}\n",
    "                composite_histogram = wandb.plot_table(vega_spec_name=\"rap1ide/composite_histogram\", data_table=s_table, fields=fields)\n",
    "                wandb.log({f\"data_parameters/separated_params_fold_{fold_idx}\": composite_histogram})\n",
    "\n",
    "            # Write out data of modified and un-modified labels and an overview image\n",
    "            print(\"Writing train sample image.\")\n",
    "            # overlay text example: d_idx=0, dp_i=1.00, dist? False\n",
    "            overlay_text_list = [f\"id:{d_id} dp:{instance_p.item():.2f}\" \\\n",
    "                for d_id, instance_p, disturb_flg in zip(d_ids, dp_weight, disturb_flags)]\n",
    "\n",
    "            if training_dataset.use_2d():\n",
    "                reduce_dim = None\n",
    "                in_type = \"batch_2D\"\n",
    "            else:\n",
    "                reduce_dim = \"W\"\n",
    "                in_type = \"batch_3D\"\n",
    "\n",
    "            use_2d = training_dataset.use_2d()\n",
    "            scf = 1/training_dataset.pre_interpolation_factor\n",
    "\n",
    "            show_img = interpolate_sample(b_label=_labels, scale_factor=scf, use_2d=use_2d)[1].unsqueeze(1)\n",
    "            show_seg = interpolate_sample(b_label=4*_predictions.squeeze(1), scale_factor=scf, use_2d=use_2d)[1]\n",
    "            show_gt = interpolate_sample(b_label=_modified_labels, scale_factor=scf, use_2d=use_2d)[1]\n",
    "\n",
    "            visualize_seg(in_type=in_type, reduce_dim=reduce_dim,\n",
    "                img=show_img, # Expert label in BW\n",
    "                seg=show_seg, # Prediction in blue\n",
    "                ground_truth=show_gt, # Modified label in red\n",
    "                crop_to_non_zero_seg=False,\n",
    "                alpha_seg = .5,\n",
    "                alpha_gt = .5,\n",
    "                n_per_row=70,\n",
    "                overlay_text=overlay_text_list,\n",
    "                annotate_color=(0,255,255),\n",
    "                frame_elements=disturb_flags,\n",
    "                file_path=seg_viz_out_path,\n",
    "            )\n",
    "\n",
    "        # End of fold loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fc80a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Config overrides\n",
    "# config_dict['wandb_mode'] = 'disabled'\n",
    "# config_dict['debug'] = True\n",
    "# Model loading\n",
    "# config_dict['checkpoint_name'] = 'treasured-water-717'\n",
    "# # config_dict['fold_override'] = 0\n",
    "# config_dict['checkpoint_epx'] = 39\n",
    "\n",
    "# Define sweep override dict\n",
    "sweep_config_dict = dict(\n",
    "    method='grid',\n",
    "    metric=dict(goal='maximize', name='scores/val_dice_mean_tumour_fold0'),\n",
    "    parameters=dict(\n",
    "        # disturbance_mode=dict(\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        # reg_state=dict(\n",
    "        #     values=['best','combined']\n",
    "        # ),\n",
    "        disturbance_strength=dict(\n",
    "            values=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "        ),\n",
    "        disturbed_percentage=dict(\n",
    "            values=[0.3, 0.6]\n",
    "        ),\n",
    "        data_param_mode=dict(\n",
    "            values=[\n",
    "                DataParamMode.INSTANCE_PARAMS,\n",
    "                DataParamMode.DISABLED,\n",
    "            ]\n",
    "        ),\n",
    "        # use_risk_regularization=dict(\n",
    "        #     values=[False, True]\n",
    "        # )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549637cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def normal_run():\n",
    "    with wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        ) as run:\n",
    "\n",
    "        run_name = run.name\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init() as run:\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "\n",
    "        run_name = run.name\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "\n",
    "if config_dict['do_sweep']:\n",
    "    # Integrate all config_dict entries into sweep_dict.parameters -> sweep overrides config_dict\n",
    "    cp_config_dict = copy.deepcopy(config_dict)\n",
    "    # cp_config_dict.update(copy.deepcopy(sweep_config_dict['parameters']))\n",
    "    for del_key in sweep_config_dict['parameters'].keys():\n",
    "        if del_key in cp_config_dict:\n",
    "            del cp_config_dict[del_key]\n",
    "    merged_sweep_config_dict = copy.deepcopy(sweep_config_dict)\n",
    "    # merged_sweep_config_dict.update(cp_config_dict)\n",
    "    for key, value in cp_config_dict.items():\n",
    "        merged_sweep_config_dict['parameters'][key] = dict(value=value)\n",
    "    # Convert enum values in parameters to string. They will be identified by their numerical index otherwise\n",
    "    for key, param_dict in merged_sweep_config_dict['parameters'].items():\n",
    "        if 'value' in param_dict and isinstance(param_dict['value'], Enum):\n",
    "            param_dict['value'] = str(param_dict['value'])\n",
    "        if 'values' in param_dict:\n",
    "            param_dict['values'] = [str(elem) if isinstance(elem, Enum) else elem for elem in param_dict['values']]\n",
    "\n",
    "        merged_sweep_config_dict['parameters'][key] = param_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(merged_sweep_config_dict, project=\"curriculum_deeplab\")\n",
    "    wandb.agent(sweep_id, function=sweep_run)\n",
    "\n",
    "else:\n",
    "    normal_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7469925",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# def inference_DL(run_name, config, inf_dataset):\n",
    "\n",
    "#     score_dicts = []\n",
    "\n",
    "#     fold_iter = range(config.num_folds)\n",
    "#     if config_dict['only_first_fold']:\n",
    "#         fold_iter = fold_iter[0:1]\n",
    "\n",
    "#     for fold_idx in fold_iter:\n",
    "#         lraspp, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "#         lraspp.eval()\n",
    "#         inf_dataset.eval()\n",
    "#         stack_dim = config.use_2d_normal_to\n",
    "\n",
    "#         inf_dices = []\n",
    "#         inf_dices_tumour = []\n",
    "#         inf_dices_cochlea = []\n",
    "\n",
    "#         for inf_sample in inf_dataset:\n",
    "#             global_idx = get_global_idx(fold_idx, sample_idx, config.epochs)\n",
    "#             crossmoda_id = sample['crossmoda_id']\n",
    "#             with amp.autocast(enabled=True):\n",
    "#                 with torch.no_grad():\n",
    "\n",
    "#                     # Create batch out of single val sample\n",
    "#                     b_inf_img = inf_sample['image'].unsqueeze(0)\n",
    "#                     b_inf_seg = inf_sample['label'].unsqueeze(0)\n",
    "\n",
    "#                     B = b_inf_img.shape[0]\n",
    "\n",
    "#                     b_inf_img = b_inf_img.unsqueeze(1).float().cuda()\n",
    "#                     b_inf_seg = b_inf_seg.cuda()\n",
    "#                     b_inf_img_2d = make_2d_stack_from_3d(b_inf_img, stack_dim=stack_dim)\n",
    "\n",
    "#                     if config.use_mind:\n",
    "#                         b_inf_img_2d = mindssc(b_inf_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "#                     output_inf = lraspp(b_inf_img_2d)['out']\n",
    "\n",
    "#                     # Prepare logits for scoring\n",
    "#                     # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "#                     inf_logits_for_score = make_3d_from_2d_stack(output_inf, stack_dim, B)\n",
    "#                     inf_logits_for_score = inf_logits_for_score.argmax(1)\n",
    "\n",
    "#                     inf_dice = dice3d(\n",
    "#                         torch.nn.functional.one_hot(inf_logits_for_score, 3),\n",
    "#                         torch.nn.functional.one_hot(b_inf_seg, 3),\n",
    "#                         one_hot_torch_style=True\n",
    "#                     )\n",
    "#                     inf_dices.append(get_batch_dice_wo_bg(inf_dice))\n",
    "#                     inf_dices_tumour.append(get_batch_dice_tumour(inf_dice))\n",
    "#                     inf_dices_cochlea.append(get_batch_dice_cochlea(inf_dice))\n",
    "\n",
    "#                     if config.do_plot:\n",
    "#                         print(\"Inference 3D image label/ground-truth\")\n",
    "#                         print(inf_dice)\n",
    "#                         # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "#                         display_seg(in_type=\"single_3D\",\n",
    "#                             reduce_dim=\"W\",\n",
    "#                             img=inf_sample['image'].unsqueeze(0).cpu(),\n",
    "#                             seg=inf_logits_for_score.squeeze(0).cpu(),\n",
    "#                             ground_truth=b_inf_seg.squeeze(0).cpu(),\n",
    "#                             crop_to_non_zero_seg=True,\n",
    "#                             crop_to_non_zero_gt=True,\n",
    "#                             alpha_seg=.4,\n",
    "#                             alpha_gt=.2\n",
    "#                         )\n",
    "\n",
    "#             if config.debug:\n",
    "#                 break\n",
    "\n",
    "#         mean_inf_dice = np.nanmean(inf_dices)\n",
    "#         mean_inf_dice_tumour = np.nanmean(inf_dices_tumour)\n",
    "#         mean_inf_dice_cochlea = np.nanmean(inf_dices_cochlea)\n",
    "\n",
    "#         print(f'inf_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_inf_dice*100:.2f}%\")\n",
    "#         print(f'inf_dice_mean_tumour_fold{fold_idx}', f\"{mean_inf_dice_tumour*100:.2f}%\")\n",
    "#         print(f'inf_dice_mean_cochlea_fold{fold_idx}', f\"{mean_inf_dice_cochlea*100:.2f}%\")\n",
    "#         wandb.log({f'scores/inf_dice_mean_wo_bg_fold{fold_idx}': mean_inf_dice}, step=global_idx)\n",
    "#         wandb.log({f'scores/inf_dice_mean_tumour_fold{fold_idx}': mean_inf_dice_tumour}, step=global_idx)\n",
    "#         wandb.log({f'scores/inf_dice_mean_cochlea_fold{fold_idx}': mean_inf_dice_cochlea}, step=global_idx)\n",
    "\n",
    "#         # Store data for inter-fold scoring\n",
    "#         class_dice_list = inf_dices.tolist()[0]\n",
    "#         for class_idx, class_dice in enumerate(class_dice_list):\n",
    "#             score_dicts.append(\n",
    "#                 {\n",
    "#                     'fold_idx': fold_idx,\n",
    "#                     'crossmoda_id': crossmoda_id,\n",
    "#                     'class_idx': class_idx,\n",
    "#                     'class_dice': class_dice,\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     mean_oa_inf_dice = np.nanmean(torch.tensor([score['class_dice'] for score in score_dicts]))\n",
    "#     print(f\"Mean dice over all folds, classes and samples: {mean_oa_inf_dice*100:.2f}%\")\n",
    "#     wandb.log({'scores/mean_dice_all_folds_samples_classes': mean_oa_inf_dice}, step=global_idx)\n",
    "\n",
    "#     return score_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a608620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds_scores = []\n",
    "# run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"testing\", job_type=\"test\",\n",
    "#         config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "#         mode=config_dict['wandb_mode']\n",
    "# )\n",
    "# config = wandb.config\n",
    "# score_dicts = inference_DL(run_name, config, validation_dataset)\n",
    "# folds_scores.append(score_dicts)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984e8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
