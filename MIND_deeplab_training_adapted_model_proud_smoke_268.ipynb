{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3175666",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name                     Util    Mem free  Cuda             User(s)\n",
      "----  --------------------------  ------  ----------  ---------------  -------------\n",
      "   2  NVIDIA GeForce RTX 2080 Ti     0 %   11016 MiB  11.5(495.29.05)\n",
      "   3  NVIDIA GeForce RTX 2080 Ti     0 %    8834 MiB  11.5(495.29.05)  andresen\n",
      "   0  NVIDIA GeForce RTX 2080 Ti     0 %    2575 MiB  11.5(495.29.05)  schneider\n",
      "   1  NVIDIA GeForce RTX 2080 Ti     0 %      55 MiB  11.5(495.29.05)  grossbroehmer\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   2  NVIDIA GeForce RTX 2080 Ti  ->  cuda:0\n",
      "1.9.0a0+gitdfbd030\n",
      "8204\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Running in: /share/data_supergrover1/weihsbach/shared_data/tmp/curriculum_deeplab\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import glob\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -4\"))\n",
    "import pickle\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import functools\n",
    "from enum import Enum, auto\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import visualize_seg\n",
    "\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "from curriculum_deeplab.utils import interpolate_sample, in_notebook, dilate_label_class, LabelDisturbanceMode\n",
    "from curriculum_deeplab.CrossmodaHybridIdLoader import CrossmodaHybridIdLoader, get_crossmoda_data_load_closure\n",
    "from curriculum_deeplab.MobileNet_LR_ASPP_3D import MobileNet_LRASPP_3D\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "if in_notebook:\n",
    "    THIS_SCRIPT_DIR = os.path.abspath('')\n",
    "else:\n",
    "    THIS_SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "print(f\"Running in: {THIS_SCRIPT_DIR}\")\n",
    "\n",
    "def get_batch_dice_per_class(b_dice, class_tags, exclude_bg=True) -> dict:\n",
    "    score_dict = {}\n",
    "    for cls_idx, cls_tag in enumerate(class_tags):\n",
    "        if exclude_bg and cls_idx == 0:\n",
    "            continue\n",
    "\n",
    "        if torch.all(torch.isnan(b_dice[:,cls_idx])):\n",
    "            score = float('nan')\n",
    "        else:\n",
    "            score = np.nanmean(b_dice[:,cls_idx]).item()\n",
    "\n",
    "        score_dict[cls_tag] = score\n",
    "\n",
    "    return score_dict\n",
    "\n",
    "def get_batch_dice_over_all(b_dice, exclude_bg=True) -> float:\n",
    "\n",
    "    start_idx = 1 if exclude_bg else 0\n",
    "    if torch.all(torch.isnan(b_dice[:,start_idx:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,start_idx:]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5750fa2f-e706-4c65-ac09-c3ab8050b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParamMode(Enum):\n",
    "    INSTANCE_PARAMS = auto()\n",
    "    GRIDDED_INSTANCE_PARAMS = auto()\n",
    "    DISABLED = auto()\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "        See https://stackoverflow.com/questions/49901590/python-using-copy-deepcopy-on-dotdict\n",
    "    \"\"\"\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        try:\n",
    "            return self[item]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError from e\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "\n",
    "    'use_mind': False,\n",
    "    'epochs': 40,\n",
    "\n",
    "    'batch_size': 16,\n",
    "    'val_batch_size': 1,\n",
    "    'use_2d_normal_to': None,\n",
    "    'train_patchwise': True,\n",
    "\n",
    "    'dataset': 'crossmoda',\n",
    "    'reg_state': \"acummulate_convex_adam_FT2_MT1\",\n",
    "    'train_set_max_len': None,\n",
    "    'crop_3d_w_dim_range': (45, 95),\n",
    "    'crop_2d_slices_gt_num_threshold': 0,\n",
    "\n",
    "    'lr': 0.0005,\n",
    "    'use_cosine_annealing': True,\n",
    "\n",
    "    # Data parameter config\n",
    "    'data_param_mode': DataParamMode.INSTANCE_PARAMS,\n",
    "    'init_inst_param': 0.0,\n",
    "    'lr_inst_param': 0.1,\n",
    "    'use_risk_regularization': True,\n",
    "\n",
    "    'grid_size_y': 64,\n",
    "    'grid_size_x': 64,\n",
    "    # ),\n",
    "\n",
    "    'save_every': 200,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'do_plot': False,\n",
    "    'save_dp_figures': False,\n",
    "    'debug': True,\n",
    "    'wandb_mode': 'disabled', # e.g. online, disabled\n",
    "    'checkpoint_name': None,\n",
    "    'do_sweep': False,\n",
    "\n",
    "    'disturbance_mode': LabelDisturbanceMode.AFFINE,\n",
    "    'disturbance_strength': 2.,\n",
    "    'disturbed_percentage': .3,\n",
    "    'start_disturbing_after_ep': 0,\n",
    "\n",
    "    'start_dilate_kernel_sz': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de18b0cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2457023498.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_653929/2457023498.py\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    combined_label_data =\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(config):\n",
    "    if config.reg_state:\n",
    "        print(\"Loading registered data.\")\n",
    "\n",
    "        REG_STATES = [\n",
    "            \"combined\", \"best_1\", \"best_n\",\n",
    "            \"multiple\", \"mix_combined_best\",\n",
    "            \"best\", \"cummulate_combined_best\"]\n",
    "\n",
    "        # assert config.reg_state in REG_STATES, f\"Unknown registration version. Choose one of {REG_STATES}\"\n",
    "\n",
    "        if config.reg_state == \"mix_combined_best\":\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "\n",
    "            perm = np.random.permutation(len(loaded_identifier))\n",
    "            _clen = int(.5*len(loaded_identifier))\n",
    "            best_choice = perm[:_clen]\n",
    "            combined_choice = perm[_clen:]\n",
    "\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)[best_choice]\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)[combined_choice]\n",
    "            label_data = torch.zeros([107,128,128,128])\n",
    "            label_data[best_choice] = best_label_data\n",
    "            label_data[combined_choice] = combined_label_data\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"cummulate_combined_best\":\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)\n",
    "            label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier] + [_id+':var001' for _id in loaded_identifier]\n",
    "        \n",
    "        elif config.reg_state == \"acummulate_convex_adam_FT2_MT1\":\n",
    "            \n",
    "            label_data = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220113_crossmoda_convex/crossmoda_convex.pth\")\n",
    "            combined_label_data = \n",
    "            # best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)\n",
    "            # combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)\n",
    "            # label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            # loaded_identifier = [_id+':var000' for _id in loaded_identifier] + [_id+':var001' for _id in loaded_identifier]\n",
    "\n",
    "        else:\n",
    "            label_data = torch.cat([label_data_left[config.reg_state+'_all'][:44], label_data_right[config.reg_state+'_all'][:63]], dim=0)\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier]\n",
    "        raise(False)\n",
    "        modified_3d_label_override = {}\n",
    "        for idx, identifier in enumerate(loaded_identifier):\n",
    "            nl_id = int(re.findall(r'\\d+', identifier)[0])\n",
    "            var_id = int(re.findall(r':var(\\d+)$', identifier)[0])\n",
    "            lr_id = re.findall(r'([lr])\\.nii\\.gz', identifier)[0]\n",
    "\n",
    "            crossmoda_var_id = f\"{nl_id:03d}{lr_id}:var{var_id:03d}\"\n",
    "\n",
    "            modified_3d_label_override[crossmoda_var_id] = label_data[idx]\n",
    "\n",
    "        prevent_disturbance = True\n",
    "\n",
    "    else:\n",
    "        modified_3d_label_override = None\n",
    "        prevent_disturbance = False\n",
    "                  \n",
    "    if config.dataset == 'crossmoda':\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.\n",
    "        clsre = get_crossmoda_data_load_closure(\n",
    "            base_dir=\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "            domain='source', state='l4', use_additional_data=False,\n",
    "            size=(128,128,128), resample=True, normalize=True, crop_3d_w_dim_range=config.crop_3d_w_dim_range,\n",
    "            ensure_labeled_pairs=True,\n",
    "            debug=config.debug\n",
    "        )\n",
    "        training_dataset = CrossmodaHybridIdLoader(\n",
    "            clsre,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(f\"Nonzero slices: \" \\\n",
    "            f\"{sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)*100}%\"\n",
    "        )\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "    if config.dataset == 'ixi':\n",
    "        raise NotImplementedError()\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.\n",
    "        clsre = get_ixi_data_load_closure()\n",
    "        training_dataset = IXIHybridIdLoader(\n",
    "            clsre,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(f\"Nonzero slices: \" \\\n",
    "            f\"{sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)*100}%\"\n",
    "        )\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "\n",
    "    elif config['dataset'] == 'organmnist3d':\n",
    "        training_dataset = WrapperOrganMNIST3D(\n",
    "            split='train', root='./data/medmnist', download=True, normalize=True,\n",
    "            max_load_num=300, crop_3d_w_dim_range=None,\n",
    "            disturbed_idxs=None, use_2d_normal_to='W'\n",
    "        )\n",
    "        print(training_dataset.mnist_set.info)\n",
    "        print(\"Classes: \", training_dataset.label_tags)\n",
    "        print(\"Samples: \", len(training_dataset))\n",
    "\n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbc5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    _, all_labels, _ = training_dataset.get_data(use_2d_override=False)\n",
    "    print(all_labels.shape)\n",
    "    sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "    plt.xlabel(\"W\")\n",
    "    plt.ylabel(\"ground truth>0\")\n",
    "    plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c2075ba-4d51-4fc1-82fc-9193a7a86043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed_ids [('108r', 108, 'crossmoda_108_Label_r.nii.gz'), ('181r', 181, 'crossmoda_181_Label_r.nii.gz'), ('160r', 160, 'crossmoda_160_Label_r.nii.gz'), ('198r', 198, 'crossmoda_198_Label_r.nii.gz'), ('173r', 173, 'crossmoda_173_Label_r.nii.gz'), ('142r', 142, 'crossmoda_142_Label_r.nii.gz'), ('179r', 179, 'crossmoda_179_Label_r.nii.gz'), ('165r', 165, 'crossmoda_165_Label_r.nii.gz'), ('120r', 120, 'crossmoda_120_Label_r.nii.gz'), ('205r', 205, 'crossmoda_205_Label_r.nii.gz'), ('118r', 118, 'crossmoda_118_Label_r.nii.gz'), ('204r', 204, 'crossmoda_204_Label_r.nii.gz'), ('112r', 112, 'crossmoda_112_Label_r.nii.gz'), ('154r', 154, 'crossmoda_154_Label_r.nii.gz'), ('171r', 171, 'crossmoda_171_Label_r.nii.gz'), ('174r', 174, 'crossmoda_174_Label_r.nii.gz'), ('144r', 144, 'crossmoda_144_Label_r.nii.gz'), ('180r', 180, 'crossmoda_180_Label_r.nii.gz'), ('185r', 185, 'crossmoda_185_Label_r.nii.gz'), ('127r', 127, 'crossmoda_127_Label_r.nii.gz'), ('195r', 195, 'crossmoda_195_Label_r.nii.gz'), ('123r', 123, 'crossmoda_123_Label_r.nii.gz'), ('167r', 167, 'crossmoda_167_Label_r.nii.gz'), ('135r', 135, 'crossmoda_135_Label_r.nii.gz'), ('166r', 166, 'crossmoda_166_Label_r.nii.gz'), ('148r', 148, 'crossmoda_148_Label_r.nii.gz'), ('209r', 209, 'crossmoda_209_Label_r.nii.gz'), ('210r', 210, 'crossmoda_210_Label_r.nii.gz'), ('168r', 168, 'crossmoda_168_Label_r.nii.gz'), ('134r', 134, 'crossmoda_134_Label_r.nii.gz')]\n",
      "moving_ids [('023r', 23, 'crossmoda_23_Label_r.nii.gz'), ('037r', 37, 'crossmoda_37_Label_r.nii.gz'), ('027r', 27, 'crossmoda_27_Label_r.nii.gz'), ('032r', 32, 'crossmoda_32_Label_r.nii.gz'), ('016r', 16, 'crossmoda_16_Label_r.nii.gz'), ('012r', 12, 'crossmoda_12_Label_r.nii.gz'), ('045r', 45, 'crossmoda_45_Label_r.nii.gz'), ('003r', 3, 'crossmoda_3_Label_r.nii.gz'), ('100r', 100, 'crossmoda_100_Label_r.nii.gz'), ('035r', 35, 'crossmoda_35_Label_r.nii.gz'), ('044r', 44, 'crossmoda_44_Label_r.nii.gz'), ('011r', 11, 'crossmoda_11_Label_r.nii.gz'), ('029r', 29, 'crossmoda_29_Label_r.nii.gz'), ('046r', 46, 'crossmoda_46_Label_r.nii.gz'), ('004r', 4, 'crossmoda_4_Label_r.nii.gz'), ('048r', 48, 'crossmoda_48_Label_r.nii.gz'), ('042r', 42, 'crossmoda_42_Label_r.nii.gz'), ('033r', 33, 'crossmoda_33_Label_r.nii.gz'), ('039r', 39, 'crossmoda_39_Label_r.nii.gz'), ('025r', 25, 'crossmoda_25_Label_r.nii.gz'), ('019r', 19, 'crossmoda_19_Label_r.nii.gz'), ('040r', 40, 'crossmoda_40_Label_r.nii.gz'), ('001r', 1, 'crossmoda_1_Label_r.nii.gz'), ('024r', 24, 'crossmoda_24_Label_r.nii.gz'), ('030r', 30, 'crossmoda_30_Label_r.nii.gz'), ('047r', 47, 'crossmoda_47_Label_r.nii.gz'), ('036r', 36, 'crossmoda_36_Label_r.nii.gz'), ('050r', 50, 'crossmoda_50_Label_r.nii.gz'), ('101r', 101, 'crossmoda_101_Label_r.nii.gz'), ('017r', 17, 'crossmoda_17_Label_r.nii.gz')]\n",
      "30\n",
      "0\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_108_L_M100_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_108_Label_r.nii.gz\n",
      "Moving file:  crossmoda_100_Label_r.nii.gz\n",
      "fixed: 108, moving: 100\n",
      "fixed: 108r, moving: 100r\n",
      "tensor([[0.9985, 0.7318, 0.3881]])\n",
      "\n",
      "60\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_112_L_M27_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_112_Label_r.nii.gz\n",
      "Moving file:  crossmoda_27_Label_r.nii.gz\n",
      "fixed: 112, moving: 27\n",
      "fixed: 112r, moving: 027r\n",
      "tensor([[0.9981, 0.5767, 0.4522]])\n",
      "\n",
      "120\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_118_L_M40_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_118_Label_r.nii.gz\n",
      "Moving file:  crossmoda_40_Label_r.nii.gz\n",
      "fixed: 118, moving: 40\n",
      "fixed: 118r, moving: 040r\n",
      "tensor([[0.9786, 0.0000, 0.2587]])\n",
      "\n",
      "240\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_127_L_M24_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_127_Label_r.nii.gz\n",
      "Moving file:  crossmoda_24_Label_r.nii.gz\n",
      "fixed: 127, moving: 24\n",
      "fixed: 127r, moving: 024r\n",
      "tensor([[0.9870, 0.0349, 0.5316]])\n",
      "\n",
      "300\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_134_L_M39_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_134_Label_r.nii.gz\n",
      "Moving file:  crossmoda_39_Label_r.nii.gz\n",
      "fixed: 134, moving: 39\n",
      "fixed: 134r, moving: 039r\n",
      "tensor([[0.9991, 0.5799, 0.5164]])\n",
      "\n",
      "360\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_135_L_M50_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_135_Label_r.nii.gz\n",
      "Moving file:  crossmoda_50_Label_r.nii.gz\n",
      "fixed: 135, moving: 50\n",
      "fixed: 135r, moving: 050r\n",
      "tensor([[0.9964, 0.8422, 0.3422]])\n",
      "\n",
      "480\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_148_L_M36_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_148_Label_r.nii.gz\n",
      "Moving file:  crossmoda_36_Label_r.nii.gz\n",
      "fixed: 148, moving: 36\n",
      "fixed: 148r, moving: 036r\n",
      "tensor([[0.9994, 0.8201, 0.5057]])\n",
      "\n",
      "540\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_154_L_M48_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_154_Label_r.nii.gz\n",
      "Moving file:  crossmoda_48_Label_r.nii.gz\n",
      "fixed: 154, moving: 48\n",
      "fixed: 154r, moving: 048r\n",
      "tensor([[0.9996, 0.7763, 0.4592]])\n",
      "\n",
      "600\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_165_L_M19_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_165_Label_r.nii.gz\n",
      "Moving file:  crossmoda_19_Label_r.nii.gz\n",
      "fixed: 165, moving: 19\n",
      "fixed: 165r, moving: 019r\n",
      "tensor([[0.9989, 0.5797, 0.6719]])\n",
      "\n",
      "660\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_166_L_M35_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_166_Label_r.nii.gz\n",
      "Moving file:  crossmoda_35_Label_r.nii.gz\n",
      "fixed: 166, moving: 35\n",
      "fixed: 166r, moving: 035r\n",
      "tensor([[0.9941, 0.2792, 0.6667]])\n",
      "\n",
      "720\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_167_L_M47_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_167_Label_r.nii.gz\n",
      "Moving file:  crossmoda_47_Label_r.nii.gz\n",
      "fixed: 167, moving: 47\n",
      "fixed: 167r, moving: 047r\n",
      "tensor([[0.9970, 0.3730, 0.5938]])\n",
      "\n",
      "900\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_174_L_M46_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_174_Label_r.nii.gz\n",
      "Moving file:  crossmoda_46_Label_r.nii.gz\n",
      "fixed: 174, moving: 46\n",
      "fixed: 174r, moving: 046r\n",
      "tensor([[0.9973, 0.7208, 0.4883]])\n",
      "\n",
      "960\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_180_L_M12_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_180_Label_r.nii.gz\n",
      "Moving file:  crossmoda_12_Label_r.nii.gz\n",
      "fixed: 180, moving: 12\n",
      "fixed: 180r, moving: 012r\n",
      "tensor([[0.9988, 0.6852, 0.4242]])\n",
      "\n",
      "1020\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_181_L_M30_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_181_Label_r.nii.gz\n",
      "Moving file:  crossmoda_30_Label_r.nii.gz\n",
      "fixed: 181, moving: 30\n",
      "fixed: 181r, moving: 030r\n",
      "tensor([[0.9965, 0.1928, 0.5911]])\n",
      "\n",
      "1140\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([0., 1., 2.], device='cuda:0', dtype=torch.float64)\n",
      "Registred file:  /share/data_supergrover1/heinrich/crossmoda_deeds/Fcrossmoda_198_L_M101_deformed_seg.nii.gz\n",
      "Fixed file:  crossmoda_198_Label_r.nii.gz\n",
      "Moving file:  crossmoda_101_Label_r.nii.gz\n",
      "fixed: 198, moving: 101\n",
      "fixed: 198r, moving: 101r\n",
      "tensor([[0.9985, 0.0000, 0.4225]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convex_path = \"/share/data_supergrover1/heinrich/crossmoda_convex/\"\n",
    "data_path = \"/share/data_supergrover1/heinrich/crossmoda_deeds/\"\n",
    "orig_path = \"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/L4_fine_localized_crop/\"\n",
    "# orig_path = \"/share/data_supergrover1/hansen/temp/crossMoDa/preprocessed_new/resampled/localised_crop/source_training/\"\n",
    "\n",
    "registered_files = glob.glob(data_path+\"*seg.nii.gz\")\n",
    "# print(registered_files)\n",
    "dice_files_right = torch.load(convex_path+\"dice_files_right.pth\")\n",
    "# dice_files_left = torch.load(data_path+\"dice_files_left.pth\")\n",
    "\n",
    "fixed_files_right = dice_files_right['target_tumour_right']\n",
    "fixed_files_right = set([elem[0] for elem in fixed_files_right])\n",
    "# fixed_files_left = dice_files_left['target_tumour_left']\n",
    "# fixed_files_left = set([elem[0] for elem in fixed_files_left])\n",
    "\n",
    "moving_files_right = dice_files_right['source_tumour_right']\n",
    "moving_files_right = set([elem[0] for elem in moving_files_right])\n",
    "# moving_files_left = dice_files_left['source_tumour_left']\n",
    "# moving_files_left = set([elem[0] for elem in moving_files_left])\n",
    "\n",
    "# all_fixed_files = sorted(list(fixed_files_left.union(fixed_files_right)))\n",
    "all_fixed_files = list(fixed_files_right)\n",
    "\n",
    "# all_moving_files = sorted(list(moving_files_left.union(moving_files_right)))\n",
    "\n",
    "all_moving_files = list(moving_files_right)\n",
    "\n",
    "# if swap_dir:\n",
    "#     all_fixed_files, all_moving_files = all_moving_files, all_fixed_files\n",
    "\n",
    "# print(\"Fixed files lengths \", len(all_fixed_files))\n",
    "# print(\"Moving files lengths \", len(all_moving_files))\n",
    "\n",
    "def filter_ids(_file):\n",
    "    num_id, lr_id = re.findall(r\"(\\d{1,3})_Label_([lr])\", _file)[0]\n",
    "    _id = f'{int(num_id):03d}{lr_id}'\n",
    "    numeric_short = int(_id[:3])\n",
    "    return (_id, numeric_short, _file)\n",
    "\n",
    "fixed_ids = [filter_ids(f_name) for f_name in all_fixed_files]\n",
    "moving_ids = [filter_ids(f_name) for f_name in all_moving_files]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "used_fids_short = []\n",
    "used_mids_short = []\n",
    "\n",
    "def get_convex_fixed_moving_num(_file):\n",
    "    fixed_num, moving_num = re.findall(r\"(\\d{1,3})_L(\\d{3})\", _file)[0]\n",
    "    fixed_num, moving_num = int(fixed_num), int(moving_num)\n",
    "    moving_num, fixed_num = fixed_num, moving_num\n",
    "    return fixed_num, moving_num\n",
    "\n",
    "def get_deeds_fixed_moving_num(_file):\n",
    "# Fcrossmoda_210_L_M47_deformed_seg.nii.gz\n",
    "    fixed_num, moving_num = re.findall(r\"(\\d{3})_L_M(\\d{,13})\", _file)[0]\n",
    "    fixed_num, moving_num = int(fixed_num), int(moving_num)\n",
    "    # moving_num, fixed_num = fixed_num, moving_num\n",
    "    return fixed_num, moving_num\n",
    "\n",
    "for gz_file in registered_files:\n",
    "    fixed_num, moving_num = get_deeds_fixed_moving_num(gz_file)\n",
    "    used_fids_short.append(fixed_num)\n",
    "    used_mids_short.append(moving_num)\n",
    "\n",
    "used_fids_short = list(set(used_fids_short))\n",
    "used_mids_short = list(set(used_mids_short))\n",
    "\n",
    "fixed_ids = list(filter(lambda elem: elem[1] in used_fids_short, fixed_ids))\n",
    "moving_ids = list(filter(lambda elem: elem[1] in used_mids_short, moving_ids))\n",
    "\n",
    "print(\"fixed_ids\", fixed_ids)\n",
    "print(\"moving_ids\", moving_ids)\n",
    "\n",
    "orig_label_dict = {}\n",
    "\n",
    "for _id, _, _file in fixed_ids: # TODO\n",
    "    file_path = orig_path + \"__omitted_labels_target_training__/\" + _file\n",
    "    file_path = file_path.replace(\"Label_l\", \"hrT2_l_Label\")\n",
    "    file_path = file_path.replace(\"Label_r\", \"hrT2_r_Label\")\n",
    "    if os.path.isfile(file_path):\n",
    "        orig_label_dict[_id] = torch.tensor(nib.load(file_path).get_fdata())\n",
    "\n",
    "# for _id, _, _file in moving_ids:\n",
    "\n",
    "#     file_path = orig_path + \"source_training_labeled/\" + _file\n",
    "#     file_path = file_path.replace(\"Label_l\", \"ceT1_l_Label\")\n",
    "#     file_path = file_path.replace(\"Label_r\", \"ceT1_r_Label\")\n",
    "\n",
    "#     if os.path.isfile(file_path):\n",
    "#         orig_label_dict[_id] = torch.tensor(nib.load(file_path).get_fdata())\n",
    "\n",
    "warped_label_dict = {}\n",
    "for gz_file in registered_files:\n",
    "    warped_label = torch.tensor(nib.load(gz_file).get_fdata()).cuda()\n",
    "    fixed_num, moving_num = get_deeds_fixed_moving_num(gz_file)\n",
    "\n",
    "    fixed_id = list(filter(lambda elem: elem[1] == fixed_num, fixed_ids))\n",
    "    if not fixed_id:\n",
    "        continue\n",
    "    else:\n",
    "        fixed_id = fixed_id[0][0]\n",
    "    moving_id = list(filter(lambda elem: elem[1] == moving_num, moving_ids))\n",
    "    if not moving_id:\n",
    "        continue\n",
    "    else:\n",
    "        moving_id = moving_id[0][0]\n",
    "    \n",
    "    dct = warped_label_dict.get(fixed_id, {})\n",
    "    dct[moving_id] = warped_label.to_sparse()\n",
    "    warped_label_dict[fixed_id] = dct\n",
    "    \n",
    "print(len(orig_label_dict))\n",
    "\n",
    "for idx, gz_file in enumerate(registered_files):\n",
    "    fixed_num, moving_num = get_deeds_fixed_moving_num(gz_file)\n",
    "\n",
    "    fixed_res = list(filter(lambda elem: elem[1] == fixed_num, fixed_ids))\n",
    "    if not fixed_res:\n",
    "        continue\n",
    "    else:\n",
    "        fixed_id, _, fixed_file = fixed_res[0]\n",
    "\n",
    "    moving_res = list(filter(lambda elem: elem[1] == moving_num, moving_ids))\n",
    "    if not moving_res:\n",
    "        continue\n",
    "    else:\n",
    "        moving_id, _, moving_file = moving_res[0]\n",
    "    dct = data_dict.get(fixed_id, {})\n",
    "    \n",
    "    # orig_label = orig_label_dict[moving_id].cuda() # TODO\n",
    "    orig_label = orig_label_dict[fixed_id].cuda()\n",
    "    warped_label = warped_label_dict[fixed_id][moving_id].to_dense()\n",
    "\n",
    "    dice = dice3d(F.one_hot(orig_label.long(), 3).unsqueeze(0), \n",
    "          F.one_hot(warped_label.long(), 3).unsqueeze(0), one_hot_torch_style=True)\n",
    "\n",
    "    dct[moving_id] = {\n",
    "        'warped_label': warped_label.to_sparse(),\n",
    "        'dice': dice\n",
    "    }\n",
    "    data_dict[fixed_id] = dct\n",
    "    \n",
    "    if idx % 60 == 0:\n",
    "    # if len(orig_label.unique()) != len(warped_label.unique()) or len(orig_label.unique()) < 2:\n",
    "        print(idx)\n",
    "        print(orig_label.unique())\n",
    "        print(warped_label.unique())\n",
    "        print(\"Registred file: \", gz_file)\n",
    "        print(\"Fixed file: \", fixed_file)\n",
    "        print(\"Moving file: \", moving_file)\n",
    "        print(f\"fixed: {fixed_num}, moving: {moving_num}\")\n",
    "        print(f\"fixed: {fixed_id}, moving: {moving_id}\")\n",
    "        print(dice)\n",
    "        print()\n",
    "    \n",
    "# torch.save(data_dict, THIS_SCRIPT_DIR+\"/data/crossmoda_convex.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9a892a0-53a9-42e5-a15d-c02547aad290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique moving_ids 30 [('001r', 1, 'crossmoda_1_Label_r.nii.gz'), ('003r', 3, 'crossmoda_3_Label_r.nii.gz'), ('004r', 4, 'crossmoda_4_Label_r.nii.gz'), ('011r', 11, 'crossmoda_11_Label_r.nii.gz'), ('012r', 12, 'crossmoda_12_Label_r.nii.gz'), ('016r', 16, 'crossmoda_16_Label_r.nii.gz'), ('017r', 17, 'crossmoda_17_Label_r.nii.gz'), ('019r', 19, 'crossmoda_19_Label_r.nii.gz'), ('023r', 23, 'crossmoda_23_Label_r.nii.gz'), ('024r', 24, 'crossmoda_24_Label_r.nii.gz'), ('025r', 25, 'crossmoda_25_Label_r.nii.gz'), ('027r', 27, 'crossmoda_27_Label_r.nii.gz'), ('029r', 29, 'crossmoda_29_Label_r.nii.gz'), ('030r', 30, 'crossmoda_30_Label_r.nii.gz'), ('032r', 32, 'crossmoda_32_Label_r.nii.gz'), ('033r', 33, 'crossmoda_33_Label_r.nii.gz'), ('035r', 35, 'crossmoda_35_Label_r.nii.gz'), ('036r', 36, 'crossmoda_36_Label_r.nii.gz'), ('037r', 37, 'crossmoda_37_Label_r.nii.gz'), ('039r', 39, 'crossmoda_39_Label_r.nii.gz'), ('040r', 40, 'crossmoda_40_Label_r.nii.gz'), ('042r', 42, 'crossmoda_42_Label_r.nii.gz'), ('044r', 44, 'crossmoda_44_Label_r.nii.gz'), ('045r', 45, 'crossmoda_45_Label_r.nii.gz'), ('046r', 46, 'crossmoda_46_Label_r.nii.gz'), ('047r', 47, 'crossmoda_47_Label_r.nii.gz'), ('048r', 48, 'crossmoda_48_Label_r.nii.gz'), ('050r', 50, 'crossmoda_50_Label_r.nii.gz'), ('100r', 100, 'crossmoda_100_Label_r.nii.gz'), ('101r', 101, 'crossmoda_101_Label_r.nii.gz')]\n",
      "unique fixed_ids 30 [('108r', 108, 'crossmoda_108_Label_r.nii.gz'), ('112r', 112, 'crossmoda_112_Label_r.nii.gz'), ('118r', 118, 'crossmoda_118_Label_r.nii.gz'), ('120r', 120, 'crossmoda_120_Label_r.nii.gz'), ('123r', 123, 'crossmoda_123_Label_r.nii.gz'), ('127r', 127, 'crossmoda_127_Label_r.nii.gz'), ('134r', 134, 'crossmoda_134_Label_r.nii.gz'), ('135r', 135, 'crossmoda_135_Label_r.nii.gz'), ('142r', 142, 'crossmoda_142_Label_r.nii.gz'), ('144r', 144, 'crossmoda_144_Label_r.nii.gz'), ('148r', 148, 'crossmoda_148_Label_r.nii.gz'), ('154r', 154, 'crossmoda_154_Label_r.nii.gz'), ('160r', 160, 'crossmoda_160_Label_r.nii.gz'), ('165r', 165, 'crossmoda_165_Label_r.nii.gz'), ('166r', 166, 'crossmoda_166_Label_r.nii.gz'), ('167r', 167, 'crossmoda_167_Label_r.nii.gz'), ('168r', 168, 'crossmoda_168_Label_r.nii.gz'), ('171r', 171, 'crossmoda_171_Label_r.nii.gz'), ('173r', 173, 'crossmoda_173_Label_r.nii.gz'), ('174r', 174, 'crossmoda_174_Label_r.nii.gz'), ('179r', 179, 'crossmoda_179_Label_r.nii.gz'), ('180r', 180, 'crossmoda_180_Label_r.nii.gz'), ('181r', 181, 'crossmoda_181_Label_r.nii.gz'), ('185r', 185, 'crossmoda_185_Label_r.nii.gz'), ('195r', 195, 'crossmoda_195_Label_r.nii.gz'), ('198r', 198, 'crossmoda_198_Label_r.nii.gz'), ('204r', 204, 'crossmoda_204_Label_r.nii.gz'), ('205r', 205, 'crossmoda_205_Label_r.nii.gz'), ('209r', 209, 'crossmoda_209_Label_r.nii.gz'), ('210r', 210, 'crossmoda_210_Label_r.nii.gz')]\n",
      "Quantile tumour:  [0.000 0.170 0.366 0.567 0.717 0.922]\n",
      "Quantile cochlea:  [0.020 0.387 0.457 0.522 0.586 0.755]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"swapdir\", swap_dir)\n",
    "print(\"unique moving_ids\", len(moving_ids), sorted(moving_ids))\n",
    "print(\"unique fixed_ids\", len(fixed_ids), sorted(fixed_ids))\n",
    "# ld_data_dict = torch.load(THIS_SCRIPT_DIR+\"/data/crossmoda_convex.pth\")\n",
    "ld_data_dict = data_dict\n",
    "dices = []\n",
    "for fid, fx in ld_data_dict.items():\n",
    "    for mid, mov in fx.items():\n",
    "        dices.append(mov['dice'])\n",
    "\n",
    "tens_dices = torch.cat(dices)\n",
    "\n",
    "plt.plot(sorted(tens_dices[:,0]), label='bg')\n",
    "plt.plot(sorted(tens_dices[:,1]), label='tumour')\n",
    "plt.plot(sorted(tens_dices[:,2]), label='cochlea')\n",
    "plt.title(\"Sorted dices deeds\")\n",
    "plt.legend()\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "print(\"Quantile tumour: \", np.quantile(tens_dices[:,1], [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]))\n",
    "print(\"Quantile cochlea: \", np.quantile(tens_dices[:,2], [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "285b9633-121d-4a82-8462-b3746824df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6564920544624329, 0.6615490913391113, 0.66754150390625, 0.6853525638580322, 0.6961158514022827, 0.7095019817352295, 0.7232212424278259, 0.7998449206352234, 0.8001408576965332, 0.8199121356010437]\n",
      "torch.Size([10, 128, 128, 128])\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "gt = []\n",
    "wpl = []\n",
    "tumour_dices = []\n",
    "comb_keys = []\n",
    "\n",
    "for key in fixed_keys:\n",
    "    for mov_key in list(warped_label_dict[key].keys()):\n",
    "        warped_label = warped_label_dict[key][mov_key]\n",
    "        tumour_dice = ld_data_dict[key][mov_key]['dice'][0][1].item()\n",
    "        orig_label = orig_label_dict[key]\n",
    "        gt.append(orig_label)\n",
    "        wpl.append(warped_label)\n",
    "        tumour_dices.append(tumour_dice)\n",
    "        comb_keys.append(f\"f{key}_m{mov_key}\")\n",
    "\n",
    "wpl = torch.stack(wpl).to_dense()\n",
    "gt = torch.stack(gt)\n",
    "srt = torch.tensor(np.argsort(tumour_dices)).long()[-10:]\n",
    "tumour_dices = [tumour_dices[idx] for idx in srt]\n",
    "wpl = wpl[srt]\n",
    "gt = gt[srt]\n",
    "\n",
    "comb_keys = [comb_keys[idx] for idx in srt]\n",
    "print(tumour_dices)\n",
    "print(wpl.shape)\n",
    "overlay_text_list = [f\"{dc:.2f}_{key}\" for key, dc in zip(comb_keys, tumour_dices)]\n",
    "\n",
    "frame_elements = [idx%2==0 for idx in range(len(overlay_text_list))]\n",
    "\n",
    "visualize_seg(in_type=\"batch_3D\", reduce_dim=\"W\",\n",
    "    img=gt.unsqueeze(1), # Expert label in BW\n",
    "    seg=wpl.long().cpu(), # Prediction in blue\n",
    "    ground_truth=gt.long(), # Modified label in red\n",
    "    # crop_to_non_zero_seg=True,\n",
    "    crop_to_non_zero_gt=True,\n",
    "    alpha_seg = .5,\n",
    "    alpha_gt = .5,\n",
    "    n_per_row=35,\n",
    "    overlay_text=overlay_text_list,\n",
    "    annotate_color=(0,255,255),\n",
    "    frame_elements=frame_elements,\n",
    "    file_path=\"out.png\",\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0adbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    # Print bare 2D data\n",
    "    # print(\"Displaying 2D bare sample\")\n",
    "    # for img, label in zip(training_dataset.img_data_2d.values(),\n",
    "    #                       training_dataset.label_data_2d.values()):\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=img.unsqueeze(0),\n",
    "    #                 ground_truth=label,\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "    # Print transformed 2D data\n",
    "    training_dataset.train(use_modified=False, augment=True)\n",
    "    print(training_dataset.disturbed_idxs)\n",
    "\n",
    "    print(\"Displaying 2D training sample\")\n",
    "    for dist_stren in [0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 5.0]:\n",
    "        print(dist_stren)\n",
    "        training_dataset.disturb_idxs(list(range(0,20,2)),\n",
    "            disturbance_mode=LabelDisturbanceMode.AFFINE,\n",
    "            disturbance_strength=dist_stren\n",
    "        )\n",
    "        img_stack = []\n",
    "        label_stack = []\n",
    "        mod_label_stack = []\n",
    "\n",
    "        for sample in (training_dataset[idx] for idx in range(20)):\n",
    "            img_stack.append(sample['image'])\n",
    "            label_stack.append(sample['label'])\n",
    "            mod_label_stack.append(sample['modified_label'])\n",
    "\n",
    "        # Change label num == hue shift for display\n",
    "        img_stack = torch.stack(img_stack).unsqueeze(1)\n",
    "        label_stack = torch.stack(label_stack)\n",
    "        mod_label_stack = torch.stack(mod_label_stack)\n",
    "\n",
    "        mod_label_stack*=4\n",
    "\n",
    "        visualize_seg(in_type=\"batch_2D\",\n",
    "            img=img_stack,\n",
    "            # ground_truth=label_stack,\n",
    "            seg=(mod_label_stack-label_stack).abs(),\n",
    "            # crop_to_non_zero_gt=True,\n",
    "            crop_to_non_zero_seg=True,\n",
    "            alpha_seg = .6,\n",
    "            file_path=f'out{dist_stren}.png'\n",
    "        )\n",
    "\n",
    "    # Print transformed 3D data\n",
    "    # training_dataset.train()\n",
    "    # print(\"Displaying 3D training sample\")\n",
    "    # leng = 1# training_dataset.__len__(use_2d_override=False)\n",
    "    # for sample in (training_dataset.get_3d_item(idx) for idx in range(leng)):\n",
    "    #     # training_dataset.set_dilate_kernel_size(1)\n",
    "    #     visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "    #                 img=sample['image'].unsqueeze(0),\n",
    "    #                 ground_truth=sample['label'],\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "#         # training_dataset.set_dilate_kernel_size(7)\n",
    "#         display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "#                     img=sample['image'].unsqueeze(0),\n",
    "#                     ground_truth=sample['modified_label'],\n",
    "#                     crop_to_non_zero_gt=True,\n",
    "#                     alpha_gt = .3)\n",
    "\n",
    "    for sidx in [0,]:\n",
    "        print(f\"Sample {sidx}:\")\n",
    "\n",
    "        training_dataset.eval()\n",
    "        sample_eval = training_dataset.get_3d_item(sidx)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "\n",
    "        training_dataset.train()\n",
    "        print(\"Train sample with ground-truth overlay\")\n",
    "        sample_train = training_dataset.get_3d_item(sidx)\n",
    "        print(sample_train['label'].unique())\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_train['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_train['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt=.3)\n",
    "\n",
    "        print(\"Eval/train diff with diff overlay\")\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=(sample_eval['image'] - sample_train['image']).unsqueeze(0),\n",
    "                    ground_truth=(sample_eval['label'] - sample_train['label']).clamp(min=0),\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "    train_plotset = (training_dataset.get_3d_item(idx) for idx in (55, 81, 63))\n",
    "    for sample in train_plotset:\n",
    "        print(f\"Sample {sample['dataset_idx']}:\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .6)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use module.named_modules() to retrieve valid keychains for layers.\n",
    "       e.g.\n",
    "       first_keychain = list(module.keys())[0]\n",
    "       new_first_replacee = torch.nn.Conv1d(1,2,3)\n",
    "       set_module(first_keychain, torch.nn.Conv1d(1,2,3))\n",
    "    \"\"\"\n",
    "\n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(_path, **statefuls):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "    _path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for name, stful in statefuls.items():\n",
    "        if stful != None:\n",
    "            torch.save(stful.state_dict(), _path.joinpath(name+'.pth'))\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config, dataset_len, num_classes, THIS_SCRIPT_DIR, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        # Use vanilla torch model\n",
    "        lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(\n",
    "            pretrained=False, progress=True, num_classes=num_classes\n",
    "        )\n",
    "        set_module(lraspp, 'backbone.0.0',\n",
    "            torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2),\n",
    "                            padding=(1, 1), bias=False)\n",
    "        )\n",
    "    else:\n",
    "        # Use custom 3d model\n",
    "        lraspp = MobileNet_LRASPP_3D(\n",
    "            in_num=in_channels, num_classes=num_classes,\n",
    "            use_checkpointing=True\n",
    "        )\n",
    "\n",
    "    # lraspp.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "    lraspp.to(device)\n",
    "    print(f\"Param count lraspp: {sum(p.numel() for p in lraspp.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(lraspp.parameters(), lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # Add data paramters embedding and optimizer\n",
    "    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len, 1, sparse=True)\n",
    "        # p_offset = torch.zeros(1, layout=torch.strided, requires_grad=True)\n",
    "        # p_offset.grad = torch.sparse_coo_tensor([[0]], 1., size=(1,))\n",
    "\n",
    "        # embedding.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "        # embedding.sigmoid_offset.register_hook(lambda grad: torch.sparse_coo_tensor([[0]], grad, size=(1,)))\n",
    "        embedding = embedding.to(device)\n",
    "\n",
    "    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len*config.grid_size_y*config.grid_size_x, 1, sparse=True).to(device)\n",
    "    else:\n",
    "        embedding = None\n",
    "\n",
    "\n",
    "    if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "        optimizer_dp = torch.optim.SparseAdam(\n",
    "            embedding.parameters(), lr=config.lr_inst_param,\n",
    "            betas=(0.9, 0.999), eps=1e-08)\n",
    "        torch.nn.init.normal_(embedding.weight.data, mean=config.init_inst_param, std=0.00)\n",
    "\n",
    "        if _path and _path.is_dir():\n",
    "            print(f\"Loading embedding and dp_optimizer from {_path}\")\n",
    "            optimizer_dp.load_state_dict(torch.load(_path.joinpath('optimizer_dp.pth'), map_location=device))\n",
    "            embedding.load_state_dict(torch.load(_path.joinpath('embedding.pth'), map_location=device))\n",
    "\n",
    "        print(f\"Param count embedding: {sum(p.numel() for p in embedding.parameters())}\")\n",
    "\n",
    "    else:\n",
    "        optimizer_dp = None\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading lr-aspp model, optimizers and grad scalers from {_path}\")\n",
    "        lraspp.load_state_dict(torch.load(_path.joinpath('lraspp.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "\n",
    "    return (lraspp, optimizer, optimizer_dp, embedding, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f1722",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "\n",
    "\n",
    "\n",
    "def save_parameter_figure(_path, title, text, parameters, reweighted_parameters, dices):\n",
    "    # Show weights and weights with compensation\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12, 4), dpi=80)\n",
    "    sc1 = axs[0].scatter(\n",
    "        range(len(parameters)),\n",
    "        parameters.cpu().detach(), c=dices,s=1, cmap='plasma', vmin=0., vmax=1.)\n",
    "    sc2 = axs[1].scatter(\n",
    "        range(len(reweighted_parameters)),\n",
    "        reweighted_parameters.cpu().detach(), s=1,c=dices, cmap='plasma', vmin=0., vmax=1.)\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.text(0, 0, text)\n",
    "    axs[0].set_title('Bare parameters')\n",
    "    axs[1].set_title('Reweighted parameters')\n",
    "    axs[0].set_ylim(-10, 10)\n",
    "    axs[1].set_ylim(-3, 1)\n",
    "    plt.colorbar(sc2)\n",
    "    plt.savefig(_path)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "\n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "\n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "\n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "\n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio\n",
    "\n",
    "def log_data_parameter_stats(log_path, epx, data_parameters):\n",
    "    \"\"\"Log stats for data parameters on wandb.\"\"\"\n",
    "    wandb.log({f'{log_path}/highest': torch.max(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/lowest': torch.min(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/mean': torch.mean(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/std': torch.std(data_parameters).item()}, step=epx)\n",
    "\n",
    "\n",
    "\n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_class_dices(log_prefix, log_postfix, class_dices, log_idx):\n",
    "    if not class_dices:\n",
    "        return\n",
    "\n",
    "    for cls_name in class_dices[0].keys():\n",
    "        log_path = f\"{log_prefix}{cls_name}{log_postfix}\"\n",
    "\n",
    "        cls_dices = list(map(lambda dct: dct[cls_name], class_dices))\n",
    "        mean_per_class =np.nanmean(cls_dices)\n",
    "        print(log_path, f\"{mean_per_class*100:.2f}%\")\n",
    "        wandb.log({log_path: mean_per_class}, step=log_idx)\n",
    "\n",
    "def CELoss(logits, targets, bin_weight=None):\n",
    "    # -logits[0,targets[0,0,0],0,0]+torch.log(logits[0,:,0,0].exp().sum())\n",
    "    targets = torch.nn.functional.one_hot(targets).permute(0,3,1,2)\n",
    "    loss = -targets*F.log_softmax(logits, 1)\n",
    "\n",
    "    if bin_weight is not None:\n",
    "        brdcast_shape = torch.Size((loss.shape[0], loss.shape[1], *((loss.dim()-2)*[1])))\n",
    "        inv_weight = (bin_weight+np.exp(1)).log()+np.exp(1)\n",
    "        inv_weight = inv_weight/inv_weight.mean()\n",
    "        loss = inv_weight.view(brdcast_shape)*loss\n",
    "\n",
    "    return loss.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embedding_idxs(idxs, grid_size_y, grid_size_x):\n",
    "    with torch.no_grad():\n",
    "        t_sz = grid_size_y * grid_size_x\n",
    "        return ((idxs*t_sz).long().repeat(t_sz).view(t_sz, idxs.numel())+torch.tensor(range(t_sz)).to(idxs).view(t_sz,1)).permute(1,0).reshape(-1)\n",
    "\n",
    "def inference_wrap(lraspp, img, use_2d, use_mind):\n",
    "    with torch.inference_mode():\n",
    "        b_img = img.unsqueeze(0).unsqueeze(0).float()\n",
    "        if use_2d and use_mind:\n",
    "            # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "            b_img = mindssc(b_img.unsqueeze(0)).squeeze(2)\n",
    "        elif not use_2d and use_mind:\n",
    "            # MIND 3D in Bx1xDxHxW out BxMINDxDxHxW\n",
    "            b_img = mindssc(b_img)\n",
    "        elif use_2d or not use_2d:\n",
    "            # 2D Bx1xHxW\n",
    "            # 3D out Bx1xDxHxW\n",
    "            pass\n",
    "\n",
    "        b_out = lraspp(b_img)['out']\n",
    "        b_out = b_out.argmax(1)\n",
    "        return b_out\n",
    "\n",
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "\n",
    "    fold_iter = enumerate(kf.split(training_dataset))\n",
    "\n",
    "    if config.get('fold_override', None):\n",
    "        selected_fold = config.get('fold_override', 0)\n",
    "        fold_iter = list(fold_iter)[selected_fold:selected_fold+1]\n",
    "    elif config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "\n",
    "    if config.wandb_mode != 'disabled':\n",
    "        # Log dataset info\n",
    "        training_dataset.eval()\n",
    "        dataset_info = [[smp['dataset_idx'], smp['id'], smp['image_path'], smp['label_path']] \\\n",
    "                        for smp in training_dataset]\n",
    "        wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', 'id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    "\n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        # Training happens in 2D, validation happens in 3D:\n",
    "        # Read 2D dataset idxs which are used for training,\n",
    "        # get their 3D super-ids and substract these from all 3D ids to get val_3d_idxs\n",
    "\n",
    "        if config.use_2d_normal_to is not None:\n",
    "            n_dims = (-2,-1)\n",
    "            trained_3d_dataset_ids = training_dataset.get_3d_from_2d_identifiers(train_idxs, 'id')\n",
    "            # trained_3d_trained_ids = training_dataset.switch_3d_identifiers(trained_3d_dataset_idxs)\n",
    "            all_3d_ids = training_dataset.get_3d_ids()\n",
    "            val_3d_ids = set(all_3d_ids) - set(trained_3d_dataset_ids)\n",
    "            val_3d_idxs = list({\n",
    "                training_dataset.extract_short_3d_id(_id):idx \\\n",
    "                    for idx, _id in enumerate(all_3d_ids) if _id in val_3d_ids}.values())\n",
    "        else:\n",
    "            n_dims = (-3,-2,-1)\n",
    "            val_3d_idxs = val_idxs\n",
    "        print(\"Will run validation with these 3D samples:\", val_3d_idxs)\n",
    "\n",
    "        _, _, all_modified_segs = training_dataset.get_data()\n",
    "\n",
    "        non_empty_train_idxs = train_idxs[(all_modified_segs[train_idxs].sum(dim=n_dims) > 0)]\n",
    "\n",
    "        ### Disturb dataset (only non-emtpy idxs)###\n",
    "        proposed_disturbed_idxs = np.random.choice(non_empty_train_idxs, size=int(len(non_empty_train_idxs)*config.disturbed_percentage), replace=False)\n",
    "        proposed_disturbed_idxs = torch.tensor(proposed_disturbed_idxs)\n",
    "        training_dataset.disturb_idxs(proposed_disturbed_idxs,\n",
    "            disturbance_mode=config.disturbance_mode,\n",
    "            disturbance_strength=config.disturbance_strength\n",
    "        )\n",
    "\n",
    "        disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "        disturbed_bool_vect[training_dataset.disturbed_idxs] = 1.\n",
    "\n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, training_dataset.disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(training_dataset.disturbed_idxs))\n",
    "\n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in training_dataset.disturbed_idxs])},\n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "\n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels = 12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "\n",
    "        class_weights = 1/(torch.bincount(all_modified_segs.reshape(-1).long())).float().pow(.35)\n",
    "        class_weights /= class_weights.mean()\n",
    "\n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "            sampler=train_subsampler, pin_memory=True, drop_last=False,\n",
    "            # collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "        )\n",
    "\n",
    "        training_dataset.set_augment_at_collate(False)\n",
    "\n",
    "#         val_dataloader = DataLoader(training_dataset, batch_size=config.val_batch_size,\n",
    "#                                     sampler=val_subsampler, pin_memory=True, drop_last=False)\n",
    "\n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        epx_start = config.get('checkpoint_epx', 0)\n",
    "\n",
    "        if config.checkpoint_name:\n",
    "            # Load from checkpoint\n",
    "            _path = f\"{config.mdl_save_prefix}/{config.checkpoint_name}_fold{fold_idx}_epx{epx_start}\"\n",
    "        else:\n",
    "            _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx_start}\"\n",
    "\n",
    "        (lraspp, optimizer, optimizer_dp, embedding, scaler) = get_model(config, len(training_dataset), len(training_dataset.label_tags),\n",
    "            THIS_SCRIPT_DIR=THIS_SCRIPT_DIR, _path=_path, device='cuda')\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=500, T_mult=2)\n",
    "\n",
    "        if optimizer_dp:\n",
    "            scheduler_dp = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer_dp, T_0=500, T_mult=2)\n",
    "        else:\n",
    "            scheduler_dp = None\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare corr coefficient scoring\n",
    "        training_dataset.eval(use_modified=True)\n",
    "        wise_labels, mod_labels = list(zip(*[(sample['label'], sample['modified_label']) \\\n",
    "            for sample in training_dataset]))\n",
    "        wise_labels, mod_labels = torch.stack(wise_labels), torch.stack(mod_labels)\n",
    "\n",
    "        dice_func = dice2d if config.use_2d_normal_to is not None else dice3d\n",
    "\n",
    "        wise_dice = dice_func(\n",
    "            torch.nn.functional.one_hot(wise_labels, len(training_dataset.label_tags)),\n",
    "            torch.nn.functional.one_hot(mod_labels, len(training_dataset.label_tags)),\n",
    "            one_hot_torch_style=True, nan_for_unlabeled_target=False\n",
    "        )\n",
    "\n",
    "        gt_num = (mod_labels > 0).sum(dim=n_dims)\n",
    "        t_metric = (gt_num+np.exp(1)).log()+np.exp(1)\n",
    "\n",
    "        if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "            union_wise_mod_label = torch.logical_or(wise_labels, mod_labels)\n",
    "            union_wise_mod_label = union_wise_mod_label.cuda()\n",
    "\n",
    "        class_weights = class_weights.cuda()\n",
    "        gt_num = gt_num.cuda()\n",
    "        t_metric = t_metric.cuda()\n",
    "\n",
    "        for epx in range(epx_start, config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "\n",
    "            lraspp.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            training_dataset.train(use_modified=(epx >= config.start_disturbing_after_ep))\n",
    "            wandb.log({\"use_modified\": float(training_dataset.use_modified)}, step=global_idx)\n",
    "\n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            class_dices = []\n",
    "\n",
    "            # Load data\n",
    "            for batch_idx, batch in enumerate(train_dataloader):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if optimizer_dp:\n",
    "                    optimizer_dp.zero_grad()\n",
    "\n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "                b_spat_aug_grid = batch['spat_augment_grid']\n",
    "\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "                b_img = b_img.float()\n",
    "\n",
    "                b_img = b_img.cuda()\n",
    "                b_seg_modified = b_seg_modified.cuda()\n",
    "                b_idxs_dataset = b_idxs_dataset.cuda()\n",
    "                b_seg = b_seg.cuda()\n",
    "                b_spat_aug_grid = b_spat_aug_grid.cuda()\n",
    "\n",
    "                if training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "                elif not training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 3D\n",
    "                    b_img = mindssc(b_img.unsqueeze(1))\n",
    "                else:\n",
    "                    b_img = b_img.unsqueeze(1)\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=True):\n",
    "                    assert b_img.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input image for model must be {len(n_dims)+2}D: BxCxSPATIAL but is {b_img.shape}\"\n",
    "\n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    ### Calculate loss ###\n",
    "                    assert logits.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input shape for loss must be BxNUM_CLASSESxSPATIAL but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == len(n_dims)+1, \\\n",
    "                        f\"Target shape for loss must be BxSPATIAL but is {b_seg_modified.shape}\"\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                        # batch_bins = torch.zeros([len(b_idxs_dataset), len(training_dataset.label_tags)]).to(logits.device)\n",
    "                        # bin_list = [slc.view(-1).bincount() for slc in b_seg_modified]\n",
    "                        # for b_idx, _bins in enumerate(bin_list):\n",
    "                        #     batch_bins[b_idx][:len(_bins)] = _bins\n",
    "                        # loss = CELoss(logits, b_seg_modified, bin_weight=batch_bins)\n",
    "\n",
    "                        loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified)\n",
    "                        loss = loss.mean(n_dims)\n",
    "\n",
    "                        bare_weight = embedding(b_idxs_dataset).squeeze()\n",
    "\n",
    "                        weight = torch.sigmoid(bare_weight)\n",
    "                        weight = weight/weight.mean()\n",
    "\n",
    "                        # This improves scores significantly: Reweight with log(gt_numel)\n",
    "                        weight = weight/t_metric[b_idxs_dataset]\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = logits.argmax(1)\n",
    "\n",
    "                        if config.use_risk_regularization:\n",
    "                            p_pred_num = (logits_for_score > 0).sum(dim=n_dims).detach()\n",
    "                            risk_regularization = -weight*p_pred_num/(logits_for_score.shape[-2]*logits_for_score.shape[-1])\n",
    "                            loss = (loss*weight).sum() + risk_regularization.sum()\n",
    "                        else:\n",
    "                            loss = (loss*weight).sum()\n",
    "\n",
    "                    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                        loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified)\n",
    "                        m_dp_idxs = map_embedding_idxs(b_idxs_dataset, config.grid_size_y, config.grid_size_x)\n",
    "                        weight = embedding(m_dp_idxs)\n",
    "                        weight = weight.reshape(-1, config.grid_size_y, config.grid_size_x)\n",
    "                        weight = weight.unsqueeze(1)\n",
    "                        weight = torch.nn.functional.interpolate(\n",
    "                            weight,\n",
    "                            size=(b_seg_modified.shape[-2:]),\n",
    "                            mode='bilinear',\n",
    "                            align_corners=True\n",
    "                        )\n",
    "                        weight = weight/weight.mean()\n",
    "                        weight = F.grid_sample(weight, b_spat_aug_grid,\n",
    "                            padding_mode='border', align_corners=False)\n",
    "                        loss = (loss.unsqueeze(1)*weight).sum()\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = (logits*weight).argmax(1)\n",
    "\n",
    "                    else:\n",
    "                        loss = nn.CrossEntropyLoss(class_weights)(logits, b_seg_modified)\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = logits.argmax(1)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                    scaler.step(optimizer_dp)\n",
    "\n",
    "                scaler.update()\n",
    "\n",
    "                epx_losses.append(loss.item())\n",
    "\n",
    "                # Calculate dice score\n",
    "                b_dice = dice_func(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(b_seg, len(training_dataset.label_tags)), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_over_all(\n",
    "                    b_dice, exclude_bg=True))\n",
    "                class_dices.append(get_batch_dice_per_class(\n",
    "                    b_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                ###  Scheduler management ###\n",
    "                if config.use_cosine_annealing:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if config.save_dp_figures and batch_idx % 10 == 0:\n",
    "                    # Output data parameter figure\n",
    "                    train_params = embedding.weight[train_idxs].squeeze()\n",
    "                    # order = np.argsort(train_params.cpu().detach()) # Order by DP value\n",
    "                    order = torch.arange(len(train_params))\n",
    "                    wise_corr_coeff = np.corrcoef(train_params.cpu().detach(), wise_dice[train_idxs][:,1].cpu().detach())[0,1]\n",
    "                    dp_figure_path = Path(f\"data/output_figures/{wandb.run.name}_fold{fold_idx}/dp_figure_epx{epx:03d}_batch{batch_idx:03d}.png\")\n",
    "                    dp_figure_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    save_parameter_figure(dp_figure_path, wandb.run.name, f\"corr. coeff. DP vs. dice(expert label, train gt): {wise_corr_coeff:4f}\",\n",
    "                        train_params[order], train_params[order]/t_metric[train_idxs][order], dices=wise_dice[train_idxs][:,1][order])\n",
    "\n",
    "                if config.debug:\n",
    "                    break\n",
    "\n",
    "            ### Logging ###\n",
    "            print(f\"### Log epoch {epx} @ {time.time()-t0:.2f}s\")\n",
    "            print(\"### Training\")\n",
    "            ### Log wandb data ###\n",
    "            # Log the epoch idx per fold - so we can recover the diagram by setting\n",
    "            # ref_epoch_idx as x-axis in wandb interface\n",
    "            wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "\n",
    "            mean_loss = torch.tensor(epx_losses).mean()\n",
    "            wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "\n",
    "            mean_dice = np.nanmean(dices)\n",
    "            print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "            wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "\n",
    "            log_class_dices(\"scores/dice_mean_\", f\"_fold{fold_idx}\", class_dices, global_idx)\n",
    "\n",
    "            # Log data parameters of disturbed samples\n",
    "            if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                # Calculate dice score corr coeff (unknown to network)\n",
    "                train_params = embedding.weight[train_idxs].squeeze()\n",
    "                order = np.argsort(train_params.cpu().detach())\n",
    "                wise_corr_coeff = np.corrcoef(train_params[order].cpu().detach(), wise_dice[train_idxs][:,1][order].cpu().detach())[0,1]\n",
    "\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/wise_corr_coeff_fold{fold_idx}': wise_corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                print(f'data_parameters/wise_corr_coeff_fold{fold_idx}', f\"{wise_corr_coeff:.2f}\")\n",
    "\n",
    "                # Log stats of data parameters and figure\n",
    "                log_data_parameter_stats(f'data_parameters/iter_stats_fold{fold_idx}', global_idx, embedding.weight.data)\n",
    "\n",
    "                # Map gridded instance parameters\n",
    "                if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                    m_tr_idxs = map_embedding_idxs(train_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                    ).cuda()\n",
    "                    masks = union_wise_mod_label[train_idxs].float()\n",
    "                    masked_weights = torch.nn.functional.interpolate(\n",
    "                        embedding(m_tr_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                        size=(masks.shape[-2:]),\n",
    "                        mode='bilinear',\n",
    "                        align_corners=True\n",
    "                    ).squeeze(1) * masks\n",
    "                    masked_weights[masked_weights==0.] = float('nan')\n",
    "\n",
    "            if (epx % config.save_every == 0 and epx != 0) \\\n",
    "                or (epx+1 == config.epochs):\n",
    "                _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx}\"\n",
    "                save_model(\n",
    "                    _path,\n",
    "                    lraspp=lraspp,\n",
    "                    optimizer=optimizer, optimizer_dp=optimizer_dp,\n",
    "                    scheduler=scheduler, scheduler_dp=scheduler_dp,\n",
    "                    embedding=embedding,\n",
    "                    scaler=scaler)\n",
    "                (lraspp, optimizer, optimizer_dp, embedding, scaler) = \\\n",
    "                    get_model(\n",
    "                        config, len(training_dataset),\n",
    "                        len(training_dataset.label_tags),\n",
    "                        THIS_SCRIPT_DIR=THIS_SCRIPT_DIR,\n",
    "                        _path=_path, device='cuda')\n",
    "\n",
    "            print()\n",
    "            print(\"### Validation\")\n",
    "            lraspp.eval()\n",
    "            training_dataset.eval()\n",
    "\n",
    "            val_dices = []\n",
    "            val_class_dices = []\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    for val_idx in val_3d_idxs:\n",
    "                        val_sample = training_dataset.get_3d_item(val_idx)\n",
    "                        stack_dim = training_dataset.use_2d_normal_to\n",
    "                        # Create batch out of single val sample\n",
    "                        b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                        b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "\n",
    "                        B = b_val_img.shape[0]\n",
    "\n",
    "                        b_val_img = b_val_img.unsqueeze(1).float().cuda()\n",
    "                        b_val_seg = b_val_seg.cuda()\n",
    "\n",
    "                        if training_dataset.use_2d():\n",
    "                            b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=training_dataset.use_2d_normal_to)\n",
    "\n",
    "                            if config.use_mind:\n",
    "                                # MIND 2D model, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                                b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                            output_val = lraspp(b_val_img_2d)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "                            # Prepare logits for scoring\n",
    "                            # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                            val_logits_for_score = make_3d_from_2d_stack(\n",
    "                                val_logits_for_score.unsqueeze(1), stack_dim, B\n",
    "                            ).squeeze(1)\n",
    "\n",
    "                        else:\n",
    "                            if config.use_mind:\n",
    "                                # MIND 3D model shape BxMINDxDxHxW\n",
    "                                b_val_img = mindssc(b_val_img)\n",
    "                            else:\n",
    "                                # 3D model shape Bx1xDxHxW\n",
    "                                pass\n",
    "\n",
    "                            output_val = lraspp(b_val_img)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "\n",
    "                        b_val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(val_logits_for_score, len(training_dataset.label_tags)),\n",
    "                            torch.nn.functional.one_hot(b_val_seg, len(training_dataset.label_tags)),\n",
    "                            one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        # Get mean score over batch\n",
    "                        val_dices.append(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=True))\n",
    "\n",
    "                        val_class_dices.append(get_batch_dice_per_class(\n",
    "                            b_val_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                        if config.do_plot:\n",
    "                            print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                            print(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=False))\n",
    "                            # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                            display_seg(in_type=\"single_3D\",\n",
    "                                reduce_dim=\"W\",\n",
    "                                img=val_sample['image'].unsqueeze(0).cpu(),\n",
    "                                seg=val_logits_for_score_3d.squeeze(0).cpu(), # CHECK TODO\n",
    "                                ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                crop_to_non_zero_seg=True,\n",
    "                                crop_to_non_zero_gt=True,\n",
    "                                alpha_seg=.3,\n",
    "                                alpha_gt=.0\n",
    "                            )\n",
    "\n",
    "                    mean_val_dice = np.nanmean(val_dices)\n",
    "                    print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                    wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                    log_class_dices(\"scores/val_dice_mean_\", f\"_fold{fold_idx}\", val_class_dices, global_idx)\n",
    "\n",
    "            print()\n",
    "            # End of training loop\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "            # Write sample data\n",
    "\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            all_idxs = torch.tensor(range(len(training_dataset))).cuda()\n",
    "            train_label_snapshot_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/train_label_snapshot.pth\")\n",
    "            seg_viz_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weighted_samples.png\")\n",
    "\n",
    "            train_label_snapshot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if str(config.data_param_mode) == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                dp_weights = embedding(all_idxs)\n",
    "                save_data = []\n",
    "                data_generator = zip(\n",
    "                    dp_weights[train_idxs], \\\n",
    "                    disturbed_bool_vect[train_idxs],\n",
    "                    torch.utils.data.Subset(training_dataset, train_idxs)\n",
    "                )\n",
    "\n",
    "                for dp_weight, disturb_flg, sample in data_generator:\n",
    "                    data_tuple = ( \\\n",
    "                        dp_weight,\n",
    "                        bool(disturb_flg.item()),\n",
    "                        sample['id'],\n",
    "                        sample['dataset_idx'],\n",
    "                        sample['image'],\n",
    "                        sample['label'],\n",
    "                        sample['modified_label'],\n",
    "                        inference_wrap(lraspp, sample['image'].cuda(), use_2d=training_dataset.use_2d(), use_mind=config.use_mind)\n",
    "                    )\n",
    "                    save_data.append(data_tuple)\n",
    "\n",
    "                save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "                (dp_weight, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _imgs,\n",
    "                 _labels, _modified_labels, _predictions) = zip(*save_data)\n",
    "\n",
    "                dp_weight = torch.stack(dp_weight)\n",
    "                dataset_idxs = torch.stack(dataset_idxs)\n",
    "                _imgs = torch.stack(_imgs)\n",
    "                _labels = torch.stack(_labels)\n",
    "                _modified_labels = torch.stack(_modified_labels)\n",
    "                _predictions = torch.stack(_predictions)\n",
    "\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'data_parameters': dp_weight.cpu(),\n",
    "                        'disturb_flags': disturb_flags,\n",
    "                        'd_ids': d_ids,\n",
    "                        'dataset_idxs': dataset_idxs.cpu(),\n",
    "                        'labels': _labels.cpu().to_sparse(),\n",
    "                        'modified_labels': _modified_labels.cpu().to_sparse(),\n",
    "                        'train_predictions': _predictions.cpu().to_sparse(),\n",
    "                    },\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "            elif str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                # Log histogram of clean and disturbed parameters\n",
    "                if not training_dataset.use_2d():\n",
    "                    raise NotImplementedError(\"Script does not support 3D model and GRIDDED_INSTANCE_PARAMS yet.\")\n",
    "\n",
    "                m_all_idxs = map_embedding_idxs(all_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                ).cuda()\n",
    "                masks = union_wise_mod_label[all_idxs].float()\n",
    "                all_weights = torch.nn.functional.interpolate(\n",
    "                    embedding(m_all_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                    size=(masks.shape[-2:]),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=True\n",
    "                ).squeeze(1)\n",
    "\n",
    "                masked_weights = all_weights * masks\n",
    "                masked_weights[masked_weights==0.] = float('nan')\n",
    "                dp_weights = np.nanmean(masked_weights.detach().cpu(), axis=n_dims)\n",
    "\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weightmap.png\")\n",
    "\n",
    "                save_data = []\n",
    "                data_generator = zip(\n",
    "                    dp_weights[train_idxs],\n",
    "                    all_weights[train_idxs],\n",
    "                    disturbed_bool_vect[train_idxs],\n",
    "                    torch.utils.data.Subset(training_dataset,train_idxs)\n",
    "                )\n",
    "\n",
    "                for dp_weight, weightmap, disturb_flg, sample in data_generator:\n",
    "                    data_tuple = (\n",
    "                        dp_weight,\n",
    "                        weightmap,\n",
    "                        bool(disturb_flg.item()),\n",
    "                        sample['id'],\n",
    "                        sample['dataset_idx'],\n",
    "                        sample['image'],\n",
    "                        sample['label'],\n",
    "                        sample['modified_label']\n",
    "                    )\n",
    "                    save_data.append(data_tuple)\n",
    "\n",
    "                save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "\n",
    "                (dp_weight, dp_weightmap, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _2d_imgs,\n",
    "                 _2d_labels, _2d_modified_labels) = zip(*save_data)\n",
    "\n",
    "                dp_weight = torch.stack(dp_weight)\n",
    "                dp_weightmap = torch.stack(dp_weightmap)\n",
    "                dataset_idxs = torch.stack(dataset_idxs)\n",
    "                _2d_imgs = torch.stack(_2d_imgs)\n",
    "                _2d_labels = torch.stack(_2d_labels)\n",
    "                _2d_modified_labels = torch.stack(_2d_modified_labels)\n",
    "                _2d_predictions = torch.stack(_2d_predictions)\n",
    "\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'data_parameters': dp_weight.cpu(),\n",
    "                        'data_parameter_weightmaps': dp_weightmap.cpu(),\n",
    "                        'disturb_flags': disturb_flags,\n",
    "                        'd_ids': d_ids,\n",
    "                        'dataset_idxs': dataset_idxs.cpu(),\n",
    "                        'labels': _2d_labels.cpu().to_sparse(),\n",
    "                        'modified_labels': _2d_modified_labels.cpu().to_sparse(),\n",
    "                        'train_predictions': _2d_predictions.cpu().to_sparse(),\n",
    "                    },\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "                print(\"Writing weight map image.\")\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}_data_parameter_weightmap.png\")\n",
    "                visualize_seg(in_type=\"batch_2D\",\n",
    "                    img=torch.stack(dp_weightmap).unsqueeze(1),\n",
    "                    seg=torch.stack(_2d_modified_labels),\n",
    "                    alpha_seg = 0.,\n",
    "                    n_per_row=70,\n",
    "                    overlay_text=overlay_text_list,\n",
    "                    annotate_color=(0,255,255),\n",
    "                    frame_elements=disturb_flags,\n",
    "                    file_path=weightmap_out_path,\n",
    "                )\n",
    "\n",
    "            if len(training_dataset.disturbed_idxs) > 0:\n",
    "                # Log histogram\n",
    "                separated_params = list(zip(dp_weights[clean_idxs], dp_weights[training_dataset.disturbed_idxs]))\n",
    "                s_table = wandb.Table(columns=['clean_idxs', 'disturbed_idxs'], data=separated_params)\n",
    "                fields = {\"primary_bins\" : \"clean_idxs\", \"secondary_bins\" : \"disturbed_idxs\", \"title\" : \"Data parameter composite histogram\"}\n",
    "                composite_histogram = wandb.plot_table(vega_spec_name=\"rap1ide/composite_histogram\", data_table=s_table, fields=fields)\n",
    "                wandb.log({f\"data_parameters/separated_params_fold_{fold_idx}\": composite_histogram})\n",
    "\n",
    "            # Write out data of modified and un-modified labels and an overview image\n",
    "            print(\"Writing train sample image.\")\n",
    "            # overlay text example: d_idx=0, dp_i=1.00, dist? False\n",
    "            overlay_text_list = [f\"id:{d_id} dp:{instance_p.item():.2f}\" \\\n",
    "                for d_id, instance_p, disturb_flg in zip(d_ids, dp_weight, disturb_flags)]\n",
    "\n",
    "            if training_dataset.use_2d():\n",
    "                reduce_dim = None\n",
    "                in_type = \"batch_2D\"\n",
    "            else:\n",
    "                reduce_dim = \"W\"\n",
    "                in_type = \"batch_3D\"\n",
    "\n",
    "            use_2d = training_dataset.use_2d()\n",
    "            scf = 1/training_dataset.pre_interpolation_factor\n",
    "\n",
    "            show_img = interpolate_sample(b_label=_labels, scale_factor=scf, use_2d=use_2d)[1].unsqueeze(1)\n",
    "            show_seg = interpolate_sample(b_label=4*_predictions.squeeze(1), scale_factor=scf, use_2d=use_2d)[1]\n",
    "            show_gt = interpolate_sample(b_label=_modified_labels, scale_factor=scf, use_2d=use_2d)[1]\n",
    "\n",
    "            visualize_seg(in_type=in_type, reduce_dim=reduce_dim,\n",
    "                img=show_img, # Expert label in BW\n",
    "                seg=show_seg, # Prediction in blue\n",
    "                ground_truth=show_gt, # Modified label in red\n",
    "                crop_to_non_zero_seg=False,\n",
    "                alpha_seg = .5,\n",
    "                alpha_gt = .5,\n",
    "                n_per_row=70,\n",
    "                overlay_text=overlay_text_list,\n",
    "                annotate_color=(0,255,255),\n",
    "                frame_elements=disturb_flags,\n",
    "                file_path=seg_viz_out_path,\n",
    "            )\n",
    "\n",
    "        # End of fold loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fc80a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Config overrides\n",
    "# config_dict['wandb_mode'] = 'disabled'\n",
    "# config_dict['debug'] = True\n",
    "# Model loading\n",
    "# config_dict['checkpoint_name'] = 'treasured-water-717'\n",
    "# # config_dict['fold_override'] = 0\n",
    "# config_dict['checkpoint_epx'] = 39\n",
    "\n",
    "# Define sweep override dict\n",
    "sweep_config_dict = dict(\n",
    "    method='grid',\n",
    "    metric=dict(goal='maximize', name='scores/val_dice_mean_tumour_fold0'),\n",
    "    parameters=dict(\n",
    "        # disturbance_mode=dict(\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        # reg_state=dict(\n",
    "        #     values=['best','combined']\n",
    "        # ),\n",
    "        disturbance_strength=dict(\n",
    "            values=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "        ),\n",
    "        disturbed_percentage=dict(\n",
    "            values=[0.3, 0.6]\n",
    "        ),\n",
    "        data_param_mode=dict(\n",
    "            values=[\n",
    "                DataParamMode.INSTANCE_PARAMS,\n",
    "                DataParamMode.DISABLED,\n",
    "            ]\n",
    "        ),\n",
    "        # use_risk_regularization=dict(\n",
    "        #     values=[False, True]\n",
    "        # )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549637cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def normal_run():\n",
    "    with wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        ) as run:\n",
    "\n",
    "        run_name = run.name\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init() as run:\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "\n",
    "        run_name = run.name\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "\n",
    "if config_dict['do_sweep']:\n",
    "    # Integrate all config_dict entries into sweep_dict.parameters -> sweep overrides config_dict\n",
    "    cp_config_dict = copy.deepcopy(config_dict)\n",
    "    # cp_config_dict.update(copy.deepcopy(sweep_config_dict['parameters']))\n",
    "    for del_key in sweep_config_dict['parameters'].keys():\n",
    "        if del_key in cp_config_dict:\n",
    "            del cp_config_dict[del_key]\n",
    "    merged_sweep_config_dict = copy.deepcopy(sweep_config_dict)\n",
    "    # merged_sweep_config_dict.update(cp_config_dict)\n",
    "    for key, value in cp_config_dict.items():\n",
    "        merged_sweep_config_dict['parameters'][key] = dict(value=value)\n",
    "    # Convert enum values in parameters to string. They will be identified by their numerical index otherwise\n",
    "    for key, param_dict in merged_sweep_config_dict['parameters'].items():\n",
    "        if 'value' in param_dict and isinstance(param_dict['value'], Enum):\n",
    "            param_dict['value'] = str(param_dict['value'])\n",
    "        if 'values' in param_dict:\n",
    "            param_dict['values'] = [str(elem) if isinstance(elem, Enum) else elem for elem in param_dict['values']]\n",
    "\n",
    "        merged_sweep_config_dict['parameters'][key] = param_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(merged_sweep_config_dict, project=\"curriculum_deeplab\")\n",
    "    wandb.agent(sweep_id, function=sweep_run)\n",
    "\n",
    "else:\n",
    "    normal_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7469925",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# def inference_DL(run_name, config, inf_dataset):\n",
    "\n",
    "#     score_dicts = []\n",
    "\n",
    "#     fold_iter = range(config.num_folds)\n",
    "#     if config_dict['only_first_fold']:\n",
    "#         fold_iter = fold_iter[0:1]\n",
    "\n",
    "#     for fold_idx in fold_iter:\n",
    "#         lraspp, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "#         lraspp.eval()\n",
    "#         inf_dataset.eval()\n",
    "#         stack_dim = config.use_2d_normal_to\n",
    "\n",
    "#         inf_dices = []\n",
    "#         inf_dices_tumour = []\n",
    "#         inf_dices_cochlea = []\n",
    "\n",
    "#         for inf_sample in inf_dataset:\n",
    "#             global_idx = get_global_idx(fold_idx, sample_idx, config.epochs)\n",
    "#             crossmoda_id = sample['crossmoda_id']\n",
    "#             with amp.autocast(enabled=True):\n",
    "#                 with torch.no_grad():\n",
    "\n",
    "#                     # Create batch out of single val sample\n",
    "#                     b_inf_img = inf_sample['image'].unsqueeze(0)\n",
    "#                     b_inf_seg = inf_sample['label'].unsqueeze(0)\n",
    "\n",
    "#                     B = b_inf_img.shape[0]\n",
    "\n",
    "#                     b_inf_img = b_inf_img.unsqueeze(1).float().cuda()\n",
    "#                     b_inf_seg = b_inf_seg.cuda()\n",
    "#                     b_inf_img_2d = make_2d_stack_from_3d(b_inf_img, stack_dim=stack_dim)\n",
    "\n",
    "#                     if config.use_mind:\n",
    "#                         b_inf_img_2d = mindssc(b_inf_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "#                     output_inf = lraspp(b_inf_img_2d)['out']\n",
    "\n",
    "#                     # Prepare logits for scoring\n",
    "#                     # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "#                     inf_logits_for_score = make_3d_from_2d_stack(output_inf, stack_dim, B)\n",
    "#                     inf_logits_for_score = inf_logits_for_score.argmax(1)\n",
    "\n",
    "#                     inf_dice = dice3d(\n",
    "#                         torch.nn.functional.one_hot(inf_logits_for_score, 3),\n",
    "#                         torch.nn.functional.one_hot(b_inf_seg, 3),\n",
    "#                         one_hot_torch_style=True\n",
    "#                     )\n",
    "#                     inf_dices.append(get_batch_dice_wo_bg(inf_dice))\n",
    "#                     inf_dices_tumour.append(get_batch_dice_tumour(inf_dice))\n",
    "#                     inf_dices_cochlea.append(get_batch_dice_cochlea(inf_dice))\n",
    "\n",
    "#                     if config.do_plot:\n",
    "#                         print(\"Inference 3D image label/ground-truth\")\n",
    "#                         print(inf_dice)\n",
    "#                         # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "#                         display_seg(in_type=\"single_3D\",\n",
    "#                             reduce_dim=\"W\",\n",
    "#                             img=inf_sample['image'].unsqueeze(0).cpu(),\n",
    "#                             seg=inf_logits_for_score.squeeze(0).cpu(),\n",
    "#                             ground_truth=b_inf_seg.squeeze(0).cpu(),\n",
    "#                             crop_to_non_zero_seg=True,\n",
    "#                             crop_to_non_zero_gt=True,\n",
    "#                             alpha_seg=.4,\n",
    "#                             alpha_gt=.2\n",
    "#                         )\n",
    "\n",
    "#             if config.debug:\n",
    "#                 break\n",
    "\n",
    "#         mean_inf_dice = np.nanmean(inf_dices)\n",
    "#         mean_inf_dice_tumour = np.nanmean(inf_dices_tumour)\n",
    "#         mean_inf_dice_cochlea = np.nanmean(inf_dices_cochlea)\n",
    "\n",
    "#         print(f'inf_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_inf_dice*100:.2f}%\")\n",
    "#         print(f'inf_dice_mean_tumour_fold{fold_idx}', f\"{mean_inf_dice_tumour*100:.2f}%\")\n",
    "#         print(f'inf_dice_mean_cochlea_fold{fold_idx}', f\"{mean_inf_dice_cochlea*100:.2f}%\")\n",
    "#         wandb.log({f'scores/inf_dice_mean_wo_bg_fold{fold_idx}': mean_inf_dice}, step=global_idx)\n",
    "#         wandb.log({f'scores/inf_dice_mean_tumour_fold{fold_idx}': mean_inf_dice_tumour}, step=global_idx)\n",
    "#         wandb.log({f'scores/inf_dice_mean_cochlea_fold{fold_idx}': mean_inf_dice_cochlea}, step=global_idx)\n",
    "\n",
    "#         # Store data for inter-fold scoring\n",
    "#         class_dice_list = inf_dices.tolist()[0]\n",
    "#         for class_idx, class_dice in enumerate(class_dice_list):\n",
    "#             score_dicts.append(\n",
    "#                 {\n",
    "#                     'fold_idx': fold_idx,\n",
    "#                     'crossmoda_id': crossmoda_id,\n",
    "#                     'class_idx': class_idx,\n",
    "#                     'class_dice': class_dice,\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     mean_oa_inf_dice = np.nanmean(torch.tensor([score['class_dice'] for score in score_dicts]))\n",
    "#     print(f\"Mean dice over all folds, classes and samples: {mean_oa_inf_dice*100:.2f}%\")\n",
    "#     wandb.log({'scores/mean_dice_all_folds_samples_classes': mean_oa_inf_dice}, step=global_idx)\n",
    "\n",
    "#     return score_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a608620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds_scores = []\n",
    "# run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"testing\", job_type=\"test\",\n",
    "#         config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "#         mode=config_dict['wandb_mode']\n",
    "# )\n",
    "# config = wandb.config\n",
    "# score_dicts = inference_DL(run_name, config, validation_dataset)\n",
    "# folds_scores.append(score_dicts)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984e8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
