{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3175666",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import glob\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars(select=\"* -4\"))\n",
    "import pickle\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import functools\n",
    "from enum import Enum, auto\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, dice2d\n",
    "from mdl_seg_class.visualization import visualize_seg\n",
    "\n",
    "from curriculum_deeplab.mindssc import mindssc\n",
    "from curriculum_deeplab.utils import interpolate_sample, in_notebook, dilate_label_class, LabelDisturbanceMode\n",
    "from curriculum_deeplab.CrossmodaHybridIdLoader import CrossmodaHybridIdLoader, get_crossmoda_data_load_closure\n",
    "from curriculum_deeplab.MobileNet_LR_ASPP_3D import MobileNet_LRASPP_3D\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "if in_notebook:\n",
    "    THIS_SCRIPT_DIR = os.path.abspath('')\n",
    "else:\n",
    "    THIS_SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "print(f\"Running in: {THIS_SCRIPT_DIR}\")\n",
    "\n",
    "def get_batch_dice_per_class(b_dice, class_tags, exclude_bg=True) -> dict:\n",
    "    score_dict = {}\n",
    "    for cls_idx, cls_tag in enumerate(class_tags):\n",
    "        if exclude_bg and cls_idx == 0:\n",
    "            continue\n",
    "\n",
    "        if torch.all(torch.isnan(b_dice[:,cls_idx])):\n",
    "            score = float('nan')\n",
    "        else:\n",
    "            score = np.nanmean(b_dice[:,cls_idx]).item()\n",
    "\n",
    "        score_dict[cls_tag] = score\n",
    "\n",
    "    return score_dict\n",
    "\n",
    "def get_batch_dice_over_all(b_dice, exclude_bg=True) -> float:\n",
    "\n",
    "    start_idx = 1 if exclude_bg else 0\n",
    "    if torch.all(torch.isnan(b_dice[:,start_idx:])):\n",
    "        return float('nan')\n",
    "    return np.nanmean(b_dice[:,start_idx:]).item()\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_stack_batch_size(b_input_size: torch.Size, stack_dim):\n",
    "    assert len(b_input_size) == 5, f\"Input size must be 5D: BxCxDxHxW but is {b_input_size}\"\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input_size[0]*b_input_size[2]\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input_size[0]*b_input_size[3]\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input_size[0]*b_input_size[4]\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_2d_stack_from_3d(b_input, stack_dim):\n",
    "    assert b_input.dim() == 5, f\"Input must be 5D: BxCxDxHxW but is {b_input.shape}\"\n",
    "    B, C, D, H, W = b_input.shape\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4).reshape(B*D, C, H, W)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 3, 1, 2, 4).reshape(B*H, C, D, W)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 4, 1, 2, 3).reshape(B*W, C, D, H)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim '{stack_dim}' must be 'D' or 'H' or 'W'.\")\n",
    "\n",
    "\n",
    "\n",
    "def make_3d_from_2d_stack(b_input, stack_dim, orig_stack_size):\n",
    "    assert b_input.dim() == 4, f\"Input must be 4D: (orig_batch_size/B)xCxSPAT1xSPAT0 but is {b_input.shape}\"\n",
    "    B, C, SPAT1, SPAT0 = b_input.shape\n",
    "    b_input = b_input.reshape(orig_stack_size, int(B//orig_stack_size), C, SPAT1, SPAT0)\n",
    "\n",
    "    if stack_dim == \"D\":\n",
    "        return b_input.permute(0, 2, 1, 3, 4)\n",
    "    if stack_dim == \"H\":\n",
    "        return b_input.permute(0, 2, 3, 1, 4)\n",
    "    if stack_dim == \"W\":\n",
    "        return b_input.permute(0, 2, 3, 4, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"stack_dim is '{stack_dim}' but must be 'D' or 'H' or 'W'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750fa2f-e706-4c65-ac09-c3ab8050b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParamMode(Enum):\n",
    "    INSTANCE_PARAMS = auto()\n",
    "    GRIDDED_INSTANCE_PARAMS = auto()\n",
    "    DISABLED = auto()\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "        See https://stackoverflow.com/questions/49901590/python-using-copy-deepcopy-on-dotdict\n",
    "    \"\"\"\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        try:\n",
    "            return self[item]\n",
    "        except KeyError as e:\n",
    "            raise AttributeError from e\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 3,\n",
    "    'only_first_fold': True,\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "\n",
    "    'use_mind': False,\n",
    "    'epochs': 40,\n",
    "\n",
    "    'batch_size': 16,\n",
    "    'val_batch_size': 1,\n",
    "    'use_2d_normal_to': None,\n",
    "    'train_patchwise': True,\n",
    "\n",
    "    'dataset': 'crossmoda',\n",
    "    'reg_state': \"acummulate_convex_adam_FT2_MT1\",\n",
    "    'train_set_max_len': None,\n",
    "    'crop_3d_w_dim_range': (45, 95),\n",
    "    'crop_2d_slices_gt_num_threshold': 0,\n",
    "\n",
    "    'lr': 0.0005,\n",
    "    'use_cosine_annealing': True,\n",
    "\n",
    "    # Data parameter config\n",
    "    'data_param_mode': DataParamMode.INSTANCE_PARAMS,\n",
    "    'init_inst_param': 0.0,\n",
    "    'lr_inst_param': 0.1,\n",
    "    'use_risk_regularization': True,\n",
    "\n",
    "    'grid_size_y': 64,\n",
    "    'grid_size_x': 64,\n",
    "    # ),\n",
    "\n",
    "    'save_every': 200,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'do_plot': False,\n",
    "    'save_dp_figures': False,\n",
    "    'debug': True,\n",
    "    'wandb_mode': 'disabled', # e.g. online, disabled\n",
    "    'checkpoint_name': None,\n",
    "    'do_sweep': False,\n",
    "\n",
    "    'disturbance_mode': LabelDisturbanceMode.AFFINE,\n",
    "    'disturbance_strength': 2.,\n",
    "    'disturbed_percentage': .3,\n",
    "    'start_disturbing_after_ep': 0,\n",
    "\n",
    "    'start_dilate_kernel_sz': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de18b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(config):\n",
    "    if config.reg_state:\n",
    "        print(\"Loading registered data.\")\n",
    "\n",
    "        REG_STATES = [\n",
    "            \"combined\", \"best_1\", \"best_n\",\n",
    "            \"multiple\", \"mix_combined_best\",\n",
    "            \"best\", \"acummulate_combined_best\"]\n",
    "\n",
    "        # assert config.reg_state in REG_STATES, f\"Unknown registration version. Choose one of {REG_STATES}\"\n",
    "\n",
    "        if config.reg_state == \"mix_combined_best\":\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "\n",
    "            perm = np.random.permutation(len(loaded_identifier))\n",
    "            _clen = int(.5*len(loaded_identifier))\n",
    "            best_choice = perm[:_clen]\n",
    "            combined_choice = perm[_clen:]\n",
    "\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)[best_choice]\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)[combined_choice]\n",
    "            label_data = torch.zeros([107,128,128,128])\n",
    "            label_data[best_choice] = best_label_data\n",
    "            label_data[combined_choice] = combined_label_data\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"acummulate_combined_best\":\n",
    "            label_data_left = torch.load('./data/optimal_reg_left.pth')\n",
    "            label_data_right = torch.load('./data/optimal_reg_right.pth')\n",
    "            loaded_identifier = label_data_left['valid_left_t1'] + label_data_right['valid_right_t1']\n",
    "            best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)\n",
    "            combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)\n",
    "            label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier] + [_id+':var001' for _id in loaded_identifier]\n",
    "\n",
    "        elif config.reg_state == \"acummulate_convex_adam_FT2_MT1\":\n",
    "\n",
    "            label_data = torch.load(\"/share/data_supergrover1/weihsbach/shared_data/important_data_artifacts/curriculum_deeplab/20220113_crossmoda_convex/crossmoda_convex.pth\")\n",
    "            combined_label_data =\n",
    "            # best_label_data = torch.cat([label_data_left['best_all'][:44], label_data_right['best_all'][:63]], dim=0)\n",
    "            # combined_label_data = torch.cat([label_data_left['combined_all'][:44], label_data_right['combined_all'][:63]], dim=0)\n",
    "            # label_data = torch.cat([best_label_data, combined_label_data])\n",
    "            # loaded_identifier = [_id+':var000' for _id in loaded_identifier] + [_id+':var001' for _id in loaded_identifier]\n",
    "\n",
    "        else:\n",
    "            label_data = torch.cat([label_data_left[config.reg_state+'_all'][:44], label_data_right[config.reg_state+'_all'][:63]], dim=0)\n",
    "            loaded_identifier = [_id+':var000' for _id in loaded_identifier]\n",
    "        raise(False)\n",
    "        modified_3d_label_override = {}\n",
    "        for idx, identifier in enumerate(loaded_identifier):\n",
    "            nl_id = int(re.findall(r'\\d+', identifier)[0])\n",
    "            var_id = int(re.findall(r':var(\\d+)$', identifier)[0])\n",
    "            lr_id = re.findall(r'([lr])\\.nii\\.gz', identifier)[0]\n",
    "\n",
    "            crossmoda_var_id = f\"{nl_id:03d}{lr_id}:var{var_id:03d}\"\n",
    "\n",
    "            modified_3d_label_override[crossmoda_var_id] = label_data[idx]\n",
    "\n",
    "        prevent_disturbance = True\n",
    "\n",
    "    else:\n",
    "        modified_3d_label_override = None\n",
    "        prevent_disturbance = False\n",
    "\n",
    "    if config.dataset == 'crossmoda':\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.\n",
    "        clsre = get_crossmoda_data_load_closure(\n",
    "            base_dir=\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "            domain='source', state='l4', use_additional_data=False,\n",
    "            size=(128,128,128), resample=True, normalize=True, crop_3d_w_dim_range=config.crop_3d_w_dim_range,\n",
    "            ensure_labeled_pairs=True,\n",
    "            debug=config.debug\n",
    "        )\n",
    "        training_dataset = CrossmodaHybridIdLoader(\n",
    "            clsre,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(f\"Nonzero slices: \" \\\n",
    "            f\"{sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)*100}%\"\n",
    "        )\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "    if config.dataset == 'ixi':\n",
    "        raise NotImplementedError()\n",
    "        # Use double size in 2D prediction, normal size in 3D\n",
    "        pre_interpolation_factor = 2. if config.use_2d_normal_to is not None else 1.\n",
    "        clsre = get_ixi_data_load_closure()\n",
    "        training_dataset = IXIHybridIdLoader(\n",
    "            clsre,\n",
    "            ensure_labeled_pairs=True,\n",
    "            max_load_3d_num=config.train_set_max_len,\n",
    "            modified_3d_label_override=modified_3d_label_override, prevent_disturbance=prevent_disturbance,\n",
    "            use_2d_normal_to=config.use_2d_normal_to,\n",
    "            crop_2d_slices_gt_num_threshold=config.crop_2d_slices_gt_num_threshold,\n",
    "            pre_interpolation_factor=pre_interpolation_factor\n",
    "        )\n",
    "        training_dataset.eval()\n",
    "        print(f\"Nonzero slices: \" \\\n",
    "            f\"{sum([b['label'].unique().numel() > 1 for b in training_dataset])/len(training_dataset)*100}%\"\n",
    "        )\n",
    "        # validation_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"validation\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "        # target_dataset = CrossmodaHybridIdLoader(\"/share/data_supergrover1/weihsbach/shared_data/tmp/CrossMoDa/\",\n",
    "        #     domain=\"target\", state=\"l4\", ensure_labeled_pairs=True)\n",
    "\n",
    "\n",
    "    elif config['dataset'] == 'organmnist3d':\n",
    "        training_dataset = WrapperOrganMNIST3D(\n",
    "            split='train', root='./data/medmnist', download=True, normalize=True,\n",
    "            max_load_num=300, crop_3d_w_dim_range=None,\n",
    "            disturbed_idxs=None, use_2d_normal_to='W'\n",
    "        )\n",
    "        print(training_dataset.mnist_set.info)\n",
    "        print(\"Classes: \", training_dataset.label_tags)\n",
    "        print(\"Samples: \", len(training_dataset))\n",
    "\n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    _, all_labels, _ = training_dataset.get_data(use_2d_override=False)\n",
    "    print(all_labels.shape)\n",
    "    sum_over_w = torch.sum(all_labels, dim=(0,1,2))\n",
    "    plt.xlabel(\"W\")\n",
    "    plt.ylabel(\"ground truth>0\")\n",
    "    plt.plot(sum_over_w);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0adbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict['do_plot']:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    # Print bare 2D data\n",
    "    # print(\"Displaying 2D bare sample\")\n",
    "    # for img, label in zip(training_dataset.img_data_2d.values(),\n",
    "    #                       training_dataset.label_data_2d.values()):\n",
    "    #     display_seg(in_type=\"single_2D\",\n",
    "    #                 img=img.unsqueeze(0),\n",
    "    #                 ground_truth=label,\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "    # Print transformed 2D data\n",
    "    training_dataset.train(use_modified=False, augment=True)\n",
    "    print(training_dataset.disturbed_idxs)\n",
    "\n",
    "    print(\"Displaying 2D training sample\")\n",
    "    for dist_stren in [0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 5.0]:\n",
    "        print(dist_stren)\n",
    "        training_dataset.disturb_idxs(list(range(0,20,2)),\n",
    "            disturbance_mode=LabelDisturbanceMode.AFFINE,\n",
    "            disturbance_strength=dist_stren\n",
    "        )\n",
    "        img_stack = []\n",
    "        label_stack = []\n",
    "        mod_label_stack = []\n",
    "\n",
    "        for sample in (training_dataset[idx] for idx in range(20)):\n",
    "            img_stack.append(sample['image'])\n",
    "            label_stack.append(sample['label'])\n",
    "            mod_label_stack.append(sample['modified_label'])\n",
    "\n",
    "        # Change label num == hue shift for display\n",
    "        img_stack = torch.stack(img_stack).unsqueeze(1)\n",
    "        label_stack = torch.stack(label_stack)\n",
    "        mod_label_stack = torch.stack(mod_label_stack)\n",
    "\n",
    "        mod_label_stack*=4\n",
    "\n",
    "        visualize_seg(in_type=\"batch_2D\",\n",
    "            img=img_stack,\n",
    "            # ground_truth=label_stack,\n",
    "            seg=(mod_label_stack-label_stack).abs(),\n",
    "            # crop_to_non_zero_gt=True,\n",
    "            crop_to_non_zero_seg=True,\n",
    "            alpha_seg = .6,\n",
    "            file_path=f'out{dist_stren}.png'\n",
    "        )\n",
    "\n",
    "    # Print transformed 3D data\n",
    "    # training_dataset.train()\n",
    "    # print(\"Displaying 3D training sample\")\n",
    "    # leng = 1# training_dataset.__len__(use_2d_override=False)\n",
    "    # for sample in (training_dataset.get_3d_item(idx) for idx in range(leng)):\n",
    "    #     # training_dataset.set_dilate_kernel_size(1)\n",
    "    #     visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "    #                 img=sample['image'].unsqueeze(0),\n",
    "    #                 ground_truth=sample['label'],\n",
    "    #                 crop_to_non_zero_gt=True,\n",
    "    #                 alpha_gt = .3)\n",
    "\n",
    "#         # training_dataset.set_dilate_kernel_size(7)\n",
    "#         display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "#                     img=sample['image'].unsqueeze(0),\n",
    "#                     ground_truth=sample['modified_label'],\n",
    "#                     crop_to_non_zero_gt=True,\n",
    "#                     alpha_gt = .3)\n",
    "\n",
    "    for sidx in [0,]:\n",
    "        print(f\"Sample {sidx}:\")\n",
    "\n",
    "        training_dataset.eval()\n",
    "        sample_eval = training_dataset.get_3d_item(sidx)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_eval['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_eval['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .0)\n",
    "\n",
    "        training_dataset.train()\n",
    "        print(\"Train sample with ground-truth overlay\")\n",
    "        sample_train = training_dataset.get_3d_item(sidx)\n",
    "        print(sample_train['label'].unique())\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=sample_train['image'].unsqueeze(0),\n",
    "                    ground_truth=sample_train['label'],\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt=.3)\n",
    "\n",
    "        print(\"Eval/train diff with diff overlay\")\n",
    "        visualize_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "                    img=(sample_eval['image'] - sample_train['image']).unsqueeze(0),\n",
    "                    ground_truth=(sample_eval['label'] - sample_train['label']).clamp(min=0),\n",
    "                    crop_to_non_zero_gt=True,\n",
    "                    alpha_gt = .3)\n",
    "\n",
    "    train_plotset = (training_dataset.get_3d_item(idx) for idx in (55, 81, 63))\n",
    "    for sample in train_plotset:\n",
    "        print(f\"Sample {sample['dataset_idx']}:\")\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .6)\n",
    "        display_seg(in_type=\"single_3D\", reduce_dim=\"W\",\n",
    "            img=sample_eval['image'].unsqueeze(0),\n",
    "            ground_truth=sample_eval['label'],\n",
    "            crop_to_non_zero_gt=True,\n",
    "            alpha_gt = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add functions to replace modules of a model\n",
    "MOD_GET_FN = lambda self, key: self[int(key)] if isinstance(self, nn.Sequential) \\\n",
    "                                              else getattr(self, key)\n",
    "\n",
    "def get_module(module, keychain):\n",
    "    \"\"\"Retrieves any module inside a pytorch module for a given keychain.\n",
    "       module.named_ to retrieve valid keychains for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return functools.reduce(MOD_GET_FN, keychain.split('.'), module)\n",
    "\n",
    "def set_module(module, keychain, replacee):\n",
    "    \"\"\"Replaces any module inside a pytorch module for a given keychain with \"replacee\".\n",
    "       Use module.named_modules() to retrieve valid keychains for layers.\n",
    "       e.g.\n",
    "       first_keychain = list(module.keys())[0]\n",
    "       new_first_replacee = torch.nn.Conv1d(1,2,3)\n",
    "       set_module(first_keychain, torch.nn.Conv1d(1,2,3))\n",
    "    \"\"\"\n",
    "\n",
    "    key_list = keychain.split('.')\n",
    "    root = functools.reduce(MOD_GET_FN, key_list[:-1], module)\n",
    "    leaf = key_list[-1]\n",
    "    if isinstance(root, nn.Sequential):\n",
    "        root[int(leaf)] = replacee\n",
    "    else:\n",
    "        setattr(root, leaf, replacee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(_path, **statefuls):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "    _path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for name, stful in statefuls.items():\n",
    "        if stful != None:\n",
    "            torch.save(stful.state_dict(), _path.joinpath(name+'.pth'))\n",
    "\n",
    "\n",
    "\n",
    "def get_model(config, dataset_len, num_classes, THIS_SCRIPT_DIR, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    if config.use_mind:\n",
    "        in_channels = 12\n",
    "    else:\n",
    "        in_channels = 1\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        # Use vanilla torch model\n",
    "        lraspp = torchvision.models.segmentation.lraspp_mobilenet_v3_large(\n",
    "            pretrained=False, progress=True, num_classes=num_classes\n",
    "        )\n",
    "        set_module(lraspp, 'backbone.0.0',\n",
    "            torch.nn.Conv2d(in_channels, 16, kernel_size=(3, 3), stride=(2, 2),\n",
    "                            padding=(1, 1), bias=False)\n",
    "        )\n",
    "    else:\n",
    "        # Use custom 3d model\n",
    "        lraspp = MobileNet_LRASPP_3D(\n",
    "            in_num=in_channels, num_classes=num_classes,\n",
    "            use_checkpointing=True\n",
    "        )\n",
    "\n",
    "    # lraspp.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "    lraspp.to(device)\n",
    "    print(f\"Param count lraspp: {sum(p.numel() for p in lraspp.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(lraspp.parameters(), lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # Add data paramters embedding and optimizer\n",
    "    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len, 1, sparse=True)\n",
    "        # p_offset = torch.zeros(1, layout=torch.strided, requires_grad=True)\n",
    "        # p_offset.grad = torch.sparse_coo_tensor([[0]], 1., size=(1,))\n",
    "\n",
    "        # embedding.register_parameter('sigmoid_offset', nn.Parameter(torch.tensor([0.])))\n",
    "        # embedding.sigmoid_offset.register_hook(lambda grad: torch.sparse_coo_tensor([[0]], grad, size=(1,)))\n",
    "        embedding = embedding.to(device)\n",
    "\n",
    "    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "        embedding = nn.Embedding(dataset_len*config.grid_size_y*config.grid_size_x, 1, sparse=True).to(device)\n",
    "    else:\n",
    "        embedding = None\n",
    "\n",
    "\n",
    "    if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "        optimizer_dp = torch.optim.SparseAdam(\n",
    "            embedding.parameters(), lr=config.lr_inst_param,\n",
    "            betas=(0.9, 0.999), eps=1e-08)\n",
    "        torch.nn.init.normal_(embedding.weight.data, mean=config.init_inst_param, std=0.00)\n",
    "\n",
    "        if _path and _path.is_dir():\n",
    "            print(f\"Loading embedding and dp_optimizer from {_path}\")\n",
    "            optimizer_dp.load_state_dict(torch.load(_path.joinpath('optimizer_dp.pth'), map_location=device))\n",
    "            embedding.load_state_dict(torch.load(_path.joinpath('embedding.pth'), map_location=device))\n",
    "\n",
    "        print(f\"Param count embedding: {sum(p.numel() for p in embedding.parameters())}\")\n",
    "\n",
    "    else:\n",
    "        optimizer_dp = None\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading lr-aspp model, optimizers and grad scalers from {_path}\")\n",
    "        lraspp.load_state_dict(torch.load(_path.joinpath('lraspp.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "    else:\n",
    "        print(\"Generating fresh lr-aspp model, optimizer and grad scaler.\")\n",
    "\n",
    "    return (lraspp, optimizer, optimizer_dp, embedding, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f1722",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_global_idx(fold_idx, epoch_idx, max_epochs):\n",
    "    # Get global index e.g. 2250 for fold_idx=2, epoch_idx=250 @ max_epochs<1000\n",
    "    return 10**len(str(int(max_epochs)))*fold_idx + epoch_idx\n",
    "\n",
    "\n",
    "\n",
    "def save_parameter_figure(_path, title, text, parameters, reweighted_parameters, dices):\n",
    "    # Show weights and weights with compensation\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12, 4), dpi=80)\n",
    "    sc1 = axs[0].scatter(\n",
    "        range(len(parameters)),\n",
    "        parameters.cpu().detach(), c=dices,s=1, cmap='plasma', vmin=0., vmax=1.)\n",
    "    sc2 = axs[1].scatter(\n",
    "        range(len(reweighted_parameters)),\n",
    "        reweighted_parameters.cpu().detach(), s=1,c=dices, cmap='plasma', vmin=0., vmax=1.)\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.text(0, 0, text)\n",
    "    axs[0].set_title('Bare parameters')\n",
    "    axs[1].set_title('Reweighted parameters')\n",
    "    axs[0].set_ylim(-10, 10)\n",
    "    axs[1].set_ylim(-3, 1)\n",
    "    plt.colorbar(sc2)\n",
    "    plt.savefig(_path)\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def calc_inst_parameters_in_target_pos_ratio(dpm, disturbed_inst_idxs, target_pos='min'):\n",
    "\n",
    "    assert target_pos == 'min' or target_pos == 'max', \"Value of target_pos must be 'min' or 'max'.\"\n",
    "    descending = False if target_pos == 'min' else True\n",
    "\n",
    "    target_len = len(disturbed_inst_idxs)\n",
    "\n",
    "    disturbed_params = dpm.get_parameter_list(inst_keys=disturbed_inst_idxs)\n",
    "    all_params = sorted(dpm.get_parameter_list(inst_keys='all'), reverse=descending)\n",
    "    target_param_ids = [id(param) for param in all_params[:target_len]]\n",
    "\n",
    "    ratio = [1. for param in disturbed_params if id(param) in target_param_ids]\n",
    "    ratio = sum(ratio)/target_len\n",
    "    return ratio\n",
    "\n",
    "def log_data_parameter_stats(log_path, epx, data_parameters):\n",
    "    \"\"\"Log stats for data parameters on wandb.\"\"\"\n",
    "    wandb.log({f'{log_path}/highest': torch.max(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/lowest': torch.min(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/mean': torch.mean(data_parameters).item()}, step=epx)\n",
    "    wandb.log({f'{log_path}/std': torch.std(data_parameters).item()}, step=epx)\n",
    "\n",
    "\n",
    "\n",
    "def reset_determinism():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_class_dices(log_prefix, log_postfix, class_dices, log_idx):\n",
    "    if not class_dices:\n",
    "        return\n",
    "\n",
    "    for cls_name in class_dices[0].keys():\n",
    "        log_path = f\"{log_prefix}{cls_name}{log_postfix}\"\n",
    "\n",
    "        cls_dices = list(map(lambda dct: dct[cls_name], class_dices))\n",
    "        mean_per_class =np.nanmean(cls_dices)\n",
    "        print(log_path, f\"{mean_per_class*100:.2f}%\")\n",
    "        wandb.log({log_path: mean_per_class}, step=log_idx)\n",
    "\n",
    "def CELoss(logits, targets, bin_weight=None):\n",
    "    # -logits[0,targets[0,0,0],0,0]+torch.log(logits[0,:,0,0].exp().sum())\n",
    "    targets = torch.nn.functional.one_hot(targets).permute(0,3,1,2)\n",
    "    loss = -targets*F.log_softmax(logits, 1)\n",
    "\n",
    "    if bin_weight is not None:\n",
    "        brdcast_shape = torch.Size((loss.shape[0], loss.shape[1], *((loss.dim()-2)*[1])))\n",
    "        inv_weight = (bin_weight+np.exp(1)).log()+np.exp(1)\n",
    "        inv_weight = inv_weight/inv_weight.mean()\n",
    "        loss = inv_weight.view(brdcast_shape)*loss\n",
    "\n",
    "    return loss.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embedding_idxs(idxs, grid_size_y, grid_size_x):\n",
    "    with torch.no_grad():\n",
    "        t_sz = grid_size_y * grid_size_x\n",
    "        return ((idxs*t_sz).long().repeat(t_sz).view(t_sz, idxs.numel())+torch.tensor(range(t_sz)).to(idxs).view(t_sz,1)).permute(1,0).reshape(-1)\n",
    "\n",
    "def inference_wrap(lraspp, img, use_2d, use_mind):\n",
    "    with torch.inference_mode():\n",
    "        b_img = img.unsqueeze(0).unsqueeze(0).float()\n",
    "        if use_2d and use_mind:\n",
    "            # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "            b_img = mindssc(b_img.unsqueeze(0)).squeeze(2)\n",
    "        elif not use_2d and use_mind:\n",
    "            # MIND 3D in Bx1xDxHxW out BxMINDxDxHxW\n",
    "            b_img = mindssc(b_img)\n",
    "        elif use_2d or not use_2d:\n",
    "            # 2D Bx1xHxW\n",
    "            # 3D out Bx1xDxHxW\n",
    "            pass\n",
    "\n",
    "        b_out = lraspp(b_img)['out']\n",
    "        b_out = b_out.argmax(1)\n",
    "        return b_out\n",
    "\n",
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    kf.get_n_splits(training_dataset)\n",
    "\n",
    "    fold_iter = enumerate(kf.split(training_dataset))\n",
    "\n",
    "    if config.get('fold_override', None):\n",
    "        selected_fold = config.get('fold_override', 0)\n",
    "        fold_iter = list(fold_iter)[selected_fold:selected_fold+1]\n",
    "    elif config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "\n",
    "    if config.wandb_mode != 'disabled':\n",
    "        # Log dataset info\n",
    "        training_dataset.eval()\n",
    "        dataset_info = [[smp['dataset_idx'], smp['id'], smp['image_path'], smp['label_path']] \\\n",
    "                        for smp in training_dataset]\n",
    "        wandb.log({'datasets/training_dataset':wandb.Table(columns=['dataset_idx', 'id', 'image', 'label'], data=dataset_info)}, step=0)\n",
    "\n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        # Training happens in 2D, validation happens in 3D:\n",
    "        # Read 2D dataset idxs which are used for training,\n",
    "        # get their 3D super-ids and substract these from all 3D ids to get val_3d_idxs\n",
    "\n",
    "        if config.use_2d_normal_to is not None:\n",
    "            n_dims = (-2,-1)\n",
    "            trained_3d_dataset_ids = training_dataset.get_3d_from_2d_identifiers(train_idxs, 'id')\n",
    "            # trained_3d_trained_ids = training_dataset.switch_3d_identifiers(trained_3d_dataset_idxs)\n",
    "            all_3d_ids = training_dataset.get_3d_ids()\n",
    "            val_3d_ids = set(all_3d_ids) - set(trained_3d_dataset_ids)\n",
    "            val_3d_idxs = list({\n",
    "                training_dataset.extract_short_3d_id(_id):idx \\\n",
    "                    for idx, _id in enumerate(all_3d_ids) if _id in val_3d_ids}.values())\n",
    "        else:\n",
    "            n_dims = (-3,-2,-1)\n",
    "            val_3d_idxs = val_idxs\n",
    "        print(\"Will run validation with these 3D samples:\", val_3d_idxs)\n",
    "\n",
    "        _, _, all_modified_segs = training_dataset.get_data()\n",
    "\n",
    "        non_empty_train_idxs = train_idxs[(all_modified_segs[train_idxs].sum(dim=n_dims) > 0)]\n",
    "\n",
    "        ### Disturb dataset (only non-emtpy idxs)###\n",
    "        proposed_disturbed_idxs = np.random.choice(non_empty_train_idxs, size=int(len(non_empty_train_idxs)*config.disturbed_percentage), replace=False)\n",
    "        proposed_disturbed_idxs = torch.tensor(proposed_disturbed_idxs)\n",
    "        training_dataset.disturb_idxs(proposed_disturbed_idxs,\n",
    "            disturbance_mode=config.disturbance_mode,\n",
    "            disturbance_strength=config.disturbance_strength\n",
    "        )\n",
    "\n",
    "        disturbed_bool_vect = torch.zeros(len(training_dataset))\n",
    "        disturbed_bool_vect[training_dataset.disturbed_idxs] = 1.\n",
    "\n",
    "        clean_idxs = train_idxs[np.isin(train_idxs, training_dataset.disturbed_idxs, invert=True)]\n",
    "        print(\"Disturbed indexes:\", sorted(training_dataset.disturbed_idxs))\n",
    "\n",
    "        if clean_idxs.numel() < 200:\n",
    "            print(f\"Clean indexes: {sorted(clean_idxs.tolist())}\")\n",
    "\n",
    "        wandb.log({f'datasets/disturbed_idxs_fold{fold_idx}':wandb.Table(columns=['train_idxs'], data=[[idx] for idx in training_dataset.disturbed_idxs])},\n",
    "            step=get_global_idx(fold_idx, 0, config.epochs))\n",
    "\n",
    "        ### Configure MIND ###\n",
    "        if config.use_mind:\n",
    "            in_channels = 12\n",
    "        else:\n",
    "            in_channels = 1\n",
    "\n",
    "        class_weights = 1/(torch.bincount(all_modified_segs.reshape(-1).long())).float().pow(.35)\n",
    "        class_weights /= class_weights.mean()\n",
    "\n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "            sampler=train_subsampler, pin_memory=True, drop_last=False,\n",
    "            # collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "        )\n",
    "\n",
    "        training_dataset.set_augment_at_collate(False)\n",
    "\n",
    "#         val_dataloader = DataLoader(training_dataset, batch_size=config.val_batch_size,\n",
    "#                                     sampler=val_subsampler, pin_memory=True, drop_last=False)\n",
    "\n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        epx_start = config.get('checkpoint_epx', 0)\n",
    "\n",
    "        if config.checkpoint_name:\n",
    "            # Load from checkpoint\n",
    "            _path = f\"{config.mdl_save_prefix}/{config.checkpoint_name}_fold{fold_idx}_epx{epx_start}\"\n",
    "        else:\n",
    "            _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx_start}\"\n",
    "\n",
    "        (lraspp, optimizer, optimizer_dp, embedding, scaler) = get_model(config, len(training_dataset), len(training_dataset.label_tags),\n",
    "            THIS_SCRIPT_DIR=THIS_SCRIPT_DIR, _path=_path, device='cuda')\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=500, T_mult=2)\n",
    "\n",
    "        if optimizer_dp:\n",
    "            scheduler_dp = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer_dp, T_0=500, T_mult=2)\n",
    "        else:\n",
    "            scheduler_dp = None\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare corr coefficient scoring\n",
    "        training_dataset.eval(use_modified=True)\n",
    "        wise_labels, mod_labels = list(zip(*[(sample['label'], sample['modified_label']) \\\n",
    "            for sample in training_dataset]))\n",
    "        wise_labels, mod_labels = torch.stack(wise_labels), torch.stack(mod_labels)\n",
    "\n",
    "        dice_func = dice2d if config.use_2d_normal_to is not None else dice3d\n",
    "\n",
    "        wise_dice = dice_func(\n",
    "            torch.nn.functional.one_hot(wise_labels, len(training_dataset.label_tags)),\n",
    "            torch.nn.functional.one_hot(mod_labels, len(training_dataset.label_tags)),\n",
    "            one_hot_torch_style=True, nan_for_unlabeled_target=False\n",
    "        )\n",
    "\n",
    "        gt_num = (mod_labels > 0).sum(dim=n_dims)\n",
    "        t_metric = (gt_num+np.exp(1)).log()+np.exp(1)\n",
    "\n",
    "        if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "            union_wise_mod_label = torch.logical_or(wise_labels, mod_labels)\n",
    "            union_wise_mod_label = union_wise_mod_label.cuda()\n",
    "\n",
    "        class_weights = class_weights.cuda()\n",
    "        gt_num = gt_num.cuda()\n",
    "        t_metric = t_metric.cuda()\n",
    "\n",
    "        for epx in range(epx_start, config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "\n",
    "            lraspp.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            training_dataset.train(use_modified=(epx >= config.start_disturbing_after_ep))\n",
    "            wandb.log({\"use_modified\": float(training_dataset.use_modified)}, step=global_idx)\n",
    "\n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            class_dices = []\n",
    "\n",
    "            # Load data\n",
    "            for batch_idx, batch in enumerate(train_dataloader):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if optimizer_dp:\n",
    "                    optimizer_dp.zero_grad()\n",
    "\n",
    "                b_img = batch['image']\n",
    "                b_seg = batch['label']\n",
    "                b_spat_aug_grid = batch['spat_augment_grid']\n",
    "\n",
    "                b_seg_modified = batch['modified_label']\n",
    "                b_idxs_dataset = batch['dataset_idx']\n",
    "                b_img = b_img.float()\n",
    "\n",
    "                b_img = b_img.cuda()\n",
    "                b_seg_modified = b_seg_modified.cuda()\n",
    "                b_idxs_dataset = b_idxs_dataset.cuda()\n",
    "                b_seg = b_seg.cuda()\n",
    "                b_spat_aug_grid = b_spat_aug_grid.cuda()\n",
    "\n",
    "                if training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 2D, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                    b_img = mindssc(b_img.unsqueeze(1).unsqueeze(1)).squeeze(2)\n",
    "                elif not training_dataset.use_2d() and config.use_mind:\n",
    "                    # MIND 3D\n",
    "                    b_img = mindssc(b_img.unsqueeze(1))\n",
    "                else:\n",
    "                    b_img = b_img.unsqueeze(1)\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=True):\n",
    "                    assert b_img.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input image for model must be {len(n_dims)+2}D: BxCxSPATIAL but is {b_img.shape}\"\n",
    "\n",
    "                    logits = lraspp(b_img)['out']\n",
    "\n",
    "                    ### Calculate loss ###\n",
    "                    assert logits.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input shape for loss must be BxNUM_CLASSESxSPATIAL but is {logits.shape}\"\n",
    "                    assert b_seg_modified.dim() == len(n_dims)+1, \\\n",
    "                        f\"Target shape for loss must be BxSPATIAL but is {b_seg_modified.shape}\"\n",
    "\n",
    "                    if config.data_param_mode == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                        # batch_bins = torch.zeros([len(b_idxs_dataset), len(training_dataset.label_tags)]).to(logits.device)\n",
    "                        # bin_list = [slc.view(-1).bincount() for slc in b_seg_modified]\n",
    "                        # for b_idx, _bins in enumerate(bin_list):\n",
    "                        #     batch_bins[b_idx][:len(_bins)] = _bins\n",
    "                        # loss = CELoss(logits, b_seg_modified, bin_weight=batch_bins)\n",
    "\n",
    "                        loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified)\n",
    "                        loss = loss.mean(n_dims)\n",
    "\n",
    "                        bare_weight = embedding(b_idxs_dataset).squeeze()\n",
    "\n",
    "                        weight = torch.sigmoid(bare_weight)\n",
    "                        weight = weight/weight.mean()\n",
    "\n",
    "                        # This improves scores significantly: Reweight with log(gt_numel)\n",
    "                        weight = weight/t_metric[b_idxs_dataset]\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = logits.argmax(1)\n",
    "\n",
    "                        if config.use_risk_regularization:\n",
    "                            p_pred_num = (logits_for_score > 0).sum(dim=n_dims).detach()\n",
    "                            risk_regularization = -weight*p_pred_num/(logits_for_score.shape[-2]*logits_for_score.shape[-1])\n",
    "                            loss = (loss*weight).sum() + risk_regularization.sum()\n",
    "                        else:\n",
    "                            loss = (loss*weight).sum()\n",
    "\n",
    "                    elif config.data_param_mode == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                        loss = nn.CrossEntropyLoss(reduction='none')(logits, b_seg_modified)\n",
    "                        m_dp_idxs = map_embedding_idxs(b_idxs_dataset, config.grid_size_y, config.grid_size_x)\n",
    "                        weight = embedding(m_dp_idxs)\n",
    "                        weight = weight.reshape(-1, config.grid_size_y, config.grid_size_x)\n",
    "                        weight = weight.unsqueeze(1)\n",
    "                        weight = torch.nn.functional.interpolate(\n",
    "                            weight,\n",
    "                            size=(b_seg_modified.shape[-2:]),\n",
    "                            mode='bilinear',\n",
    "                            align_corners=True\n",
    "                        )\n",
    "                        weight = weight/weight.mean()\n",
    "                        weight = F.grid_sample(weight, b_spat_aug_grid,\n",
    "                            padding_mode='border', align_corners=False)\n",
    "                        loss = (loss.unsqueeze(1)*weight).sum()\n",
    "\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = (logits*weight).argmax(1)\n",
    "\n",
    "                    else:\n",
    "                        loss = nn.CrossEntropyLoss(class_weights)(logits, b_seg_modified)\n",
    "                        # Prepare logits for scoring\n",
    "                        logits_for_score = logits.argmax(1)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "\n",
    "                if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                    scaler.step(optimizer_dp)\n",
    "\n",
    "                scaler.update()\n",
    "\n",
    "                epx_losses.append(loss.item())\n",
    "\n",
    "                # Calculate dice score\n",
    "                b_dice = dice_func(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(b_seg, len(training_dataset.label_tags)), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_over_all(\n",
    "                    b_dice, exclude_bg=True))\n",
    "                class_dices.append(get_batch_dice_per_class(\n",
    "                    b_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                ###  Scheduler management ###\n",
    "                if config.use_cosine_annealing:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if config.save_dp_figures and batch_idx % 10 == 0:\n",
    "                    # Output data parameter figure\n",
    "                    train_params = embedding.weight[train_idxs].squeeze()\n",
    "                    # order = np.argsort(train_params.cpu().detach()) # Order by DP value\n",
    "                    order = torch.arange(len(train_params))\n",
    "                    wise_corr_coeff = np.corrcoef(train_params.cpu().detach(), wise_dice[train_idxs][:,1].cpu().detach())[0,1]\n",
    "                    dp_figure_path = Path(f\"data/output_figures/{wandb.run.name}_fold{fold_idx}/dp_figure_epx{epx:03d}_batch{batch_idx:03d}.png\")\n",
    "                    dp_figure_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    save_parameter_figure(dp_figure_path, wandb.run.name, f\"corr. coeff. DP vs. dice(expert label, train gt): {wise_corr_coeff:4f}\",\n",
    "                        train_params[order], train_params[order]/t_metric[train_idxs][order], dices=wise_dice[train_idxs][:,1][order])\n",
    "\n",
    "                if config.debug:\n",
    "                    break\n",
    "\n",
    "            ### Logging ###\n",
    "            print(f\"### Log epoch {epx} @ {time.time()-t0:.2f}s\")\n",
    "            print(\"### Training\")\n",
    "            ### Log wandb data ###\n",
    "            # Log the epoch idx per fold - so we can recover the diagram by setting\n",
    "            # ref_epoch_idx as x-axis in wandb interface\n",
    "            wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "\n",
    "            mean_loss = torch.tensor(epx_losses).mean()\n",
    "            wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "\n",
    "            mean_dice = np.nanmean(dices)\n",
    "            print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "            wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "\n",
    "            log_class_dices(\"scores/dice_mean_\", f\"_fold{fold_idx}\", class_dices, global_idx)\n",
    "\n",
    "            # Log data parameters of disturbed samples\n",
    "            if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "                # Calculate dice score corr coeff (unknown to network)\n",
    "                train_params = embedding.weight[train_idxs].squeeze()\n",
    "                order = np.argsort(train_params.cpu().detach())\n",
    "                wise_corr_coeff = np.corrcoef(train_params[order].cpu().detach(), wise_dice[train_idxs][:,1][order].cpu().detach())[0,1]\n",
    "\n",
    "                wandb.log(\n",
    "                    {f'data_parameters/wise_corr_coeff_fold{fold_idx}': wise_corr_coeff},\n",
    "                    step=global_idx\n",
    "                )\n",
    "                print(f'data_parameters/wise_corr_coeff_fold{fold_idx}', f\"{wise_corr_coeff:.2f}\")\n",
    "\n",
    "                # Log stats of data parameters and figure\n",
    "                log_data_parameter_stats(f'data_parameters/iter_stats_fold{fold_idx}', global_idx, embedding.weight.data)\n",
    "\n",
    "                # Map gridded instance parameters\n",
    "                if str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                    m_tr_idxs = map_embedding_idxs(train_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                    ).cuda()\n",
    "                    masks = union_wise_mod_label[train_idxs].float()\n",
    "                    masked_weights = torch.nn.functional.interpolate(\n",
    "                        embedding(m_tr_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                        size=(masks.shape[-2:]),\n",
    "                        mode='bilinear',\n",
    "                        align_corners=True\n",
    "                    ).squeeze(1) * masks\n",
    "                    masked_weights[masked_weights==0.] = float('nan')\n",
    "\n",
    "            if (epx % config.save_every == 0 and epx != 0) \\\n",
    "                or (epx+1 == config.epochs):\n",
    "                _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx}\"\n",
    "                save_model(\n",
    "                    _path,\n",
    "                    lraspp=lraspp,\n",
    "                    optimizer=optimizer, optimizer_dp=optimizer_dp,\n",
    "                    scheduler=scheduler, scheduler_dp=scheduler_dp,\n",
    "                    embedding=embedding,\n",
    "                    scaler=scaler)\n",
    "                (lraspp, optimizer, optimizer_dp, embedding, scaler) = \\\n",
    "                    get_model(\n",
    "                        config, len(training_dataset),\n",
    "                        len(training_dataset.label_tags),\n",
    "                        THIS_SCRIPT_DIR=THIS_SCRIPT_DIR,\n",
    "                        _path=_path, device='cuda')\n",
    "\n",
    "            print()\n",
    "            print(\"### Validation\")\n",
    "            lraspp.eval()\n",
    "            training_dataset.eval()\n",
    "\n",
    "            val_dices = []\n",
    "            val_class_dices = []\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                with torch.no_grad():\n",
    "                    for val_idx in val_3d_idxs:\n",
    "                        val_sample = training_dataset.get_3d_item(val_idx)\n",
    "                        stack_dim = training_dataset.use_2d_normal_to\n",
    "                        # Create batch out of single val sample\n",
    "                        b_val_img = val_sample['image'].unsqueeze(0)\n",
    "                        b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "\n",
    "                        B = b_val_img.shape[0]\n",
    "\n",
    "                        b_val_img = b_val_img.unsqueeze(1).float().cuda()\n",
    "                        b_val_seg = b_val_seg.cuda()\n",
    "\n",
    "                        if training_dataset.use_2d():\n",
    "                            b_val_img_2d = make_2d_stack_from_3d(b_val_img, stack_dim=training_dataset.use_2d_normal_to)\n",
    "\n",
    "                            if config.use_mind:\n",
    "                                # MIND 2D model, in Bx1x1xHxW, out BxMINDxHxW\n",
    "                                b_val_img_2d = mindssc(b_val_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "                            output_val = lraspp(b_val_img_2d)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "                            # Prepare logits for scoring\n",
    "                            # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "                            val_logits_for_score = make_3d_from_2d_stack(\n",
    "                                val_logits_for_score.unsqueeze(1), stack_dim, B\n",
    "                            ).squeeze(1)\n",
    "\n",
    "                        else:\n",
    "                            if config.use_mind:\n",
    "                                # MIND 3D model shape BxMINDxDxHxW\n",
    "                                b_val_img = mindssc(b_val_img)\n",
    "                            else:\n",
    "                                # 3D model shape Bx1xDxHxW\n",
    "                                pass\n",
    "\n",
    "                            output_val = lraspp(b_val_img)['out']\n",
    "                            val_logits_for_score = output_val.argmax(1)\n",
    "\n",
    "                        b_val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(val_logits_for_score, len(training_dataset.label_tags)),\n",
    "                            torch.nn.functional.one_hot(b_val_seg, len(training_dataset.label_tags)),\n",
    "                            one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        # Get mean score over batch\n",
    "                        val_dices.append(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=True))\n",
    "\n",
    "                        val_class_dices.append(get_batch_dice_per_class(\n",
    "                            b_val_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                        if config.do_plot:\n",
    "                            print(f\"Validation 3D image label/ground-truth {val_3d_idxs}\")\n",
    "                            print(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=False))\n",
    "                            # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "                            display_seg(in_type=\"single_3D\",\n",
    "                                reduce_dim=\"W\",\n",
    "                                img=val_sample['image'].unsqueeze(0).cpu(),\n",
    "                                seg=val_logits_for_score_3d.squeeze(0).cpu(), # CHECK TODO\n",
    "                                ground_truth=b_val_seg.squeeze(0).cpu(),\n",
    "                                crop_to_non_zero_seg=True,\n",
    "                                crop_to_non_zero_gt=True,\n",
    "                                alpha_seg=.3,\n",
    "                                alpha_gt=.0\n",
    "                            )\n",
    "\n",
    "                    mean_val_dice = np.nanmean(val_dices)\n",
    "                    print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                    wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                    log_class_dices(\"scores/val_dice_mean_\", f\"_fold{fold_idx}\", val_class_dices, global_idx)\n",
    "\n",
    "            print()\n",
    "            # End of training loop\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        if str(config.data_param_mode) != str(DataParamMode.DISABLED):\n",
    "            # Write sample data\n",
    "\n",
    "            training_dataset.eval(use_modified=True)\n",
    "            all_idxs = torch.tensor(range(len(training_dataset))).cuda()\n",
    "            train_label_snapshot_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/train_label_snapshot.pth\")\n",
    "            seg_viz_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weighted_samples.png\")\n",
    "\n",
    "            train_label_snapshot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if str(config.data_param_mode) == str(DataParamMode.INSTANCE_PARAMS):\n",
    "                dp_weights = embedding(all_idxs)\n",
    "                save_data = []\n",
    "                data_generator = zip(\n",
    "                    dp_weights[train_idxs], \\\n",
    "                    disturbed_bool_vect[train_idxs],\n",
    "                    torch.utils.data.Subset(training_dataset, train_idxs)\n",
    "                )\n",
    "\n",
    "                for dp_weight, disturb_flg, sample in data_generator:\n",
    "                    data_tuple = ( \\\n",
    "                        dp_weight,\n",
    "                        bool(disturb_flg.item()),\n",
    "                        sample['id'],\n",
    "                        sample['dataset_idx'],\n",
    "                        sample['image'],\n",
    "                        sample['label'],\n",
    "                        sample['modified_label'],\n",
    "                        inference_wrap(lraspp, sample['image'].cuda(), use_2d=training_dataset.use_2d(), use_mind=config.use_mind)\n",
    "                    )\n",
    "                    save_data.append(data_tuple)\n",
    "\n",
    "                save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "                (dp_weight, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _imgs,\n",
    "                 _labels, _modified_labels, _predictions) = zip(*save_data)\n",
    "\n",
    "                dp_weight = torch.stack(dp_weight)\n",
    "                dataset_idxs = torch.stack(dataset_idxs)\n",
    "                _imgs = torch.stack(_imgs)\n",
    "                _labels = torch.stack(_labels)\n",
    "                _modified_labels = torch.stack(_modified_labels)\n",
    "                _predictions = torch.stack(_predictions)\n",
    "\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'data_parameters': dp_weight.cpu(),\n",
    "                        'disturb_flags': disturb_flags,\n",
    "                        'd_ids': d_ids,\n",
    "                        'dataset_idxs': dataset_idxs.cpu(),\n",
    "                        'labels': _labels.cpu().to_sparse(),\n",
    "                        'modified_labels': _modified_labels.cpu().to_sparse(),\n",
    "                        'train_predictions': _predictions.cpu().to_sparse(),\n",
    "                    },\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "            elif str(config.data_param_mode) == str(DataParamMode.GRIDDED_INSTANCE_PARAMS):\n",
    "                # Log histogram of clean and disturbed parameters\n",
    "                if not training_dataset.use_2d():\n",
    "                    raise NotImplementedError(\"Script does not support 3D model and GRIDDED_INSTANCE_PARAMS yet.\")\n",
    "\n",
    "                m_all_idxs = map_embedding_idxs(all_idxs,\n",
    "                        config.grid_size_y, config.grid_size_x\n",
    "                ).cuda()\n",
    "                masks = union_wise_mod_label[all_idxs].float()\n",
    "                all_weights = torch.nn.functional.interpolate(\n",
    "                    embedding(m_all_idxs).view(-1,1,config.grid_size_y, config.grid_size_x),\n",
    "                    size=(masks.shape[-2:]),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=True\n",
    "                ).squeeze(1)\n",
    "\n",
    "                masked_weights = all_weights * masks\n",
    "                masked_weights[masked_weights==0.] = float('nan')\n",
    "                dp_weights = np.nanmean(masked_weights.detach().cpu(), axis=n_dims)\n",
    "\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}/data_parameter_weightmap.png\")\n",
    "\n",
    "                save_data = []\n",
    "                data_generator = zip(\n",
    "                    dp_weights[train_idxs],\n",
    "                    all_weights[train_idxs],\n",
    "                    disturbed_bool_vect[train_idxs],\n",
    "                    torch.utils.data.Subset(training_dataset,train_idxs)\n",
    "                )\n",
    "\n",
    "                for dp_weight, weightmap, disturb_flg, sample in data_generator:\n",
    "                    data_tuple = (\n",
    "                        dp_weight,\n",
    "                        weightmap,\n",
    "                        bool(disturb_flg.item()),\n",
    "                        sample['id'],\n",
    "                        sample['dataset_idx'],\n",
    "                        sample['image'],\n",
    "                        sample['label'],\n",
    "                        sample['modified_label']\n",
    "                    )\n",
    "                    save_data.append(data_tuple)\n",
    "\n",
    "                save_data = sorted(save_data, key=lambda tpl: tpl[0])\n",
    "\n",
    "                (dp_weight, dp_weightmap, disturb_flags,\n",
    "                 d_ids, dataset_idxs, _2d_imgs,\n",
    "                 _2d_labels, _2d_modified_labels) = zip(*save_data)\n",
    "\n",
    "                dp_weight = torch.stack(dp_weight)\n",
    "                dp_weightmap = torch.stack(dp_weightmap)\n",
    "                dataset_idxs = torch.stack(dataset_idxs)\n",
    "                _2d_imgs = torch.stack(_2d_imgs)\n",
    "                _2d_labels = torch.stack(_2d_labels)\n",
    "                _2d_modified_labels = torch.stack(_2d_modified_labels)\n",
    "                _2d_predictions = torch.stack(_2d_predictions)\n",
    "\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'data_parameters': dp_weight.cpu(),\n",
    "                        'data_parameter_weightmaps': dp_weightmap.cpu(),\n",
    "                        'disturb_flags': disturb_flags,\n",
    "                        'd_ids': d_ids,\n",
    "                        'dataset_idxs': dataset_idxs.cpu(),\n",
    "                        'labels': _2d_labels.cpu().to_sparse(),\n",
    "                        'modified_labels': _2d_modified_labels.cpu().to_sparse(),\n",
    "                        'train_predictions': _2d_predictions.cpu().to_sparse(),\n",
    "                    },\n",
    "                    train_label_snapshot_path\n",
    "                )\n",
    "\n",
    "                print(\"Writing weight map image.\")\n",
    "                weightmap_out_path = Path(THIS_SCRIPT_DIR).joinpath(f\"data/output/{wandb.run.name}_fold{fold_idx}_epx{epx}_data_parameter_weightmap.png\")\n",
    "                visualize_seg(in_type=\"batch_2D\",\n",
    "                    img=torch.stack(dp_weightmap).unsqueeze(1),\n",
    "                    seg=torch.stack(_2d_modified_labels),\n",
    "                    alpha_seg = 0.,\n",
    "                    n_per_row=70,\n",
    "                    overlay_text=overlay_text_list,\n",
    "                    annotate_color=(0,255,255),\n",
    "                    frame_elements=disturb_flags,\n",
    "                    file_path=weightmap_out_path,\n",
    "                )\n",
    "\n",
    "            if len(training_dataset.disturbed_idxs) > 0:\n",
    "                # Log histogram\n",
    "                separated_params = list(zip(dp_weights[clean_idxs], dp_weights[training_dataset.disturbed_idxs]))\n",
    "                s_table = wandb.Table(columns=['clean_idxs', 'disturbed_idxs'], data=separated_params)\n",
    "                fields = {\"primary_bins\" : \"clean_idxs\", \"secondary_bins\" : \"disturbed_idxs\", \"title\" : \"Data parameter composite histogram\"}\n",
    "                composite_histogram = wandb.plot_table(vega_spec_name=\"rap1ide/composite_histogram\", data_table=s_table, fields=fields)\n",
    "                wandb.log({f\"data_parameters/separated_params_fold_{fold_idx}\": composite_histogram})\n",
    "\n",
    "            # Write out data of modified and un-modified labels and an overview image\n",
    "            print(\"Writing train sample image.\")\n",
    "            # overlay text example: d_idx=0, dp_i=1.00, dist? False\n",
    "            overlay_text_list = [f\"id:{d_id} dp:{instance_p.item():.2f}\" \\\n",
    "                for d_id, instance_p, disturb_flg in zip(d_ids, dp_weight, disturb_flags)]\n",
    "\n",
    "            if training_dataset.use_2d():\n",
    "                reduce_dim = None\n",
    "                in_type = \"batch_2D\"\n",
    "            else:\n",
    "                reduce_dim = \"W\"\n",
    "                in_type = \"batch_3D\"\n",
    "\n",
    "            use_2d = training_dataset.use_2d()\n",
    "            scf = 1/training_dataset.pre_interpolation_factor\n",
    "\n",
    "            show_img = interpolate_sample(b_label=_labels, scale_factor=scf, use_2d=use_2d)[1].unsqueeze(1)\n",
    "            show_seg = interpolate_sample(b_label=4*_predictions.squeeze(1), scale_factor=scf, use_2d=use_2d)[1]\n",
    "            show_gt = interpolate_sample(b_label=_modified_labels, scale_factor=scf, use_2d=use_2d)[1]\n",
    "\n",
    "            visualize_seg(in_type=in_type, reduce_dim=reduce_dim,\n",
    "                img=show_img, # Expert label in BW\n",
    "                seg=show_seg, # Prediction in blue\n",
    "                ground_truth=show_gt, # Modified label in red\n",
    "                crop_to_non_zero_seg=False,\n",
    "                alpha_seg = .5,\n",
    "                alpha_gt = .5,\n",
    "                n_per_row=70,\n",
    "                overlay_text=overlay_text_list,\n",
    "                annotate_color=(0,255,255),\n",
    "                frame_elements=disturb_flags,\n",
    "                file_path=seg_viz_out_path,\n",
    "            )\n",
    "\n",
    "        # End of fold loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fc80a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Config overrides\n",
    "# config_dict['wandb_mode'] = 'disabled'\n",
    "# config_dict['debug'] = True\n",
    "# Model loading\n",
    "# config_dict['checkpoint_name'] = 'treasured-water-717'\n",
    "# # config_dict['fold_override'] = 0\n",
    "# config_dict['checkpoint_epx'] = 39\n",
    "\n",
    "# Define sweep override dict\n",
    "sweep_config_dict = dict(\n",
    "    method='grid',\n",
    "    metric=dict(goal='maximize', name='scores/val_dice_mean_tumour_fold0'),\n",
    "    parameters=dict(\n",
    "        # disturbance_mode=dict(\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        # reg_state=dict(\n",
    "        #     values=['best','combined']\n",
    "        # ),\n",
    "        disturbance_strength=dict(\n",
    "            values=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "        ),\n",
    "        disturbed_percentage=dict(\n",
    "            values=[0.3, 0.6]\n",
    "        ),\n",
    "        data_param_mode=dict(\n",
    "            values=[\n",
    "                DataParamMode.INSTANCE_PARAMS,\n",
    "                DataParamMode.DISABLED,\n",
    "            ]\n",
    "        ),\n",
    "        # use_risk_regularization=dict(\n",
    "        #     values=[False, True]\n",
    "        # )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549637cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def normal_run():\n",
    "    with wandb.init(project=\"curriculum_deeplab\", group=\"training\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        ) as run:\n",
    "\n",
    "        run_name = run.name\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init() as run:\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "\n",
    "        run_name = run.name\n",
    "        config = wandb.config\n",
    "        training_dataset = prepare_data(config)\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "\n",
    "if config_dict['do_sweep']:\n",
    "    # Integrate all config_dict entries into sweep_dict.parameters -> sweep overrides config_dict\n",
    "    cp_config_dict = copy.deepcopy(config_dict)\n",
    "    # cp_config_dict.update(copy.deepcopy(sweep_config_dict['parameters']))\n",
    "    for del_key in sweep_config_dict['parameters'].keys():\n",
    "        if del_key in cp_config_dict:\n",
    "            del cp_config_dict[del_key]\n",
    "    merged_sweep_config_dict = copy.deepcopy(sweep_config_dict)\n",
    "    # merged_sweep_config_dict.update(cp_config_dict)\n",
    "    for key, value in cp_config_dict.items():\n",
    "        merged_sweep_config_dict['parameters'][key] = dict(value=value)\n",
    "    # Convert enum values in parameters to string. They will be identified by their numerical index otherwise\n",
    "    for key, param_dict in merged_sweep_config_dict['parameters'].items():\n",
    "        if 'value' in param_dict and isinstance(param_dict['value'], Enum):\n",
    "            param_dict['value'] = str(param_dict['value'])\n",
    "        if 'values' in param_dict:\n",
    "            param_dict['values'] = [str(elem) if isinstance(elem, Enum) else elem for elem in param_dict['values']]\n",
    "\n",
    "        merged_sweep_config_dict['parameters'][key] = param_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(merged_sweep_config_dict, project=\"curriculum_deeplab\")\n",
    "    wandb.agent(sweep_id, function=sweep_run)\n",
    "\n",
    "else:\n",
    "    normal_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7469925",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# def inference_DL(run_name, config, inf_dataset):\n",
    "\n",
    "#     score_dicts = []\n",
    "\n",
    "#     fold_iter = range(config.num_folds)\n",
    "#     if config_dict['only_first_fold']:\n",
    "#         fold_iter = fold_iter[0:1]\n",
    "\n",
    "#     for fold_idx in fold_iter:\n",
    "#         lraspp, *_ = load_model(f\"{config.mdl_save_prefix}_fold{fold_idx}\", config, len(validation_dataset))\n",
    "\n",
    "#         lraspp.eval()\n",
    "#         inf_dataset.eval()\n",
    "#         stack_dim = config.use_2d_normal_to\n",
    "\n",
    "#         inf_dices = []\n",
    "#         inf_dices_tumour = []\n",
    "#         inf_dices_cochlea = []\n",
    "\n",
    "#         for inf_sample in inf_dataset:\n",
    "#             global_idx = get_global_idx(fold_idx, sample_idx, config.epochs)\n",
    "#             crossmoda_id = sample['crossmoda_id']\n",
    "#             with amp.autocast(enabled=True):\n",
    "#                 with torch.no_grad():\n",
    "\n",
    "#                     # Create batch out of single val sample\n",
    "#                     b_inf_img = inf_sample['image'].unsqueeze(0)\n",
    "#                     b_inf_seg = inf_sample['label'].unsqueeze(0)\n",
    "\n",
    "#                     B = b_inf_img.shape[0]\n",
    "\n",
    "#                     b_inf_img = b_inf_img.unsqueeze(1).float().cuda()\n",
    "#                     b_inf_seg = b_inf_seg.cuda()\n",
    "#                     b_inf_img_2d = make_2d_stack_from_3d(b_inf_img, stack_dim=stack_dim)\n",
    "\n",
    "#                     if config.use_mind:\n",
    "#                         b_inf_img_2d = mindssc(b_inf_img_2d.unsqueeze(1)).squeeze(2)\n",
    "\n",
    "#                     output_inf = lraspp(b_inf_img_2d)['out']\n",
    "\n",
    "#                     # Prepare logits for scoring\n",
    "#                     # Scoring happens in 3D again - unstack batch tensor again to stack of 3D\n",
    "#                     inf_logits_for_score = make_3d_from_2d_stack(output_inf, stack_dim, B)\n",
    "#                     inf_logits_for_score = inf_logits_for_score.argmax(1)\n",
    "\n",
    "#                     inf_dice = dice3d(\n",
    "#                         torch.nn.functional.one_hot(inf_logits_for_score, 3),\n",
    "#                         torch.nn.functional.one_hot(b_inf_seg, 3),\n",
    "#                         one_hot_torch_style=True\n",
    "#                     )\n",
    "#                     inf_dices.append(get_batch_dice_wo_bg(inf_dice))\n",
    "#                     inf_dices_tumour.append(get_batch_dice_tumour(inf_dice))\n",
    "#                     inf_dices_cochlea.append(get_batch_dice_cochlea(inf_dice))\n",
    "\n",
    "#                     if config.do_plot:\n",
    "#                         print(\"Inference 3D image label/ground-truth\")\n",
    "#                         print(inf_dice)\n",
    "#                         # display_all_seg_slices(b_seg.unsqueeze(1), logits_for_score)\n",
    "#                         display_seg(in_type=\"single_3D\",\n",
    "#                             reduce_dim=\"W\",\n",
    "#                             img=inf_sample['image'].unsqueeze(0).cpu(),\n",
    "#                             seg=inf_logits_for_score.squeeze(0).cpu(),\n",
    "#                             ground_truth=b_inf_seg.squeeze(0).cpu(),\n",
    "#                             crop_to_non_zero_seg=True,\n",
    "#                             crop_to_non_zero_gt=True,\n",
    "#                             alpha_seg=.4,\n",
    "#                             alpha_gt=.2\n",
    "#                         )\n",
    "\n",
    "#             if config.debug:\n",
    "#                 break\n",
    "\n",
    "#         mean_inf_dice = np.nanmean(inf_dices)\n",
    "#         mean_inf_dice_tumour = np.nanmean(inf_dices_tumour)\n",
    "#         mean_inf_dice_cochlea = np.nanmean(inf_dices_cochlea)\n",
    "\n",
    "#         print(f'inf_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_inf_dice*100:.2f}%\")\n",
    "#         print(f'inf_dice_mean_tumour_fold{fold_idx}', f\"{mean_inf_dice_tumour*100:.2f}%\")\n",
    "#         print(f'inf_dice_mean_cochlea_fold{fold_idx}', f\"{mean_inf_dice_cochlea*100:.2f}%\")\n",
    "#         wandb.log({f'scores/inf_dice_mean_wo_bg_fold{fold_idx}': mean_inf_dice}, step=global_idx)\n",
    "#         wandb.log({f'scores/inf_dice_mean_tumour_fold{fold_idx}': mean_inf_dice_tumour}, step=global_idx)\n",
    "#         wandb.log({f'scores/inf_dice_mean_cochlea_fold{fold_idx}': mean_inf_dice_cochlea}, step=global_idx)\n",
    "\n",
    "#         # Store data for inter-fold scoring\n",
    "#         class_dice_list = inf_dices.tolist()[0]\n",
    "#         for class_idx, class_dice in enumerate(class_dice_list):\n",
    "#             score_dicts.append(\n",
    "#                 {\n",
    "#                     'fold_idx': fold_idx,\n",
    "#                     'crossmoda_id': crossmoda_id,\n",
    "#                     'class_idx': class_idx,\n",
    "#                     'class_dice': class_dice,\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     mean_oa_inf_dice = np.nanmean(torch.tensor([score['class_dice'] for score in score_dicts]))\n",
    "#     print(f\"Mean dice over all folds, classes and samples: {mean_oa_inf_dice*100:.2f}%\")\n",
    "#     wandb.log({'scores/mean_dice_all_folds_samples_classes': mean_oa_inf_dice}, step=global_idx)\n",
    "\n",
    "#     return score_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a608620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds_scores = []\n",
    "# run = wandb.init(project=\"curriculum_deeplab\", name=run_name, group=f\"testing\", job_type=\"test\",\n",
    "#         config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "#         mode=config_dict['wandb_mode']\n",
    "# )\n",
    "# config = wandb.config\n",
    "# score_dicts = inference_DL(run_name, config, validation_dataset)\n",
    "# folds_scores.append(score_dicts)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984e8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
