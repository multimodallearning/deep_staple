{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "characteristic-breathing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2080 Ti\n",
      "PyTorch Version: 1.8.1\n",
      "CuDNN Version: 8005\n",
      "gpu usage (current/max): 0.00 / 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "#from .utils import load_state_dict_from_url\n",
    "from typing import Callable, Any, Optional, List\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "print('PyTorch Version:', torch.__version__)\n",
    "\n",
    "print('CuDNN Version:', torch.backends.cudnn.version())\n",
    "\n",
    "def gpu_usage():\n",
    "    print('gpu usage (current/max): {:.2f} / {:.2f} GB'.format(torch.cuda.memory_allocated()*1e-9, torch.cuda.max_memory_allocated()*1e-9))\n",
    "\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "domestic-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atrous Spatial Pyramid Pooling (Segmentation Network)\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            #nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        x = F.adaptive_avg_pool3d(x,(1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')#, align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = []\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()))\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "second-divorce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#CNN layer 24\n"
     ]
    }
   ],
   "source": [
    "#Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "    \n",
    "in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "mid_channels = torch.Tensor([32,96,144,144,192,192,192,384]).long()\n",
    "out_channels = torch.Tensor([16,24,24,32,32,32,64,64]).long()\n",
    "mid_stride = torch.Tensor([1,1,1,1,1,2,1,1])\n",
    "\n",
    "net = []\n",
    "net.append(nn.Identity())\n",
    "for i in range(len(in_channels)):\n",
    "    inc = int(in_channels[i]); midc = int(mid_channels[i]); outc = int(out_channels[i]); strd = int(mid_stride[i])\n",
    "    layer = nn.Sequential(nn.Conv3d(inc,midc,1,bias=False),nn.BatchNorm3d(midc),nn.ReLU6(True),\\\n",
    "                    nn.Conv3d(midc,midc,3,stride=strd,padding=1,bias=False,groups=midc),nn.BatchNorm3d(midc),nn.ReLU6(True),\\\n",
    "                                   nn.Conv3d(midc,outc,1,bias=False),nn.BatchNorm3d(outc))\n",
    "    if(i==0):\n",
    "        layer[0] = nn.Conv3d(inc,midc,3,padding=1,stride=2,bias=False)\n",
    "    if((inc==outc)&(strd==1)):\n",
    "        net.append(ResBlock(layer))\n",
    "    else:\n",
    "        net.append(layer)\n",
    "\n",
    "backbone = nn.Sequential(*net)\n",
    "\n",
    "count = 0\n",
    "# weight initialization\n",
    "for m in backbone.modules():\n",
    "    if isinstance(m, nn.Conv3d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        count += 1\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "print('#CNN layer',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sharp-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "#newer model (one more stride, no groups in head)\n",
    "aspp = ASPP(64,(2,4,8,16),128)#ASPP(64,(1,),128)#\n",
    "num_classes = 26#14 # 25 for verse19 and 29 for verse20\n",
    "head = nn.Sequential(nn.Conv3d(128+16, 64, 1, padding=0,groups=1, bias=False),nn.BatchNorm3d(64),nn.ReLU(),\\\n",
    "                     nn.Conv3d(64, 64, 3, groups=1,padding=1, bias=False),nn.BatchNorm3d(64),nn.ReLU(),\\\n",
    "                     nn.Conv3d(64, num_classes, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "written-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerSe_iso15(Dataset):\n",
    "    \"\"\"VerSe Dataset already preprocessed: normalisation and orientation with 1.5mm spacing\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_path = []\n",
    "        self.counter = 0\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            for filename in files:\n",
    "                    path =  root + '/' + filename\n",
    "                    if \"msk\" not in path and 'nii.gz' in path:\n",
    "                        self.counter += 1\n",
    "                        self.img_path.append(path)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.counter\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = self.img_path[idx]\n",
    "        seg_name = self.img_path[idx].split('.nii.gz')[0] + '_msk.nii.gz'\n",
    "    \n",
    "        image = nib.load(img_name).get_fdata()[:, :, :]\n",
    "        label = nib.load(seg_name).get_fdata()[:, :, :]\n",
    "        \n",
    "        image = (2*(image-image.min())/ (image.max() - image.min()) - 1)\n",
    "    \n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        image = torch.from_numpy(image).unsqueeze(0)\n",
    "        label = torch.from_numpy(label).unsqueeze(0)\n",
    "        \n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "contrary-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = VerSe_iso15(root_dir='/share/data_zoe1/hempe/data/VerSeV2/preprocessed19/iso_15_val', \\\n",
    "                                 transform= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "latin-exchange",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = torch.load('mobile_aspp2_3d_verse_edge_iso_128_patch_4k.pth')\n",
    "backbone.load_state_dict(cp['backbone'])\n",
    "aspp.load_state_dict(cp['aspp'])\n",
    "head.load_state_dict(cp['head'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "civil-certification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 327, 121, 103])\n",
      "total time: 0.03352236747741699\n"
     ]
    }
   ],
   "source": [
    "head.eval()\n",
    "head.cuda()\n",
    "backbone.eval()\n",
    "backbone.cuda()\n",
    "aspp.eval()\n",
    "aspp.cuda()\n",
    "\n",
    "#pretty: 11, 15, 18, 20fx\n",
    "#not so pretty: 7, 8\n",
    "index = 11\n",
    "sample = validation_dataset[index]\n",
    "image = sample[\"image\"]\n",
    "label = sample[\"label\"]\n",
    "C,D,H,W = image.shape\n",
    "print(image.shape)\n",
    "\n",
    "import time\n",
    "ts = time.time()    \n",
    "with torch.no_grad():\n",
    "    x1 = backbone[:2](pad_img.cuda().unsqueeze(0).float())\n",
    "    y = aspp(backbone[2:](x1))\n",
    "    y = torch.cat((x1,F.interpolate(y,scale_factor=2)),1)\n",
    "    output = F.interpolate(checkpoint(head,y),scale_factor=2,mode='trilinear')\n",
    "    print('total time:', time.time() - ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
